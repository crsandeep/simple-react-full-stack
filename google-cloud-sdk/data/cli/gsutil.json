{
  "CLI_VERSION": "4.49",
  "VERSION": "1",
  "capsule": "",
  "commands": {
    "acl": {
      "capsule": "Get, set, or change bucket and/or object ACLs",
      "commands": {
        "ch": {
          "capsule": "Get, set, or change bucket and/or object ACLs",
          "commands": {},
          "flags": {
            "-d": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Remove all roles associated with the matching entity.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-d",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-f": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Normally gsutil stops at the first error. The -f option causes\n to continue when it encounters errors. With this option the\nutil exit status will be 0 even if some ACLs couldn't be\nanged.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-f",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-g": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or modify a group entity's role.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-g",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or modify a project viewers/editors/owners role.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-r": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Performs acl ch request recursively, to all objects under the\necified URL.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-r",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-u": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or modify a user entity's role.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-u",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "acl",
            "ch"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"acl ch\" (or \"acl change\") command updates access control lists, similar\nin spirit to the Linux chmod command. You can specify multiple access grant\nadditions and deletions in a single command run; all changes will be made\natomically to each object in turn. For example, if the command requests\ndeleting one grant and adding a different grant, the ACLs being updated will\nnever be left in an intermediate state where one grant has been deleted but\nthe second grant not yet added. Each change specifies a user or group grant\nto add or delete, and for grant additions, one of R, W, O (for the\npermission to be granted). A more formal description is provided in a later\nsection; below we provide examples.",
            "ENTITIES": "There are four different entity types: Users, Groups, All Authenticated Users,\nand All Users.\n\nUsers are added with -u and a plain ID or email address, as in\n\"-u john-doe@gmail.com:r\". Note: Service Accounts are considered to be users.\n\nGroups are like users, but specified with the -g flag, as in\n\"-g power-users@example.com:fc\". Groups may also be specified as a full\ndomain, as in \"-g my-company.com:r\".\n\nAllAuthenticatedUsers and AllUsers are specified directly, as\nin \"-g AllUsers:R\" or \"-g AllAuthenticatedUsers:O\". These are case\ninsensitive, and may be shortened to \"all\" and \"allauth\", respectively.\n\nRemoving roles is specified with the -d flag and an ID, email\naddress, domain, or one of AllUsers or AllAuthenticatedUsers.\n\nMany entities' roles can be specified on the same command line, allowing\nbundled changes to be executed in a single run. This will reduce the number of\nrequests made to the server.",
            "EXAMPLES": "Examples for \"ch\" sub-command:\n\nGrant anyone on the internet READ access to the object example-object:\n\n  gsutil acl ch -u AllUsers:R gs://example-bucket/example-object\n\nNOTE: By default, publicly readable objects are served with a Cache-Control\nheader allowing such objects to be cached for 3600 seconds. If you need to\nensure that updates become visible immediately, you should set a\nCache-Control header of \"Cache-Control:private, max-age=0, no-transform\" on\nsuch objects. For help doing this, see \"gsutil help setmeta\".\n\nGrant anyone on the internet WRITE access to the bucket example-bucket:\n\nWARNING: this is not recommended as you will be responsible for the content\n\n  gsutil acl ch -u AllUsers:W gs://example-bucket\n\nGrant the user john.doe@example.com WRITE access to the bucket\nexample-bucket:\n\n  gsutil acl ch -u john.doe@example.com:WRITE gs://example-bucket\n\nGrant the group admins@example.com OWNER access to all jpg files in\nthe top level of example-bucket:\n\n  gsutil acl ch -g admins@example.com:O gs://example-bucket/*.jpg\n\nGrant the owners of project example-project WRITE access to the bucket\nexample-bucket:\n\n  gsutil acl ch -p owners-example-project:W gs://example-bucket\n\nNOTE: You can replace 'owners' with 'viewers' or 'editors' to grant access\nto a project's viewers/editors respectively.\n\nRemove access to the bucket example-bucket for the viewers of project number\n12345:\n\n  gsutil acl ch -d viewers-12345 gs://example-bucket\n\nNOTE: You cannot remove the project owners group from ACLs of gs:// buckets in\nthe given project. Attempts to do so will appear to succeed, but the service\nwill add the project owners group into the new set of ACLs before applying it.\n\nNote that removing a project requires you to reference the project by\nits number (which you can see with the acl get command) as opposed to its\nproject ID string.\n\nGrant the user with the specified canonical ID READ access to all objects\nin example-bucket that begin with folder/:\n\n  gsutil acl ch -r \\\n    -u 84fac329bceSAMPLE777d5d22b8SAMPLE785ac2SAMPLE2dfcf7c4adf34da46:R \\\n    gs://example-bucket/folder/\n\nGrant the service account foo@developer.gserviceaccount.com WRITE access to\nthe bucket example-bucket:\n\n  gsutil acl ch -u foo@developer.gserviceaccount.com:W gs://example-bucket\n\nGrant all users from the `G Suite\n<https://www.google.com/work/apps/business/>`_ domain my-domain.org READ\naccess to the bucket gcs.my-domain.org:\n\n  gsutil acl ch -g my-domain.org:R gs://gcs.my-domain.org\n\nRemove any current access by john.doe@example.com from the bucket\nexample-bucket:\n\n  gsutil acl ch -d john.doe@example.com gs://example-bucket\n\nIf you have a large number of objects to update, enabling multi-threading\nwith the gsutil -m flag can significantly improve performance. The\nfollowing command adds OWNER for admin@example.org using\nmulti-threading:\n\n  gsutil -m acl ch -r -u admin@example.org:O gs://example-bucket\n\nGrant READ access to everyone from my-domain.org and to all authenticated\nusers, and grant OWNER to admin@mydomain.org, for the buckets\nmy-bucket and my-other-bucket, with multi-threading enabled:\n\n  gsutil -m acl ch -r -g my-domain.org:R -g AllAuth:R \\\n    -u admin@mydomain.org:O gs://my-bucket/ gs://my-other-bucket",
            "ROLES": "You may specify the following roles with either their shorthand or\ntheir full name:\n\n  R: READ\n  W: WRITE\n  O: OWNER\n\nFor more information on these roles and the access they grant, see the\npermissions section of the `Access Control Lists page\n<https://cloud.google.com/storage/docs/access-control/lists#permissions>`_."
          }
        },
        "get": {
          "capsule": "Get, set, or change bucket and/or object ACLs",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "acl",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"acl get\" command gets the ACL text for a bucket or object, which you can\nsave and edit for the acl set command."
          }
        },
        "set": {
          "capsule": "Get, set, or change bucket and/or object ACLs",
          "commands": {},
          "flags": {
            "-a": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Performs \"acl set\" request on all object versions.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-a",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-f": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Normally gsutil stops at the first error. The -f option causes\n to continue when it encounters errors. If some of the ACLs\nuldn't be set, gsutil's exit status will be non-zero even if\nis flag is set. This option is implicitly set when running\nsutil -m acl...\".",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-f",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-r": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Performs \"acl set\" request recursively, to all objects under\ne specified URL.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-r",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "acl",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"acl set\" command allows you to set an Access Control List on one or\nmore buckets and objects. The simplest way to use it is to specify one of\nthe canned ACLs, e.g.,:\n\n  gsutil acl set private gs://bucket\n\nIf you want to make an object or bucket publicly readable or writable, it is\nrecommended to use \"acl ch\", to avoid accidentally removing OWNER permissions.\nSee the \"acl ch\" section for details.\n\nSee `Predefined ACLs\n<https://cloud.google.com/storage/docs/access-control/lists#predefined-acl>`_\nfor a list of canned ACLs.\n\nIf you want to define more fine-grained control over your data, you can\nretrieve an ACL using the \"acl get\" command, save the output to a file, edit\nthe file, and then use the \"acl set\" command to set that ACL on the buckets\nand/or objects. For example:\n\n  gsutil acl get gs://bucket/file.txt > acl.txt\n\nMake changes to acl.txt such as adding an additional grant, then:\n\n  gsutil acl set acl.txt gs://cats/file.txt\n\nNote that you can set an ACL on multiple buckets or objects at once,\nfor example:\n\n  gsutil acl set acl.txt gs://bucket/*.jpg\n\nIf you have a large number of ACLs to update you might want to use the\ngsutil -m option, to perform a parallel (multi-threaded/multi-processing)\nupdate:\n\n  gsutil -m acl set acl.txt gs://bucket/*.jpg\n\nNote that multi-threading/multi-processing is only done when the named URLs\nrefer to objects, which happens either if you name specific objects or\nif you enumerate objects by using an object wildcard or specifying\nthe acl -r flag."
          }
        }
      },
      "flags": {
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Performs \"acl set\" request on all object versions.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Remove all roles associated with the matching entity.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Normally gsutil stops at the first error. The -f option causes\n to continue when it encounters errors. With this option the\nutil exit status will be 0 even if some ACLs couldn't be\nanged.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-g": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or modify a group entity's role.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-g",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or modify a project viewers/editors/owners role.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Performs acl ch request recursively, to all objects under the\necified URL.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-u": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or modify a user entity's role.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-u",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "acl"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CH": "The \"acl ch\" (or \"acl change\") command updates access control lists, similar\nin spirit to the Linux chmod command. You can specify multiple access grant\nadditions and deletions in a single command run; all changes will be made\natomically to each object in turn. For example, if the command requests\ndeleting one grant and adding a different grant, the ACLs being updated will\nnever be left in an intermediate state where one grant has been deleted but\nthe second grant not yet added. Each change specifies a user or group grant\nto add or delete, and for grant additions, one of R, W, O (for the\npermission to be granted). A more formal description is provided in a later\nsection; below we provide examples.",
        "DESCRIPTION": "The acl command has three sub-commands:",
        "ENTITIES": "There are four different entity types: Users, Groups, All Authenticated Users,\nand All Users.\n\nUsers are added with -u and a plain ID or email address, as in\n\"-u john-doe@gmail.com:r\". Note: Service Accounts are considered to be users.\n\nGroups are like users, but specified with the -g flag, as in\n\"-g power-users@example.com:fc\". Groups may also be specified as a full\ndomain, as in \"-g my-company.com:r\".\n\nAllAuthenticatedUsers and AllUsers are specified directly, as\nin \"-g AllUsers:R\" or \"-g AllAuthenticatedUsers:O\". These are case\ninsensitive, and may be shortened to \"all\" and \"allauth\", respectively.\n\nRemoving roles is specified with the -d flag and an ID, email\naddress, domain, or one of AllUsers or AllAuthenticatedUsers.\n\nMany entities' roles can be specified on the same command line, allowing\nbundled changes to be executed in a single run. This will reduce the number of\nrequests made to the server.",
        "EXAMPLES": "Examples for \"ch\" sub-command:\n\nGrant anyone on the internet READ access to the object example-object:\n\n  gsutil acl ch -u AllUsers:R gs://example-bucket/example-object\n\nNOTE: By default, publicly readable objects are served with a Cache-Control\nheader allowing such objects to be cached for 3600 seconds. If you need to\nensure that updates become visible immediately, you should set a\nCache-Control header of \"Cache-Control:private, max-age=0, no-transform\" on\nsuch objects. For help doing this, see \"gsutil help setmeta\".\n\nGrant anyone on the internet WRITE access to the bucket example-bucket:\n\nWARNING: this is not recommended as you will be responsible for the content\n\n  gsutil acl ch -u AllUsers:W gs://example-bucket\n\nGrant the user john.doe@example.com WRITE access to the bucket\nexample-bucket:\n\n  gsutil acl ch -u john.doe@example.com:WRITE gs://example-bucket\n\nGrant the group admins@example.com OWNER access to all jpg files in\nthe top level of example-bucket:\n\n  gsutil acl ch -g admins@example.com:O gs://example-bucket/*.jpg\n\nGrant the owners of project example-project WRITE access to the bucket\nexample-bucket:\n\n  gsutil acl ch -p owners-example-project:W gs://example-bucket\n\nNOTE: You can replace 'owners' with 'viewers' or 'editors' to grant access\nto a project's viewers/editors respectively.\n\nRemove access to the bucket example-bucket for the viewers of project number\n12345:\n\n  gsutil acl ch -d viewers-12345 gs://example-bucket\n\nNOTE: You cannot remove the project owners group from ACLs of gs:// buckets in\nthe given project. Attempts to do so will appear to succeed, but the service\nwill add the project owners group into the new set of ACLs before applying it.\n\nNote that removing a project requires you to reference the project by\nits number (which you can see with the acl get command) as opposed to its\nproject ID string.\n\nGrant the user with the specified canonical ID READ access to all objects\nin example-bucket that begin with folder/:\n\n  gsutil acl ch -r \\\n    -u 84fac329bceSAMPLE777d5d22b8SAMPLE785ac2SAMPLE2dfcf7c4adf34da46:R \\\n    gs://example-bucket/folder/\n\nGrant the service account foo@developer.gserviceaccount.com WRITE access to\nthe bucket example-bucket:\n\n  gsutil acl ch -u foo@developer.gserviceaccount.com:W gs://example-bucket\n\nGrant all users from the `G Suite\n<https://www.google.com/work/apps/business/>`_ domain my-domain.org READ\naccess to the bucket gcs.my-domain.org:\n\n  gsutil acl ch -g my-domain.org:R gs://gcs.my-domain.org\n\nRemove any current access by john.doe@example.com from the bucket\nexample-bucket:\n\n  gsutil acl ch -d john.doe@example.com gs://example-bucket\n\nIf you have a large number of objects to update, enabling multi-threading\nwith the gsutil -m flag can significantly improve performance. The\nfollowing command adds OWNER for admin@example.org using\nmulti-threading:\n\n  gsutil -m acl ch -r -u admin@example.org:O gs://example-bucket\n\nGrant READ access to everyone from my-domain.org and to all authenticated\nusers, and grant OWNER to admin@mydomain.org, for the buckets\nmy-bucket and my-other-bucket, with multi-threading enabled:\n\n  gsutil -m acl ch -r -g my-domain.org:R -g AllAuth:R \\\n    -u admin@mydomain.org:O gs://my-bucket/ gs://my-other-bucket",
        "GET": "The \"acl get\" command gets the ACL text for a bucket or object, which you can\nsave and edit for the acl set command.",
        "ROLES": "You may specify the following roles with either their shorthand or\ntheir full name:\n\n  R: READ\n  W: WRITE\n  O: OWNER\n\nFor more information on these roles and the access they grant, see the\npermissions section of the `Access Control Lists page\n<https://cloud.google.com/storage/docs/access-control/lists#permissions>`_.",
        "SET": "The \"acl set\" command allows you to set an Access Control List on one or\nmore buckets and objects. The simplest way to use it is to specify one of\nthe canned ACLs, e.g.,:\n\n  gsutil acl set private gs://bucket\n\nIf you want to make an object or bucket publicly readable or writable, it is\nrecommended to use \"acl ch\", to avoid accidentally removing OWNER permissions.\nSee the \"acl ch\" section for details.\n\nSee `Predefined ACLs\n<https://cloud.google.com/storage/docs/access-control/lists#predefined-acl>`_\nfor a list of canned ACLs.\n\nIf you want to define more fine-grained control over your data, you can\nretrieve an ACL using the \"acl get\" command, save the output to a file, edit\nthe file, and then use the \"acl set\" command to set that ACL on the buckets\nand/or objects. For example:\n\n  gsutil acl get gs://bucket/file.txt > acl.txt\n\nMake changes to acl.txt such as adding an additional grant, then:\n\n  gsutil acl set acl.txt gs://cats/file.txt\n\nNote that you can set an ACL on multiple buckets or objects at once,\nfor example:\n\n  gsutil acl set acl.txt gs://bucket/*.jpg\n\nIf you have a large number of ACLs to update you might want to use the\ngsutil -m option, to perform a parallel (multi-threaded/multi-processing)\nupdate:\n\n  gsutil -m acl set acl.txt gs://bucket/*.jpg\n\nNote that multi-threading/multi-processing is only done when the named URLs\nrefer to objects, which happens either if you name specific objects or\nif you enumerate objects by using an object wildcard or specifying\nthe acl -r flag."
      }
    },
    "bucketpolicyonly": {
      "capsule": "Configure uniform bucket-level access",
      "commands": {
        "get": {
          "capsule": "Configure uniform bucket-level access",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "bucketpolicyonly",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``bucketpolicyonly get`` command shows whether uniform bucket-level\naccess is enabled for the specified Cloud Storage bucket.",
            "EXAMPLES": "Check if your buckets are using uniform bucket-level access:\n\n  gsutil bucketpolicyonly get gs://redbucket gs://bluebucket"
          }
        },
        "set": {
          "capsule": "Configure uniform bucket-level access",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "bucketpolicyonly",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``bucketpolicyonly set`` command enables or disables the uniform bucket-level\naccess feature on Google Cloud Storage buckets.",
            "EXAMPLES": "Configure your buckets to use uniform bucket-level access:\n\n  gsutil bucketpolicyonly set on gs://redbucket gs://bluebucket\n\nConfigure your buckets to NOT use uniform bucket-level access:\n\n  gsutil bucketpolicyonly set off gs://redbucket gs://bluebucket"
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "bucketpolicyonly"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The Bucket Policy Only feature is now known as `uniform bucket-level access\n<https://cloud.google.com/storage/docs/uniform-bucket-level-access>`_.\nThe ``bucketpolicyonly`` command is still supported, but we recommend using\nthe equivalent ``ubla`` `command<https://cloud.google.com/storage/docs/gsutil/commands/ubla>`_.\n\nThe ``bucketpolicyonly`` command is used to retrieve or configure the\nuniform bucket-level access setting of Cloud Storage buckets. This command has\ntwo sub-commands, ``get`` and ``set``.",
        "EXAMPLES": "Configure your buckets to use uniform bucket-level access:\n\n  gsutil bucketpolicyonly set on gs://redbucket gs://bluebucket\n\nConfigure your buckets to NOT use uniform bucket-level access:\n\n  gsutil bucketpolicyonly set off gs://redbucket gs://bluebucket",
        "GET": "The ``bucketpolicyonly get`` command shows whether uniform bucket-level\naccess is enabled for the specified Cloud Storage bucket.",
        "SET": "The ``bucketpolicyonly set`` command enables or disables the uniform bucket-level\naccess feature on Google Cloud Storage buckets."
      }
    },
    "cat": {
      "capsule": "Concatenate object content to stdout",
      "commands": {},
      "flags": {
        "-h": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prints short header for each object. For example:\ngsutil cat -h gs://bucket/meeting_notes/2012_Feb/*.txt\nis would print a header with the object name before the contents\n each text object that matched the wildcard.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-h",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "range Causes gsutil to output just the specified byte range of the\nject. Ranges are can be of these forms:\nstart-end (e.g., -r 256-5939)\nstart- (e.g., -r 256-)\n-numbytes (e.g., -r -5)\nere offsets start at 0, start-end means to return bytes start\nrough end (inclusive), start- means to return bytes start\nrough the end of the object, and -numbytes means to return the\nst numbytes of the object. For example:\ngsutil cat -r 256-939 gs://bucket/object\nturns bytes 256 through 939, while:\ngsutil cat -r -5 gs://bucket/object\nturns the final 5 bytes of the object.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "cat"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The cat command outputs the contents of one or more URLs to stdout.\nWhile the cat command does not compute a checksum, it is otherwise\nequivalent to doing:\n\n  gsutil cp url... -\n\n(The final '-' causes gsutil to stream the output to stdout.)\n\nWARNING: The gsutil cat command does not compute a checksum of the\ndownloaded data. Therefore, we recommend that users either perform\ntheir own validation of the output of gsutil cat or use gsutil cp\nor rsync (both of which perform integrity checking automatically)."
      }
    },
    "compose": {
      "capsule": "Concatenate a sequence of objects into a new composite object.",
      "commands": {},
      "flags": {},
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "compose"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The compose command creates a new object whose content is the concatenation\nof a given sequence of source objects under the same bucket. gsutil uses\nthe content type of the first source object to determine the destination\nobject's content type. For more information, please see:\nhttps://cloud.google.com/storage/docs/composite-objects\n\nNote also that the gsutil cp command can automatically split uploads for\nlarge files into multiple component objects, upload them in parallel, and\ncompose them into a final object. This will still perform all uploads from\na single machine. For extremely large files and/or very low per-machine\nbandwidth, you may want to split the file and upload it from multiple\nmachines, and later compose these parts of the file manually. See the\n'PARALLEL COMPOSITE UPLOADS' section under 'gsutil help cp' for details.\n\nAppending simply entails uploading your new data to a temporary object,\ncomposing it with the growing append-target, and deleting the temporary\nobject:\n\n  $ echo 'new data' | gsutil cp - gs://bucket/data-to-append\n  $ gsutil compose gs://bucket/append-target gs://bucket/data-to-append \\\n      gs://bucket/append-target\n  $ gsutil rm gs://bucket/data-to-append\n\nNote that there is a limit (currently 32) to the number of components that can\nbe composed in a single operation."
      }
    },
    "config": {
      "capsule": "Obtain credentials and create configuration file",
      "commands": {},
      "flags": {
        "--reauth": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Request token with reauth access (accounts.reauth scope).",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "--reauth",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prompt for Google Cloud Storage access key and secret (the older\nthentication method before OAuth2 was supported) instead of\ntaining an OAuth2 token.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-b": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes ``gsutil config`` to launch a browser to obtain OAuth2\nproval and the project ID instead of showing the URL for each\nd asking the user to open the browser. This will probably not\nrk as expected if you are running gsutil from an ssh window, or\ning gsutil on Windows.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-b",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prompt for service account credentials. This option requires that\n-a`` is not set.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Request token with full control (devstorage.full_control scope).\nte that this does not provide non-storage scopes, such as those\neded to edit Pub/Sub and KMS resources (used with the\notification' and 'kms' commands).",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-n": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Write the configuration file without authentication configured.\nis flag is mutually exlusive with all flags other than ``-o``.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-n",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-o": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<file> Write the configuration to <file> instead of ~/.boto.\ne ``-`` for stdout.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-o",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Request token with read-only access (devstorage.read_only scope).",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<scope> Request a specific OAuth2 <scope> instead of the default(s). This\ntion may be repeated to request multiple scopes, and may be used\n conjuction with other flags that request a specific scope.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-w": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Request token with read-write access\nevstorage.read_write scope).",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-w",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "config"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CONFIGURATION-CONTROLLABLE FEATURES": "With the exception of setting up gsutil to work through a proxy, most users\nwon't need to edit values in the boto configuration file; values found in\nthe file tend to be of more specialized use than command line\noption-controllable features. For information on setting up gsutil to work\nthrough a proxy, see the comments preceding the proxy settings in your\n.boto file.\n\nThe following are the currently defined configuration settings, broken\ndown by section. Their use is documented in comments preceding each, in\nthe configuration file. If you see a setting you want to change that's not\nlisted in your current file, see the section below on Updating to the Latest\nConfiguration File.\n\nThe currently supported settings, are, by section:\n\n  [Credentials]\n    aws_access_key_id\n    aws_secret_access_key\n    gs_access_key_id\n    gs_host\n    gs_host_header\n    gs_json_host\n    gs_json_host_header\n    gs_json_port\n    gs_oauth2_refresh_token\n    gs_port\n    gs_secret_access_key\n    gs_service_client_id\n    gs_service_key_file\n    gs_service_key_file_password\n    s3_host\n    s3_host_header\n    s3_port\n\n  [Boto]\n    proxy\n    proxy_type\n    proxy_port\n    proxy_user\n    proxy_pass\n    proxy_rdns\n    http_socket_timeout\n    ca_certificates_file\n    https_validate_certificates\n    debug\n    max_retry_delay\n    num_retries\n\n  [GoogleCompute]\n    service_account\n\n  [GSUtil]\n    check_hashes\n    content_language\n    decryption_key1 ... 100\n    default_api_version\n    default_project_id\n    disable_analytics_prompt\n    encryption_key\n    json_api_version\n    max_upload_compression_buffer_size\n    parallel_composite_upload_component_size\n    parallel_composite_upload_threshold\n    sliced_object_download_component_size\n    sliced_object_download_max_components\n    sliced_object_download_threshold\n    parallel_process_count\n    parallel_thread_count\n    gzip_compression_level\n    prefer_api\n    resumable_threshold\n    resumable_tracker_dir (deprecated in 4.6, use state_dir)\n    rsync_buffer_lines\n    software_update_check_period\n    state_dir\n    tab_completion_time_logs\n    tab_completion_timeout\n    task_estimation_threshold\n    test_cmd_regional_bucket_location\n    test_notification_url\n    use_magicfile\n    test_hmac_service_account\n    test_hmac_alt_service_account\n    test_hmac_list_service_account\n\n  [OAuth2]\n    client_id\n    client_secret\n    oauth2_refresh_retries\n    provider_authorization_uri\n    provider_label\n    provider_token_uri\n    token_cache",
        "CREDENTIALS": "By default ``gsutil config`` obtains OAuth2 credentials and writes them to the\n[Credentials] section of the configuration file. Unless otherwise specified,\nit requests a token allowing full control of resources in several services,\ne.g. Cloud Storage, Cloud KMS (used for the 'kms' command), and Cloud Pub/Sub\n(used for the 'notification' command). To request a token with more limited\nscopes, you can specify additional options (see the OPTIONS section below for\nthe full list). Some examples include:\n\nCreate a token with read-only access for storage resources:\n\n  gsutil config -r\n\nCreate a token with read-write access for storage resources:\n\n  gsutil config -w\n\nCreate a token with full-control access for storage resources:\n\n  gsutil config -f\n\nIn addition, ``-s <scope>`` can be specified multiple times to request\nadditional scopes, where ``<scope>`` is specified using the full URL of the\ndesired scope as listed on\nhttps://developers.google.com/identity/protocols/googlescopes.\n\nIf you want to use credentials based on access key and secret (the older\nauthentication method before OAuth2 was supported) instead of OAuth2,\nsee help about the ``-a`` option in the OPTIONS section.\n\nIf you wish to use gsutil with other providers (or to copy data back and\nforth between multiple providers) you can edit their credentials into the\n[Credentials] section after creating the initial configuration file. See the\nlist of settings below for supported settings.",
        "DESCRIPTION": "The ``gsutil config`` command applies to users who have installed gsutil as a\nstandalone tool. If you installed gsutil via the Cloud SDK, ``gsutil config``\nwill fail unless you are specifically using the ``-a`` flag or have configured\ngcloud to not pass its managed credentials to gsutil (via the command ``gcloud\nconfig set pass_credentials_to_gsutil false``). For all other use cases, Cloud\nSDK users should use the ``gcloud auth`` group of commands instead, which will\nconfigure OAuth2 credentials that gcloud implicitly passes to gsutil at\nruntime.\n\nThe ``gsutil config`` command obtains access credentials for Google Cloud\nStorage and writes a boto/gsutil configuration file containing the obtained\ncredentials along with a number of other configuration-controllable values.\n\nUnless specified otherwise (see OPTIONS), the configuration file is written\nto ~/.boto (i.e., the file .boto under the user's home directory). If the\ndefault file already exists, an attempt is made to rename the existing file\nto ~/.boto.bak; if that attempt fails the command will exit. A different\ndestination file can be specified with the ``-o`` option (see OPTIONS).\n\nBecause the boto configuration file contains your credentials you should\nkeep its file permissions set so no one but you has read access. (The file\nis created read-only when you run ``gsutil config``.)",
        "FILE SELECTION PROCEDURE": "By default, gsutil will look for the configuration file in /etc/boto.cfg and\n~/.boto. You can override this choice by setting the BOTO_CONFIG environment\nvariable. This is also useful if you have several different identities or\ncloud storage environments: By setting up the credentials and any additional\nconfiguration in separate files for each, you can switch environments by\nchanging environment variables.\n\nYou can also set up a path of configuration files, by setting the BOTO_PATH\nenvironment variable to contain a \":\" delimited path (or \";\" for Windows).\nFor example setting the BOTO_PATH environment variable to:\n\n  /etc/projects/my_group_project.boto.cfg:/home/mylogin/.boto\n\nwill cause gsutil to load each configuration file found in the path in\norder. This is useful if you want to set up some shared configuration\nstate among many users: The shared state can go in the central shared file\n( /etc/projects/my_group_project.boto.cfg) and each user's individual\ncredentials can be placed in the configuration file in each of their home\ndirectories. For security reasons, users should never share credentials\nvia a shared configuration file.",
        "FILE STRUCTURE": "The configuration file contains a number of sections: [Credentials],\n[Boto], [GSUtil], and [OAuth2]. If you edit the file, make sure to edit the\nappropriate section (discussed below), and to be careful not to mis-edit\nany of the setting names (like \"gs_access_key_id\") and not to remove the\nsection delimiters (like [Credentials]).",
        "SERVICE ACCOUNT CREDENTIALS": "Service accounts are useful for authenticating on behalf of a service or\napplication (as opposed to a user). You can configure credentials for service\naccounts using the ``-e`` option:\n\n  gsutil config -e\n\nNote that if you are using gsutil through the Cloud SDK, you should instead\nactivate your service account via the ``gcloud auth activate-service-account``\ncommand.\n\nWhen you run ``gsutil config -e``, you will be prompted for the path to your\nprivate key file and, if not using a JSON key file, your service account\nemail address and key file password. To get this data, follow the instructions\non `Service Accounts <https://cloud.google.com/storage/docs/authentication#generating-a-private-key>`_.\nUsing this information, gsutil populates the \"gs_service_key_file\" attribute,\nalong with \"gs_service_client_id\" and \"gs_service_key_file_password\" if not\nusing a JSON key file.\n\nNote that your service account will NOT be considered an Owner for the\npurposes of API access (see \"gsutil help creds\" for more information about\nthis). See https://developers.google.com/identity/protocols/OAuth2ServiceAccount\nfor further information on service account authentication.",
        "TO THE LATEST CONFIGURATION FILE": "We add new configuration controllable features to the boto configuration file\nover time, but most gsutil users create a configuration file once and then\nkeep it for a long time, so new features aren't apparent when you update to a\nnewer version of gsutil. If you want to get the latest configuration file\n(which includes all the latest settings and documentation about each) you can\nrename your current file (e.g., to '.boto_old'), run ``gsutil config``, and\nthen edit any configuration settings you wanted from your old file into the\nnewly created file. Note, however, that if you're using OAuth2 credentials and\nyou go back through the OAuth2 configuration dialog it will invalidate your\nprevious OAuth2 credentials."
      }
    },
    "cors": {
      "capsule": "Get or set a CORS JSON document for one or more buckets",
      "commands": {
        "get": {
          "capsule": "Get or set a CORS JSON document for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "cors",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Gets the CORS configuration for a single bucket. The output from\n\"cors get\" can be redirected into a file, edited and then updated using\n\"cors set\"."
          }
        },
        "set": {
          "capsule": "Get or set a CORS JSON document for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "cors",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Sets the CORS configuration for one or more buckets. The\ncors-json-file specified on the command line should be a path to a local\nfile containing a JSON document as described above."
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "cors"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "Gets or sets the Cross-Origin Resource Sharing (CORS) configuration on one or\nmore buckets. This command is supported for buckets only, not objects. An\nexample CORS JSON document looks like the following:\n\n  [\n    {\n      \"origin\": [\"http://origin1.example.com\"],\n      \"responseHeader\": [\"Content-Type\"],\n      \"method\": [\"GET\"],\n      \"maxAgeSeconds\": 3600\n    }\n  ]\n\nThe above JSON document explicitly allows cross-origin GET requests from\nhttp://origin1.example.com and may include the Content-Type response header.\nThe preflight request may be cached for 1 hour.\n\nNote that requests to the authenticated browser download endpoint ``storage.cloud.google.com``\ndo not allow CORS requests. For more information about supported endpoints for CORS, see\n`Cloud Storage CORS support <https://cloud.google.com/storage/docs/cross-origin#server-side-support>`_.\n\nThe following (empty) CORS JSON document removes all CORS configuration for\na bucket:\n\n[]\n\nThe cors command has two sub-commands:",
        "GET": "Gets the CORS configuration for a single bucket. The output from\n\"cors get\" can be redirected into a file, edited and then updated using\n\"cors set\".",
        "SET": "Sets the CORS configuration for one or more buckets. The\ncors-json-file specified on the command line should be a path to a local\nfile containing a JSON document as described above."
      }
    },
    "cp": {
      "capsule": "Copy files and objects",
      "commands": {},
      "flags": {
        "-A": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Copy all source versions from a source buckets/folders.\n If not set, only the live version of each source object is\n copied.\n NOTE: this option is only useful when the destination\n bucket has versioning enabled.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-A",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-D": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Copy in \"daisy chain\" mode, i.e., copying between two buckets\n by hooking a download to an upload, via the machine where\n gsutil is run. This stands in contrast to the default, where\n data are copied between two buckets \"in the cloud\", i.e.,\n without needing to copy via the machine where gsutil runs.\n By default, a \"copy in the cloud\" when the source is a\n composite object will retain the composite nature of the\n object. However, Daisy chain mode can be used to change a\n composite object into a non-composite object. For example:\n gsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\n gsutil mv -p gs://bucket/obj_tmp gs://bucket/obj\n NOTE: Daisy chain mode is automatically used when copying\n between providers (e.g., to copy data from Google Cloud Storage\n to another provider).",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-D",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-I": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes gsutil to read the list of files or objects to copy from\n stdin. This allows you to run a program that generates the list\n of files to upload/download.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-I",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-J": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Applies gzip transport encoding to file uploads. This option\n works like the -j option described above, but it applies to\n all uploaded files, regardless of extension.\n CAUTION: If you use this option and some of the source files\n don't compress well (e.g., that's often true of binary data),\n this option may result in longer uploads.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-J",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-L": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<file> Outputs a manifest log file with detailed information about\n each item that was copied. This manifest contains the following\n information for each item:\n - Source path.\n - Destination path.\n - Source size.\n - Bytes transferred.\n - MD5 hash.\n - UTC date and time transfer was started in ISO 8601 format.\n - UTC date and time transfer was completed in ISO 8601 format.\n - Upload id, if a resumable upload was performed.\n - Final result of the attempted transfer, success or failure.\n - Failure details, if any.\n If the log file already exists, gsutil will use the file as an\n input to the copy process, and will also append log items to\n the existing file. Files/objects that are marked in the\n existing log file as having been successfully copied (or\n skipped) will be ignored. Files/objects without entries will be\n copied and ones previously marked as unsuccessful will be\n retried. This can be used in conjunction with the -c option to\n build a script that copies a large number of objects reliably,\n using a bash script like the following:\n until gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n sleep 1\n done\n The -c option will cause copying to continue after failures\n occur, and the -L option will allow gsutil to pick up where it\n left off without duplicating work. The loop will continue\n running as long as gsutil exits with a non-zero status (such a\n status indicates there was at least one failure during the\n gsutil run).\n NOTE: If you're trying to synchronize the contents of a\n directory and a bucket (or two buckets), see\n \"gsutil help rsync\".",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-L",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-P": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes POSIX attributes to be preserved when objects are\n copied. With this feature enabled, gsutil cp will copy fields\n provided by stat. These are the user ID of the owner, the group\n ID of the owning group, the mode (permissions) of the file, and\n the access/modification time of the file. For downloads, these\n attributes will only be set if the source objects were uploaded\n with this flag enabled.\n On Windows, this flag will only set and restore access time and\n modification time. This is because Windows doesn't have a\n notion of POSIX uid/gid/mode.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-P",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-U": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Skip objects with unsupported object types instead of failing.\n Unsupported object types are Amazon S3 Objects in the GLACIER\n storage class.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-U",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-Z": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Applies gzip content-encoding to file uploads. This option\n works like the -z option described above, but it applies to\n all uploaded files, regardless of extension.\n CAUTION: If you use this option and some of the source files\n don't compress well (e.g., that's often true of binary data),\n this option may result in files taking up more space in the\n cloud than they would if left uncompressed.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-Z",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "canned_acl Sets named canned_acl when uploaded objects created. See\n \"gsutil help acls\" for further details.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "If an error occurs, continue to attempt to copy the remaining\n files. If any copies were unsuccessful, gsutil's exit status\n will be non-zero even if this flag is set. This option is\n implicitly set when running \"gsutil -m cp...\".\n NOTE: -c only applies to the actual copying operation. If an\n error occurs while iterating over the files in the local\n directory (e.g., invalid Unicode file name) gsutil will print\n an error message and abort.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Exclude symlinks. When specified, symbolic links will not be\n copied.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-j": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<ext,...> Applies gzip transport encoding to any file upload whose\n extension matches the -j extension list. This is useful when\n uploading files with compressible content (such as .js, .css,\n or .html files) because it saves network bandwidth while\n also leaving the data uncompressed in Google Cloud Storage.\n When you specify the -j option, files being uploaded are\n compressed in-memory and on-the-wire only. Both the local\n files and Cloud Storage objects remain uncompressed. The\n uploaded objects retain the Content-Type and name of the\n original files.\n Note that if you want to use the top-level -m option to\n parallelize copies along with the -j/-J options, you should\n prefer using multiple processes instead of multiple threads;\n when using -j/-J, multiple threads in the same process are\n bottlenecked by Python's GIL. Thread and process count can be\n set using the \"parallel_thread_count\" and\n \"parallel_process_count\" boto config options, e.g.:\n gsutil -o \"GSUtil:parallel_process_count=8\" \\\n -o \"GSUtil:parallel_thread_count=1\" \\\n -m cp -j html -r /local/source/dir gs://bucket/path",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-j",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-n": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "No-clobber. When specified, existing files or objects at the\n destination will not be overwritten. Any items that are skipped\n by this option will be reported as being skipped. This option\n will perform an additional GET request to check if an item\n exists before attempting to upload the data. This will save\n retransmitting data, but the additional HTTP requests may make\n small object transfers slower and more expensive.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-n",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes ACLs to be preserved when copying in the cloud. Note\n that this option has performance and cost implications when\n using the XML API, as it requires separate HTTP calls for\n interacting with ACLs. (There are no such performance or cost\n implications when using the -p option with the JSON API.) The\n performance issue can be mitigated to some degree by using\n gsutil -m cp to cause parallel copying. Note that this option\n only works if you have OWNER access to all of the objects that\n are copied.\n You can avoid the additional performance and cost of using\n cp -p if you want all objects in the destination bucket to end\n up with the same ACL by setting a default object ACL on that\n bucket instead of using cp -p. See \"gsutil help defacl\".\n Note that it's not valid to specify both the -a and -p options\n together.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "The -R and -r options are synonymous. Causes directories,\n buckets, and bucket subdirectories to be copied recursively.\n If you neglect to use this option for an upload, gsutil will\n copy any files it finds and skip any directories. Similarly,\n neglecting to specify this option for a download will cause\n gsutil to copy any objects at the current bucket directory\n level, and skip any subdirectories.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<class> The storage class of the destination object(s). If not\n specified, the default storage class of the destination bucket\n is used. Not valid for copying to non-cloud destinations.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-v": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Requests that the version-specific URL for each uploaded object\n be printed. Given this URL you can make future upload requests\n that are safe in the face of concurrent updates, because Google\n Cloud Storage will refuse to perform the update if the current\n object version doesn't match the version-specific URL. See\n \"gsutil help versions\" for more details.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-v",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-z": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<ext,...> Applies gzip content-encoding to any file upload whose\n extension matches the -z extension list. This is useful when\n uploading files with compressible content (such as .js, .css,\n or .html files) because it saves network bandwidth and space\n in Google Cloud Storage, which in turn reduces storage costs.\n When you specify the -z option, the data from your files is\n compressed before it is uploaded, but your actual files are\n left uncompressed on the local disk. The uploaded objects\n retain the Content-Type and name of the original files but are\n given a Content-Encoding header with the value \"gzip\" to\n indicate that the object data stored are compressed on the\n Google Cloud Storage servers.\n For example, the following command:\n gsutil cp -z html -a public-read \\\n cattypes.html tabby.jpeg gs://mycats\n will do all of the following:\n - Upload the files cattypes.html and tabby.jpeg to the bucket\n gs://mycats (cp command)\n - Set the Content-Type of cattypes.html to text/html and\n tabby.jpeg to image/jpeg (based on file extensions)\n - Compress the data in the file cattypes.html (-z option)\n - Set the Content-Encoding for cattypes.html to gzip\n (-z option)\n - Set the ACL for both files to public-read (-a option)\n - If a user tries to view cattypes.html in a browser, the\n browser will know to uncompress the data based on the\n Content-Encoding header and to render it as HTML based on\n the Content-Type header.\n Note that if you download an object with Content-Encoding:gzip\n gsutil will decompress the content before writing the local\n file.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-z",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "cp"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "COMPOSITE UPLOADS": "gsutil can automatically use\n`object composition <https://cloud.google.com/storage/docs/composite-objects>`_\nto perform uploads in parallel for large, local files being uploaded to Google\nCloud Storage. If enabled (see below), a large file will be split into\ncomponent pieces that are uploaded in parallel and then composed in the cloud\n(and the temporary components finally deleted). A file can be broken into as\nmany as 32 component pieces; until this piece limit is reached, the maximum\nsize of each component piece is determined by the variable\n\"parallel_composite_upload_component_size,\" specified in the [GSUtil] section\nof your .boto configuration file (for files that are otherwise too big,\ncomponents are as large as needed to fit into 32 pieces). No additional local\ndisk space is required for this operation.\n\nUsing parallel composite uploads presents a tradeoff between upload\nperformance and download configuration: If you enable parallel composite\nuploads your uploads will run faster, but someone will need to install a\ncompiled crcmod (see \"gsutil help crcmod\") on every machine where objects are\ndownloaded by gsutil or other Python applications. Note that for such uploads,\ncrcmod is required for downloading regardless of whether the parallel\ncomposite upload option is on or not. For some distributions this is easy\n(e.g., it comes pre-installed on macOS), but in other cases some users have\nfound it difficult. Because of this, at present parallel composite uploads are\ndisabled by default. Google is actively working with a number of the Linux\ndistributions to get crcmod included with the stock distribution. Once that is\ndone we will re-enable parallel composite uploads by default in gsutil.\n\nWARNING: Parallel composite uploads should not be used with NEARLINE,\nCOLDLINE, or ARCHIVE storage class buckets, because doing so incurs an early\ndeletion charge for each component object.\n\nWARNING: Parallel composite uploads should not be used in buckets that have a\n`retention policy <https://cloud.google.com/storage/docs/bucket-lock>`_,\nbecause the component pieces cannot be deleted until each has met the\nbucket's minimum retention period.\n\nTo try parallel composite uploads you can run the command:\n\n  gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp bigfile gs://your-bucket\n\nwhere bigfile is larger than 150 MiB. When you do this, notice that the upload\nprogress indicator continuously updates for the file, until all parts of the\nupload complete. If after trying this you want to enable parallel composite\nuploads for all of your future uploads (notwithstanding the caveats mentioned\nearlier), you can uncomment and set the \"parallel_composite_upload_threshold\"\nconfig value in your .boto configuration file to this value.\n\nNote that the crcmod problem only impacts downloads via Python applications\n(such as gsutil). If all users who need to download the data using gsutil or\nother Python applications can install crcmod, or if no Python users will\nneed to download your objects, it makes sense to enable parallel composite\nuploads (see above). For example, if you use gsutil to upload video assets,\nand those assets will only ever be served via a Java application, it would\nmake sense to enable parallel composite uploads on your machine (there are\nefficient CRC32C implementations available in Java).\n\nIf a parallel composite upload fails prior to composition, re-running the\ngsutil command will take advantage of resumable uploads for the components\nthat failed, and the component objects will be deleted after the first\nsuccessful attempt. Any temporary objects that were uploaded successfully\nbefore gsutil failed will still exist until the upload is completed\nsuccessfully. The temporary objects will be named in the following fashion:\n\n  <random ID>/gsutil/tmp/parallel_composite_uploads/for_details_see/gsutil_help_cp/<hash>\n\nwhere <random ID> is a numerical value, and <hash> is an MD5 hash (not related\nto the hash of the contents of the file or object).\n\nTo avoid leaving temporary objects around, you should make sure to check the\nexit status from the gsutil command.  This can be done in a bash script, for\nexample, by doing:\n\n  if ! gsutil cp ./local-file gs://your-bucket/your-object; then\n    << Code that handles failures >>\n  fi\n\nOr, for copying a directory, use this instead:\n\n  if ! gsutil cp -c -L cp.log -r ./dir gs://bucket; then\n    << Code that handles failures >>\n  fi\n\nNote that an object uploaded using parallel composite uploads will have a\nCRC32C hash, but it will not have an MD5 hash (and because of that, users who\ndownload the object must have crcmod installed, as noted earlier). For details\nsee \"gsutil help crc32c\".\n\nParallel composite uploads can be disabled by setting the\n\"parallel_composite_upload_threshold\" variable in the .boto config file to 0.",
        "DESCRIPTION": "The gsutil cp command allows you to copy data between your local file\nsystem and the cloud, copy data within the cloud, and copy data between\ncloud storage providers. For example, to upload all text files from the\nlocal directory to a bucket you could do:\n\n  gsutil cp *.txt gs://my-bucket\n\nSimilarly, you can download text files from a bucket by doing:\n\n  gsutil cp gs://my-bucket/*.txt .\n\nIf you want to copy an entire directory tree you need to use the -r option.\nFor example, to upload the directory tree \"dir\":\n\n  gsutil cp -r dir gs://my-bucket\n\nIf you have a large number of files to transfer you might want to use the\ntop-level gsutil -m option (see \"gsutil help options\"), to perform a\nparallel (multi-threaded/multi-processing) copy:\n\n  gsutil -m cp -r dir gs://my-bucket\n\nYou can pass a list of URLs (one per line) to copy on stdin instead of as\ncommand line arguments by using the -I option. This allows you to use gsutil\nin a pipeline to upload or download files / objects as generated by a program,\nsuch as:\n\n  some_program | gsutil -m cp -I gs://my-bucket\n\nor:\n\n  some_program | gsutil -m cp -I ./download_dir\n\nThe contents of stdin can name files, cloud URLs, and wildcards of files\nand cloud URLs.\n\nNOTE: Shells (like bash, zsh) sometimes attempt to expand wildcards in ways\nthat can be surprising. Also, attempting to copy files whose names contain\nwildcard characters can result in problems. For more details about these\nissues see the section \"POTENTIALLY SURPRISING BEHAVIOR WHEN USING WILDCARDS\"\nunder \"gsutil help wildcards\".",
        "HANDLING": "The cp command will retry when failures occur, but if enough failures happen\nduring a particular copy or delete operation the cp command will skip that\nobject and move on. At the end of the copy run if any failures were not\nsuccessfully retried, the cp command will report the count of failures, and\nexit with non-zero status.\n\nNote that there are cases where retrying will never succeed, such as if you\ndon't have write permission to the destination bucket or if the destination\npath for some objects is longer than the maximum allowed length.\n\nFor more details about gsutil's retry handling, please see\n\"gsutil help retries\".",
        "IN THE CLOUD AND METADATA PRESERVATION": "If both the source and destination URL are cloud URLs from the same\nprovider, gsutil copies data \"in the cloud\" (i.e., without downloading\nto and uploading from the machine where you run gsutil). In addition to\nthe performance and cost advantages of doing this, copying in the cloud\npreserves metadata (like Content-Type and Cache-Control). In contrast,\nwhen you download data from the cloud it ends up in a file, which has\nno associated metadata. Thus, unless you have some way to hold on to\nor re-create that metadata, downloading to a file will not retain the\nmetadata.\n\nCopies spanning locations and/or storage classes cause data to be rewritten\nin the cloud, which may take some time (but still will be faster than\ndownloading and re-uploading). Such operations can be resumed with the same\ncommand if they are interrupted, so long as the command parameters are\nidentical.\n\nNote that by default, the gsutil cp command does not copy the object\nACL to the new object, and instead will use the default bucket ACL (see\n\"gsutil help defacl\"). You can override this behavior with the -p\noption (see OPTIONS below).\n\nOne additional note about copying in the cloud: If the destination bucket has\nversioning enabled, by default gsutil cp will copy only live versions of the\nsource object(s). For example:\n\n  gsutil cp gs://bucket1/obj gs://bucket2\n\nwill cause only the single live version of gs://bucket1/obj to be copied to\ngs://bucket2, even if there are noncurrent versions of gs://bucket1/obj. To\nalso copy noncurrent versions, use the -A flag:\n\n  gsutil cp -A gs://bucket1/obj gs://bucket2\n\nThe top-level gsutil -m flag is disallowed when using the cp -A flag, to\nensure that version ordering is preserved.",
        "NAMES ARE CONSTRUCTED": "The gsutil cp command strives to name objects in a way consistent with how\nLinux cp works, which causes names to be constructed in varying ways depending\non whether you're performing a recursive directory copy or copying\nindividually named objects; and whether you're copying to an existing or\nnon-existent directory.\n\nWhen performing recursive directory copies, object names are constructed that\nmirror the source directory structure starting at the point of recursive\nprocessing. For example, if dir1/dir2 contains the file a/b/c then the\ncommand:\n\n  gsutil cp -r dir1/dir2 gs://my-bucket\n\nwill create the object gs://my-bucket/dir2/a/b/c.\n\nIn contrast, copying individually named files will result in objects named by\nthe final path component of the source files. For example, again assuming\ndir1/dir2 contains a/b/c, the command:\n\n  gsutil cp dir1/dir2/** gs://my-bucket\n\nwill create the object gs://my-bucket/c.\n\nThe same rules apply for downloads: recursive copies of buckets and\nbucket subdirectories produce a mirrored filename structure, while copying\nindividually (or wildcard) named objects produce flatly named files.\n\nNote that in the above example the '**' wildcard matches all names\nanywhere under dir. The wildcard '*' will match names just one level deep. For\nmore details see \"gsutil help wildcards\".\n\nThere's an additional wrinkle when working with subdirectories: the resulting\nnames depend on whether the destination subdirectory exists. For example,\nif gs://my-bucket/subdir exists as a subdirectory, the command:\n\n  gsutil cp -r dir1/dir2 gs://my-bucket/subdir\n\nwill create the object gs://my-bucket/subdir/dir2/a/b/c. In contrast, if\ngs://my-bucket/subdir does not exist, this same gsutil cp command will create\nthe object gs://my-bucket/subdir/a/b/c.\n\nNOTE: If you use the\n`Google Cloud Platform Console <https://console.cloud.google.com>`_\nto create folders, it does so by creating a \"placeholder\" object that ends\nwith a \"/\" character. gsutil skips these objects when downloading from the\ncloud to the local file system, because attempting to create a file that\nends with a \"/\" is not allowed on Linux and macOS. Because of this, it is\nrecommended that you not create objects that end with \"/\" (unless you don't\nneed to be able to download such objects using gsutil).",
        "OBJECT DOWNLOADS": "gsutil uses HTTP Range GET requests to perform \"sliced\" downloads in parallel\nwhen downloading large objects from Google Cloud Storage. This means that disk\nspace for the temporary download destination file will be pre-allocated and\nbyte ranges (slices) within the file will be downloaded in parallel. Once all\nslices have completed downloading, the temporary file will be renamed to the\ndestination file. No additional local disk space is required for this\noperation.\n\nThis feature is only available for Google Cloud Storage objects because it\nrequires a fast composable checksum (CRC32C) that can be used to verify the\ndata integrity of the slices. And because it depends on CRC32C, using sliced\nobject downloads also requires a compiled crcmod (see \"gsutil help crcmod\") on\nthe machine performing the download. If compiled crcmod is not available,\na non-sliced object download will instead be performed.\n\nNOTE: since sliced object downloads cause multiple writes to occur at various\nlocations on disk, this mechanism can degrade performance for disks with slow\nseek times, especially for large numbers of slices. While the default number\nof slices is set small to avoid this problem, you can disable sliced object\ndownload if necessary by setting the \"sliced_object_download_threshold\"\nvariable in the .boto config file to 0.",
        "OVER OS-SPECIFIC FILE TYPES (SYMLINKS, DEVICES, ETC.)": "Please see the section about OS-specific file types in \"gsutil help rsync\".\nWhile that section was written specifically about the rsync command, analogous\npoints apply to the cp command.",
        "TEMP DIRECTORIES": "gsutil writes data to a temporary directory in several cases:\n\n- when compressing data to be uploaded (see the -z and -Z options)\n- when decompressing data being downloaded (when the data has\n  Content-Encoding:gzip, e.g., as happens when uploaded using gsutil cp -z\n  or gsutil cp -Z)\n- when running integration tests (using the gsutil test command)\n\nIn these cases it's possible the temp file location on your system that\ngsutil selects by default may not have enough space. If gsutil runs out of\nspace during one of these operations (e.g., raising\n\"CommandException: Inadequate temp space available to compress <your file>\"\nduring a gsutil cp -z operation), you can change where it writes these\ntemp files by setting the TMPDIR environment variable. On Linux and macOS\nyou can do this either by running gsutil this way:\n\n  TMPDIR=/some/directory gsutil cp ...\n\nor by adding this line to your ~/.bashrc file and then restarting the shell\nbefore running gsutil:\n\n  export TMPDIR=/some/directory\n\nOn Windows 7 you can change the TMPDIR environment variable from Start ->\nComputer -> System -> Advanced System Settings -> Environment Variables.\nYou need to reboot after making this change for it to take effect. (Rebooting\nis not necessary after running the export command on Linux and macOS.)",
        "TO/FROM SUBDIRECTORIES; DISTRIBUTING TRANSFERS ACROSS MACHINES": "You can use gsutil to copy to and from subdirectories by using a command\nlike:\n\n  gsutil cp -r dir gs://my-bucket/data\n\nThis will cause dir and all of its files and nested subdirectories to be\ncopied under the specified destination, resulting in objects with names like\ngs://my-bucket/data/dir/a/b/c. Similarly you can download from bucket\nsubdirectories by using a command like:\n\n  gsutil cp -r gs://my-bucket/data dir\n\nThis will cause everything nested under gs://my-bucket/data to be downloaded\ninto dir, resulting in files with names like dir/data/a/b/c.\n\nCopying subdirectories is useful if you want to add data to an existing\nbucket directory structure over time. It's also useful if you want\nto parallelize uploads and downloads across multiple machines (potentially\nreducing overall transfer time compared with simply running gsutil -m\ncp on one machine). For example, if your bucket contains this structure:\n\n  gs://my-bucket/data/result_set_01/\n  gs://my-bucket/data/result_set_02/\n  ...\n  gs://my-bucket/data/result_set_99/\n\nyou could perform concurrent downloads across 3 machines by running these\ncommands on each machine, respectively:\n\n  gsutil -m cp -r gs://my-bucket/data/result_set_[0-3]* dir\n  gsutil -m cp -r gs://my-bucket/data/result_set_[4-6]* dir\n  gsutil -m cp -r gs://my-bucket/data/result_set_[7-9]* dir\n\nNote that dir could be a local directory on each machine, or it could be a\ndirectory mounted off of a shared file server; whether the latter performs\nacceptably will depend on a number of factors, so we recommend experimenting\nto find out what works best for your computing environment.",
        "TRANSFERS": "Use '-' in place of src_url or dst_url to perform a streaming\ntransfer. For example:\n\n  long_running_computation | gsutil cp - gs://my-bucket/obj\n\nStreaming uploads using the JSON API (see \"gsutil help apis\") are buffered in\nmemory part-way back into the file and can thus retry in the event of network\nor service problems.\n\nStreaming transfers using the XML API do not support resumable\nuploads/downloads. If you have a large amount of data to upload (say, more\nthan 100 MiB) it is recommended that you write the data to a local file and\nthen copy that file to the cloud rather than streaming it (and similarly for\nlarge downloads).\n\nCAUTION: When performing a streaming transfer to or from Cloud Storage,\nneither Cloud Storage nor gsutil compute a checksum. If you require data\nvalidation, use a non-streaming transfer, which performs integrity checking\nautomatically.\n\nNOTE: Streaming transfers are not allowed when the top-level gsutil -m flag\nis used.",
        "VALIDATION": "At the end of every upload or download the gsutil cp command validates that\nthe checksum it computes for the source file/object matches the checksum\nthe service computes. If the checksums do not match, gsutil will delete the\ncorrupted object and print a warning message. This very rarely happens, but\nif it does, please contact gs-team@google.com.\n\nIf you know the MD5 of a file before uploading you can specify it in the\nContent-MD5 header, which will cause the cloud storage service to reject the\nupload if the MD5 doesn't match the value computed by the service. For\nexample:\n\n  % gsutil hash obj\n  Hashing     obj:\n  Hashes [base64] for obj:\n          Hash (crc32c):          lIMoIw==\n          Hash (md5):             VgyllJgiiaRAbyUUIqDMmw==\n\n  % gsutil -h Content-MD5:VgyllJgiiaRAbyUUIqDMmw== cp obj gs://your-bucket/obj\n  Copying file://obj [Content-Type=text/plain]...\n  Uploading   gs://your-bucket/obj:                                182 b/182 B\n\n  If the checksum didn't match the service would instead reject the upload and\n  gsutil would print a message like:\n\n  BadRequestException: 400 Provided MD5 hash \"VgyllJgiiaRAbyUUIqDMmw==\"\n  doesn't match calculated MD5 hash \"7gyllJgiiaRAbyUUIqDMmw==\".\n\nEven if you don't do this gsutil will delete the object if the computed\nchecksum mismatches, but specifying the Content-MD5 header has several\nadvantages:\n\n1. It prevents the corrupted object from becoming visible at all, whereas\n   otherwise it would be visible for 1-3 seconds before gsutil deletes it.\n\n2. If an object already exists with the given name, specifying the\n   Content-MD5 header will cause the existing object never to be replaced,\n   whereas otherwise it would be replaced by the corrupted object and then\n   deleted a few seconds later.\n\n3. It will definitively prevent the corrupted object from being left in\n   the cloud, whereas the gsutil approach of deleting after the upload\n   completes could fail if (for example) the gsutil process gets ^C'd\n   between upload and deletion request.\n\n4. It supports a customer-to-service integrity check handoff. For example,\n   if you have a content production pipeline that generates data to be\n   uploaded to the cloud along with checksums of that data, specifying the\n   MD5 computed by your content pipeline when you run gsutil cp will ensure\n   that the checksums match all the way through the process (e.g., detecting\n   if data gets corrupted on your local disk between the time it was written\n   by your content pipeline and the time it was uploaded to Google Cloud\n   Storage).\n\nNOTE: The Content-MD5 header is ignored for composite objects, because such\nobjects only have a CRC32C checksum."
      }
    },
    "defacl": {
      "capsule": "Get, set, or change default ACL on buckets",
      "commands": {
        "ch": {
          "capsule": "Get, set, or change default ACL on buckets",
          "commands": {},
          "flags": {
            "-d": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Remove all roles associated with the matching entity.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-d",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-f": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Normally gsutil stops at the first error. The -f option causes\n to continue when it encounters errors. With this option the\nutil exit status will be 0 even if some ACLs couldn't be\nanged.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-f",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-g": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or modify a group entity's role.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-g",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or modify a project viewers/editors/owners role.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-u": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or modify a user entity's role.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-u",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "defacl",
            "ch"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"defacl ch\" (or \"defacl change\") command updates the default object\naccess control list for a bucket. The syntax is shared with the \"acl ch\"\ncommand, so see the \"CH\" section of \"gsutil help acl\" for the full help\ndescription.",
            "EXAMPLES": "Grant anyone on the internet READ access by default to any object created\nin the bucket example-bucket:\n\n  gsutil defacl ch -u AllUsers:R gs://example-bucket\n\nNOTE: By default, publicly readable objects are served with a Cache-Control\nheader allowing such objects to be cached for 3600 seconds. If you need to\nensure that updates become visible immediately, you should set a\nCache-Control header of \"Cache-Control:private, max-age=0, no-transform\" on\nsuch objects. For help doing this, see \"gsutil help setmeta\".\n\nAdd the user john.doe@example.com to the default object ACL on bucket\nexample-bucket with READ access:\n\n  gsutil defacl ch -u john.doe@example.com:READ gs://example-bucket\n\nAdd the group admins@example.com to the default object ACL on bucket\nexample-bucket with OWNER access:\n\n  gsutil defacl ch -g admins@example.com:O gs://example-bucket\n\nRemove the group admins@example.com from the default object ACL on bucket\nexample-bucket:\n\n  gsutil defacl ch -d admins@example.com gs://example-bucket\n\nAdd the owners of project example-project-123 to the default object ACL on\nbucket example-bucket with READ access:\n\n  gsutil defacl ch -p owners-example-project-123:R gs://example-bucket\n\nNOTE: You can replace 'owners' with 'viewers' or 'editors' to grant access\nto a project's viewers/editors respectively."
          }
        },
        "get": {
          "capsule": "Get, set, or change default ACL on buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "defacl",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Gets the default ACL text for a bucket, which you can save and edit\nfor use with the \"defacl set\" command."
          }
        },
        "set": {
          "capsule": "Get, set, or change default ACL on buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "defacl",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"defacl set\" command sets default object ACLs for the specified buckets.\nIf you specify a default object ACL for a certain bucket, Google Cloud\nStorage applies the default object ACL to all new objects uploaded to that\nbucket, unless an ACL for that object is separately specified during upload.\n\nSimilar to the \"acl set\" command, the file-or-canned_acl_name names either a\ncanned ACL or the path to a file that contains ACL text. See \"gsutil help\nacl\" for examples of editing and setting ACLs via the acl command. See\n`Predefined ACLs\n<https://cloud.google.com/storage/docs/access-control/lists#predefined-acl>`_\nfor a list of canned ACLs.\n\nSetting a default object ACL on a bucket provides a convenient way to ensure\nnewly uploaded objects have a specific ACL. If you don't set the bucket's\ndefault object ACL, it will default to project-private. If you then upload\nobjects that need a different ACL, you will need to perform a separate ACL\nupdate operation for each object. Depending on how many objects require\nupdates, this could be very time-consuming."
          }
        }
      },
      "flags": {
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Remove all roles associated with the matching entity.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Normally gsutil stops at the first error. The -f option causes\n to continue when it encounters errors. With this option the\nutil exit status will be 0 even if some ACLs couldn't be\nanged.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-g": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or modify a group entity's role.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-g",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or modify a project viewers/editors/owners role.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-u": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or modify a user entity's role.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-u",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "defacl"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CH": "The \"defacl ch\" (or \"defacl change\") command updates the default object\naccess control list for a bucket. The syntax is shared with the \"acl ch\"\ncommand, so see the \"CH\" section of \"gsutil help acl\" for the full help\ndescription.",
        "DESCRIPTION": "The defacl command has three sub-commands:",
        "EXAMPLES": "Grant anyone on the internet READ access by default to any object created\nin the bucket example-bucket:\n\n  gsutil defacl ch -u AllUsers:R gs://example-bucket\n\nNOTE: By default, publicly readable objects are served with a Cache-Control\nheader allowing such objects to be cached for 3600 seconds. If you need to\nensure that updates become visible immediately, you should set a\nCache-Control header of \"Cache-Control:private, max-age=0, no-transform\" on\nsuch objects. For help doing this, see \"gsutil help setmeta\".\n\nAdd the user john.doe@example.com to the default object ACL on bucket\nexample-bucket with READ access:\n\n  gsutil defacl ch -u john.doe@example.com:READ gs://example-bucket\n\nAdd the group admins@example.com to the default object ACL on bucket\nexample-bucket with OWNER access:\n\n  gsutil defacl ch -g admins@example.com:O gs://example-bucket\n\nRemove the group admins@example.com from the default object ACL on bucket\nexample-bucket:\n\n  gsutil defacl ch -d admins@example.com gs://example-bucket\n\nAdd the owners of project example-project-123 to the default object ACL on\nbucket example-bucket with READ access:\n\n  gsutil defacl ch -p owners-example-project-123:R gs://example-bucket\n\nNOTE: You can replace 'owners' with 'viewers' or 'editors' to grant access\nto a project's viewers/editors respectively.",
        "GET": "Gets the default ACL text for a bucket, which you can save and edit\nfor use with the \"defacl set\" command.",
        "SET": "The \"defacl set\" command sets default object ACLs for the specified buckets.\nIf you specify a default object ACL for a certain bucket, Google Cloud\nStorage applies the default object ACL to all new objects uploaded to that\nbucket, unless an ACL for that object is separately specified during upload.\n\nSimilar to the \"acl set\" command, the file-or-canned_acl_name names either a\ncanned ACL or the path to a file that contains ACL text. See \"gsutil help\nacl\" for examples of editing and setting ACLs via the acl command. See\n`Predefined ACLs\n<https://cloud.google.com/storage/docs/access-control/lists#predefined-acl>`_\nfor a list of canned ACLs.\n\nSetting a default object ACL on a bucket provides a convenient way to ensure\nnewly uploaded objects have a specific ACL. If you don't set the bucket's\ndefault object ACL, it will default to project-private. If you then upload\nobjects that need a different ACL, you will need to perform a separate ACL\nupdate operation for each object. Depending on how many objects require\nupdates, this could be very time-consuming."
      }
    },
    "defstorageclass": {
      "capsule": "Get or set the default storage class on buckets",
      "commands": {
        "get": {
          "capsule": "Get or set the default storage class on buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "defstorageclass",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Gets the default storage class for a bucket."
          }
        },
        "set": {
          "capsule": "Get or set the default storage class on buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "defstorageclass",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"defstorageclass set\" command sets the default\n`storage class <https://cloud.google.com/storage/docs/storage-classes>`_ for\nthe specified bucket(s). If you specify a default storage class for a certain\nbucket, Google Cloud Storage applies the default storage class to all new\nobjects uploaded to that bucket, except when the storage class is overridden\nby individual upload requests.\n\nSetting a default storage class on a bucket provides a convenient way to\nensure newly uploaded objects have a specific storage class. If you don't set\nthe bucket's default storage class, it will default to Standard."
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "defstorageclass"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The defstorageclass command has two sub-commands:",
        "GET": "Gets the default storage class for a bucket.",
        "SET": "The \"defstorageclass set\" command sets the default\n`storage class <https://cloud.google.com/storage/docs/storage-classes>`_ for\nthe specified bucket(s). If you specify a default storage class for a certain\nbucket, Google Cloud Storage applies the default storage class to all new\nobjects uploaded to that bucket, except when the storage class is overridden\nby individual upload requests.\n\nSetting a default storage class on a bucket provides a convenient way to\nensure newly uploaded objects have a specific storage class. If you don't set\nthe bucket's default storage class, it will default to Standard."
      }
    },
    "du": {
      "capsule": "Display object size usage",
      "commands": {},
      "flags": {
        "-0": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Ends each output line with a 0 byte rather than a newline. This\nn be useful to make the output more easily machine-readable.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-0",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-X": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Similar to -e, but excludes patterns from the given file. The\ntterns to exclude should be one per line.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-X",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Includes non-current object versions / generations in the listing\nnly useful with a versioning-enabled bucket). Also prints\nneration and metageneration for each listed object.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Includes a grand total at the end of the output.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "A pattern to exclude from reporting. Example: -e \"*.o\" would\nclude any object that ends in \".o\". Can be specified multiple\nmes.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-h": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prints object sizes in human-readable format (e.g., 1 KiB,\n4 MiB, 2GiB, etc.)",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-h",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Displays only the grand total for each argument.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "du"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The du command displays the amount of space (in bytes) being used by the\nobjects in the file or object hierarchy under a given URL. The syntax emulates\nthe Linux du -b command (which stands for disk usage). For example, the\ncommand:\n\ngsutil du -s gs://your-bucket/dir\n\nwill report the total space used by all objects under gs://your-bucket/dir and\nany sub-directories.",
        "EXAMPLES": "To list the size of all objects in a bucket:\n\n  gsutil du gs://bucketname\n\nTo list the size of all objects underneath a prefix:\n\n  gsutil du gs://bucketname/prefix/*\n\nTo print the total number of bytes in a bucket, in human-readable form:\n\n  gsutil du -ch gs://bucketname\n\nTo see a summary of the total bytes in the two given buckets:\n\n  gsutil du -s gs://bucket1 gs://bucket2\n\nTo list the size of all objects in a versioned bucket, including objects that\nare not the latest:\n\n  gsutil du -a gs://bucketname\n\nTo list all objects in a bucket, except objects that end in \".bak\",\nwith each object printed ending in a null byte:\n\n  gsutil du -e \"*.bak\" -0 gs://bucketname\n\nTo get a total of all buckets in a project with a grand total for an entire\nproject:\n\n    gsutil -o GSUtil:default_project_id=project-name du -shc"
      }
    },
    "hash": {
      "capsule": "Calculate file hashes",
      "commands": {},
      "flags": {
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Calculate a CRC32c hash for the file.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-h": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Output hashes in hex format. By default, gsutil uses base64.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-h",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-m": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Calculate a MD5 hash for the file.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-m",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "hash"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The hash command calculates hashes on a local file that can be used to compare\nwith gsutil ls -L output. If a specific hash option is not provided, this\ncommand calculates all gsutil-supported hashes for the file.\n\nNote that gsutil automatically performs hash validation when uploading or\ndownloading files, so this command is only needed if you want to write a\nscript that separately checks the hash for some reason.\n\nIf you calculate a CRC32c hash for the file without a precompiled crcmod\ninstallation, hashing will be very slow. See \"gsutil help crcmod\" for details."
      }
    },
    "help": {
      "capsule": "",
      "commands": {
        "acls": {
          "capsule": "Working With Access Control Lists",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "acls"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "ACCESSING PUBLIC OBJECTS": "Objects with public READ access can be accessed anonymously by gsutil, via\na browser, or via Cloud Storage APIs. For more details on accessing public\nobjects, see:\n\n  https://cloud.google.com/storage/docs/access-public-data",
            "ACL JSON": "When you use a canned ACL, it is translated into an JSON representation\nthat can later be retrieved and edited to specify more fine-grained\ndetail about who can read and write buckets and objects. By running\nthe \"gsutil acl get\" command you can retrieve the ACL JSON, and edit it to\ncustomize the permissions.\n\nAs an example, if you create an object in a bucket that has no default\nobject ACL set and then retrieve the ACL on the object, it will look\nsomething like this:\n\n[\n  {\n    \"entity\": \"group-00b4903a9740e42c29800f53bd5a9a62a2f96eb3f64a4313a115df3f3a776bf7\",\n    \"entityId\": \"00b4903a9740e42c29800f53bd5a9a62a2f96eb3f64a4313a115df3f3a776bf7\",\n    \"role\": \"OWNER\"\n  },\n  {\n    \"entity\": \"group-00b4903a977fd817e9da167bc81306489181a110456bb635f466d71cf90a0d51\",\n    \"entityId\": \"00b4903a977fd817e9da167bc81306489181a110456bb635f466d71cf90a0d51\",\n    \"role\": \"OWNER\"\n  },\n  {\n    \"entity\": \"00b4903a974898cc8fc309f2f2835308ba3d3df1b889d3fc7e33e187d52d8e71\",\n    \"entityId\": \"00b4903a974898cc8fc309f2f2835308ba3d3df1b889d3fc7e33e187d52d8e71\",\n    \"role\": \"READER\"\n  }\n]\n\nThe ACL consists collection of elements, each of which specifies an Entity\nand a Role.  Entities are the way you specify an individual or group of\nindividuals, and Roles specify what access they're permitted.\n\nThis particular ACL grants OWNER to two groups (which means members\nof those groups are allowed to read the object and read and write the ACL),\nand READ permission to a third group. The project groups are (in order)\nthe project owners group, editors group, and viewers group.\n\nThe 64 digit hex identifiers (following any prefixes like \"group-\") used in\nthis ACL are called canonical IDs.  They are used to identify predefined\ngroups associated with the project that owns the bucket: the Project Owners,\nProject Editors, and All Project Team Members groups. For more information\nthe permissions and roles of these project groups, see \"gsutil help projects\".\n\nHere's an example of an ACL specified using the group-by-email and\ngroup-by-domain entities:\n\n\n{\n  \"entity\": \"group-travel-companion-owners@googlegroups.com\"\n  \"email\": \"travel-companion-owners@googlegroups.com\",\n  \"role\": \"OWNER\",\n}\n{\n  \"domain\": \"example.com\",\n  \"entity\": \"domain-example.com\"\n  \"role\": \"READER\",\n},\n\n\nThis ACL grants members of an email group OWNER, and grants READ\naccess to any user in a domain (which must be a Google Apps for Business\ndomain). By applying email group grants to a collection of objects\nyou can edit access control for large numbers of objects at once via\nhttp://groups.google.com. That way, for example, you can easily and quickly\nchange access to a group of company objects when employees join and leave\nyour company (i.e., without having to individually change ACLs across\npotentially millions of objects).",
            "BUCKET VS OBJECT ACLS": "In Google Cloud Storage, the bucket ACL works as follows:\n\n- Users granted READ access are allowed to list the bucket contents and read\n  bucket metadata other than its ACL.\n\n- Users granted WRITE access are allowed READ access and also are allowed to\n  write and delete objects in that bucket, including overwriting previously\n  written objects.\n\n- Users granted OWNER access are allowed WRITE access and also are allowed to\n  read and write the bucket's ACL.\n\nThe object ACL works as follows:\n\n- Users granted READ access are allowed to read the object's data and\n  metadata.\n\n- Users granted OWNER access are allowed READ access and also are allowed to\n  read and write the object's ACL.\n\nA couple of points are worth noting, that sometimes surprise users:\n\n1. There is no WRITE access for objects; attempting to set an ACL with WRITE\n   permission for an object will result in an error.\n\n2. The bucket ACL plays no role in determining who can read objects; only the\n   object ACL matters for that purpose. This is different from how things\n   work in Linux file systems, where both the file and directory permission\n   control file read access. It also means, for example, that someone with\n   OWNER over the bucket may not have read access to objects in the bucket.\n   This is by design, and supports useful cases. For example, you might want\n   to set up bucket ownership so that a small group of administrators have\n   OWNER on the bucket (with the ability to delete data to control storage\n   costs), but not grant those users read access to the object data (which\n   might be sensitive data that should only be accessed by a different\n   specific group of users).",
            "CANNED ACLS": "The simplest way to set an ACL on a bucket or object is using a \"canned\nACL\". The available canned ACLs are:\n\nproject-private\n  Gives permission to the project team based on their roles. Anyone who is\n  part of the team has READ permission, and project owners and project editors\n  have OWNER permission. This is the default ACL for newly created\n  buckets. This is also the default ACL for newly created objects unless the\n  default object ACL for that bucket has been changed. For more details see\n  \"gsutil help projects\".\n\nprivate\n  Gives the requester (and only the requester) OWNER permission for a\n  bucket or object.\n\npublic-read\n  Gives all users (whether logged in or anonymous) READ permission. When\n  you apply this to an object, anyone on the Internet can read the object\n  without authenticating.\n\n  NOTE: By default, publicly readable objects are served with a Cache-Control\n  header allowing such objects to be cached for 3600 seconds. If you need to\n  ensure that updates become visible immediately, you should set a\n  Cache-Control header of \"Cache-Control:private, max-age=0, no-transform\" on\n  such objects. For help doing this, see 'gsutil help setmeta'.\n\n  NOTE: Setting a bucket ACL to public-read will remove all OWNER and WRITE\n  permissions from everyone except the project owner group. Setting an object\n  ACL to public-read will remove all OWNER and WRITE permissions from\n  everyone except the object owner. For this reason, we recommend using\n  the \"acl ch\" command to make these changes; see \"gsutil help acl ch\" for\n  details.\n\npublic-read-write\n  Gives all users READ and WRITE permission. This ACL applies only to buckets.\n  NOTE: Setting a bucket to public-read-write will allow anyone on the\n  Internet to upload anything to your bucket. You will be responsible for this\n  content.\n\n  NOTE: Setting a bucket ACL to public-read-write will remove all OWNER\n  permissions from everyone except the project owner group. Setting an object\n  ACL to public-read-write will remove all OWNER permissions from\n  everyone except the object owner. For this reason, we recommend using\n  the \"acl ch\" command to make these changes; see \"gsutil help acl ch\" for\n  details.\n\nauthenticated-read\n  Gives the requester OWNER permission and gives all authenticated\n  Google account holders READ permission.\n\nbucket-owner-read\n  Gives the requester OWNER permission and gives the bucket owner READ\n  permission. This is used only with objects.\n\nbucket-owner-full-control\n  Gives the requester OWNER permission and gives the bucket owner\n  OWNER permission. This is used only with objects.",
            "DESCRIPTION": "Access Control Lists (ACLs) allow you to control who can read and write\nyour data, and who can read and write the ACLs themselves.\n\nIf not specified at the time an object is uploaded (e.g., via the gsutil cp\n-a option), objects will be created with a default object ACL set on the\nbucket (see \"gsutil help defacl\"). You can replace the ACL on an object\nor bucket using the \"gsutil acl set\" command, or\nmodify the existing ACL using the \"gsutil acl ch\" command (see \"gsutil help\nacl\").",
            "SHARING SCENARIOS": "For more detailed examples how to achieve various useful sharing use\ncases see https://cloud.google.com/storage/docs/collaboration"
          }
        },
        "anon": {
          "capsule": "Accessing Public Data Without Credentials",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "anon"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "You can access publicly readable data through gsutil without obtaining\ncredentials. For example, the gs://uspto-pair bucket contains a number\nof publicly readable objects, so you can run the following command\nwithout first obtaining credentials:\n\n  gsutil ls gs://uspto-pair/applications/0800401*\n\nYou can also download publicly readable objects.\n\nRun the ``gsutil help acls`` command for more details about data protection."
          }
        },
        "apis": {
          "capsule": "Cloud Storage APIs",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "apis"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "CONFIGURING WHICH API IS USED": "To use a certain API for interacting with Google Cloud Storage, you can set\nthe 'prefer_api' variable in the \"GSUtil\" section of .boto config file to\n'xml' or 'json' like so:\n\n  prefer_api = json\n\nThis will cause gsutil to use that API where possible (falling back to the\nother API in cases as noted above). This applies to the gsutil test command\nas well; it will run integration tests against the preferred API.",
            "DESCRIPTION": "Google Cloud Storage offers two APIs: an XML and a JSON API. Gsutil can\ninteract with both APIs. By default, gsutil versions starting with 4.0\ninteract with the JSON API. If it is not possible to perform a command using\none of the APIs (for example, the notification command is not supported in\nthe XML API), gsutil will silently fall back to using the other API. Also,\ngsutil will automatically fall back to using the XML API when interacting\nwith cloud storage providers that only support that API.",
            "PERFORMANCE AND COST DIFFERENCES BETWEEN APIS": "The XML API uses the boto framework.  This framework re-reads downloaded files\nto compute an MD5 hash if one is not present. For objects that do not\ninclude MD5 hashes in their metadata (for example Google Cloud Storage\ncomposite objects), this doubles the bandwidth consumed and elapsed time\nneeded by the download. Therefore, if you are working with composite objects,\nit is recommended that you use the default value for prefer_api.\n\nThe XML API also requires separate calls to get different object and bucket\nmetadata fields, such as ACLs or bucket configuration. Thus, using the JSON\nAPI when possible uses fewer operations (and thus has a lower cost)."
          }
        },
        "crc32c": {
          "capsule": "CRC32C and Installing crcmod",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "crc32c"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "CONFIGURATION": "To determine if the compiled version of crcmod is available in your Python\nenvironment, you can inspect the output of the ``gsutil version`` command for\nthe \"compiled crcmod\" entry:\n\n  $ gsutil version -l\n  ...\n  compiled crcmod: True\n  ...\n\nIf your crcmod library is compiled to a native binary, this value will be\nTrue. If using the pure-Python version, the value will be False.\n\nTo control gsutil's behavior in response to crcmod's status, you can set the\n\"check_hashes\" configuration variable. For details on this variable, see the\nsurrounding comments in your boto configuration file. If \"check_hashes\"\nis not present in your configuration file, rerun ``gsutil config`` to\nregenerate the file.",
            "DESCRIPTION": "Google Cloud Storage provides a cyclic redundancy check (CRC) header that\nallows clients to verify the integrity of object contents. For non-composite\nobjects Google Cloud Storage also provides an MD5 header to allow clients to\nverify object integrity, but for composite objects only the CRC is available.\ngsutil automatically performs integrity checks on all uploads and downloads.\nAdditionally, you can use the ``gsutil hash`` command to calculate a CRC for\nany local file.\n\nThe CRC variant used by Google Cloud Storage is called CRC32C (Castagnoli),\nwhich is not available in the standard Python distribution. The implementation\nof CRC32C used by gsutil is provided by a third-party Python module called\n`crcmod <https://pypi.python.org/pypi/crcmod>`_.\n\nThe crcmod module contains a pure-Python implementation of CRC32C, but using\nit results in very poor performance. A Python C extension is also provided by\ncrcmod, which requires compiling into a binary module for use. gsutil ships\nwith a precompiled crcmod C extension for macOS; for other platforms, see\nthe installation instructions below.\n\nAt the end of each copy operation, the ``gsutil cp`` and ``gsutil rsync``\ncommands validate that the checksum of the source file/object matches the\nchecksum of the destination file/object. If the checksums do not match,\ngsutil will delete the invalid copy and print a warning message. This very\nrarely happens, but if it does, please contact gs-team@google.com.",
            "INSTALLATION": "These installation instructions assume that:\n\n-  You have ``pip`` installed. Consult the `pip installation instructions\n   <https://pip.pypa.io/en/stable/installing/>`_ for details on how\n   to install ``pip``.\n-  Your installation of ``pip`` can be found in your ``PATH`` environment\n   variable. If it cannot, you may need to replace ``pip`` in the commands\n   below with the full path to the executable.\n-  You are installing the crcmod package for use with your system installation\n   of Python, and thus use the ``sudo`` command. If installing crcmod for a\n   different Python environment (e.g. in a virtualenv), you should omit\n   ``sudo`` from the commands below.\n\nCentOS, RHEL, and Fedora\n------------------------\n\nNote that CentOS 6 and similar variants use Python 2.6 by default, which will\nnot run gsutil. To enable Python 2.7 and compile/install crcmod on CentOS 6:\n\n  sudo su  # Run as root; need shell session with Python 2.7 enabled\n  yum install gcc python-devel python-setuptools redhat-rpm-config\n  source /opt/rh/python27/enable  # Make default `python` executable use 2.7.X\n  python -m pip install -U pip  # Upgrade old default version of pip\n  python -m pip uninstall crcmod\n  python -m pip install --no-cache-dir -U crcmod\n  exit  # Exit su session\n\nTo compile and install crcmod on OS versions that use Python 2.7 by default:\n\n  sudo yum install gcc python-devel python-setuptools redhat-rpm-config\n  sudo pip uninstall crcmod\n  sudo pip install --no-cache-dir -U crcmod\n\nDebian and Ubuntu\n-----------------\n\nTo compile and install crcmod:\n\n  sudo apt-get install gcc python-dev python-setuptools\n  sudo pip uninstall crcmod\n  sudo pip install --no-cache-dir -U crcmod\n\nEnterprise SUSE\n-----------------\n\nTo compile and install crcmod:\n\n  sudo zypper install gcc python-devel\n  sudo pip uninstall crcmod\n  sudo pip install --no-cache-dir -U crcmod\n\nmacOS\n-----\n\ngsutil distributes a pre-compiled version of crcmod for macOS, so you shouldn't\nneed to compile and install it yourself. If for some reason the pre-compiled\nversion is not being detected, please let the Google Cloud Storage team know\n(see ``gsutil help support``).\n\nTo compile manually on macOS, you will first need to install\n`XCode <https://developer.apple.com/xcode/>`_ and then run:\n\n  sudo pip install -U crcmod\n\nWindows\n-------\n\nAn installer is available for the compiled version of crcmod from the Python\nPackage Index (PyPi) at the following URL:\n\nhttps://pypi.python.org/pypi/crcmod/1.7\n\nMSI installers are available for the 32-bit versions of Python 2.7.\nMake sure to install to a 32-bit Python directory. If you're using 64-bit\nPython it won't work with 32-bit crcmod, and instead you'll need to install\n32-bit Python in order to use crcmod.\n\nNOTE: If you have installed crcmod and gsutil hasn't detected it, it may have\nbeen installed to the wrong directory. It should be located at\n<python_dir>\\files\\Lib\\site-packages\\crcmod\\\n\nIn some cases the installer will incorrectly install to\n<python_dir>\\Lib\\site-packages\\crcmod\\\n\nManually copying the crcmod directory to the correct location should resolve\nthe issue."
          }
        },
        "creds": {
          "capsule": "Credential Types Supporting Various Use Cases",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "creds"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "gsutil currently supports several types of credentials/authentication, as\nwell as the ability to access public data anonymously (see \"gsutil help anon\"\nfor more on anonymous access). Each of these type of credentials is discussed\nin more detail below, along with information about configuring and using\ncredentials via either the Cloud SDK or standalone installations of gsutil.",
            "SUPPORTED CREDENTIAL TYPES": "gsutil supports several types of credentials (the specific subset depends on\nwhich distribution of gsutil you are using; see above discussion).\n\nOAuth2 User Account:\n  This is the preferred type of credentials for authenticating requests on\n  behalf of a specific user (which is probably the most common use of gsutil).\n  This is the default type of credential that will be created when you run\n  \"gsutil config\" (or \"gcloud init\" for Cloud SDK installs).\n  For more details about OAuth2 authentication, see:\n  https://developers.google.com/accounts/docs/OAuth2#scenarios\n\nHMAC:\n  This type of credential can be used by programs that are implemented using\n  HMAC authentication, which is an authentication mechanism supported by\n  certain other cloud storage service providers. This type of credential can\n  also be used for interactive use when moving data to/from service providers\n  that support HMAC credentials. This is the type of credential that will be\n  created when you run \"gsutil config -a\".\n\n  Note that it's possible to set up HMAC credentials for both Google Cloud\n  Storage and another service provider; or to set up OAuth2 user account\n  credentials for Google Cloud Storage and HMAC credentials for another\n  service provider. To do so, after you run the \"gsutil config\" command (or\n  \"gcloud init\" for Cloud SDK installs), you can edit the generated ~/.boto\n  config file and look for comments for where other credentials can be added.\n\n  For more details about HMAC authentication, see:\n    https://developers.google.com/storage/docs/reference/v1/getting-startedv1#keys\n\nOAuth2 Service Account:\n  This is the preferred type of credential to use when authenticating on\n  behalf of a service or application (as opposed to a user). For example, if\n  you will run gsutil out of a nightly cron job to upload/download data,\n  using a service account allows the cron job not to depend on credentials of\n  an individual employee at your company. This is the type of credential that\n  will be configured when you run \"gsutil config -e\". To configure service\n  account credentials when installed via the Cloud SDK, run \"gcloud auth\n  activate-service-account\".\n\n  It is important to note that a service account is considered an Editor by\n  default for the purposes of API access, rather than an Owner. In particular,\n  the fact that Editors have OWNER access in the default object and\n  bucket ACLs, but the canned ACL options remove OWNER access from\n  Editors, can lead to unexpected results. The solution to this problem is to\n  use \"gsutil acl ch\" instead of \"gsutil acl set <canned-ACL>\" to change\n  permissions on a bucket.\n\n  To set up a service account for use with \"gsutil config -e\" or \"gcloud auth\n  activate-service-account\", see:\n   https://cloud.google.com/storage/docs/authentication#generating-a-private-key\n\n  For more details about OAuth2 service accounts, see:\n    https://developers.google.com/accounts/docs/OAuth2ServiceAccount\n\n  For further information about account roles, see:\n    https://developers.google.com/console/help/#DifferentRoles\n\nGoogle Compute Engine Internal Service Account:\n  This is the type of service account used for accounts hosted by App Engine\n  or Google Compute Engine. Such credentials are created automatically for\n  you on Google Compute Engine when you run the gcloud compute instances\n  creates command and the credentials can be controlled with the --scopes\n  flag.\n\n  For more details about Google Compute Engine service accounts, see:\n    https://developers.google.com/compute/docs/authentication;\n\n  For more details about App Engine service accounts, see:\n    https://developers.google.com/appengine/docs/python/appidentity/overview\n\nService Account Impersonation:\n  Impersonating a service account is useful in scenarios where you need to\n  grant short-term access to specific resources. For example, if you have a\n  bucket of sensitive data that is typically read-only and want to\n  temporarily grant write access through a trusted service account.\n\n  You can specify which service account to use for impersonation by running\n  \"gsutil -i\", \"gsutil config\" and editing the boto configuration file, or\n  \"gcloud config set auth/impersonate_service_account\".\n\n  In order to impersonate, your original credentials need to be granted\n  roles/iam.serviceAccountTokenCreator on the target service account.\n  For more information see:\n    https://cloud.google.com/iam/docs/creating-short-lived-service-account-credentials"
          }
        },
        "dev": {
          "capsule": "Contributing Code to gsutil",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "dev"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "We're open to incorporating gsutil code changes authored by users. Here\nare some guidelines:\n\n1. Before we can accept code submissions, we have to jump a couple of legal\n   hurdles. Please fill out either the individual or corporate Contributor\n   License Agreement:\n\n   - If you are an individual writing original source code and you're\n     sure you own the intellectual property,\n     then you'll need to sign an individual CLA\n     (https://cla.developers.google.com/about/google-individual).\n   - If you work for a company that wants to allow you to contribute your\n     work to gsutil, then you'll need to sign a corporate CLA\n     (https://cla.developers.google.com/about/google-corporate)\n\n   Follow either of the two links above to access the appropriate CLA and\n   instructions for how to sign and return it. Once we receive it, we'll\n   add you to the official list of contributors and be able to accept\n   your patches.\n\n2. If you found a bug or have an idea for a feature enhancement, we suggest\n   you check https://github.com/GoogleCloudPlatform/gsutil/issues to see if it\n   has already been reported by another user. From there you can also\n   subscribe to updates to the issue.\n\n3. If a GitHub issue doesn't already exist, create one about your idea before\n   sending actual code. Often we can discuss the idea and help propose things\n   that could save you later revision work.\n\n4. We tend to avoid adding command line options that are of use to only\n   a very small fraction of users, especially if there's some other way\n   to accommodate such needs. Adding such options complicates the code and\n   also adds overhead to users having to read through an \"alphabet soup\"\n   list of option documentation.\n\n5. While gsutil has a number of features specific to Google Cloud Storage,\n   it can also be used with other cloud storage providers. We're open to\n   including changes for making gsutil support features specific to other\n   providers, as long as those changes don't make gsutil work worse for Google\n   Cloud Storage. If you do make such changes we recommend including someone\n   with knowledge of the specific provider as a code reviewer (see below).\n\n6. You can check out the gsutil code from the GitHub repository:\n\n     https://github.com/GoogleCloudPlatform/gsutil\n\n   To clone a read-only copy of the repository:\n\n     git clone git://github.com/GoogleCloudPlatform/gsutil.git\n\n   To push your own changes to GitHub, click the Fork button on the\n   repository page and clone the repository from your own fork.\n\n7. The gsutil git repository uses git submodules to pull in external modules.\n   After checking out the repository, make sure to also pull the submodules\n   by entering into the gsutil top-level directory and run:\n\n     git submodule update --init --recursive\n\n8. Please make sure to run all tests against your modified code. To\n   do this, change directories into the gsutil top-level directory and run:\n\n     ./gsutil test\n\n   The above tests take a long time to run because they send many requests to\n   the production service. The gsutil test command has a -u argument that will\n   only run unit tests. These run quickly, as they are executed with an\n   in-memory mock storage service implementation. To run only the unit tests,\n   run:\n\n     ./gsutil test -u\n\n   If you made changes to boto, please run the boto tests. For these tests you\n   need to use HMAC credentials (from gsutil config -a), because the current\n   boto test suite doesn't import the OAuth2 handler. You'll also need to\n   install some python modules. Change directories into the boto root\n   directory at third_party/boto and run:\n\n     pip install -r requirements.txt\n\n   (You probably need to run this command using sudo.)\n   Make sure each of the individual installations succeeded. If they don't\n   you may need to run the install command again.\n\n   Then ensure your .boto file has HMAC credentials defined (the boto tests\n   don't load the OAUTH2 plugin), and then change directories into boto's\n   tests directory and run:\n\n     python test.py unit\n     python test.py -t s3 -t gs -t ssl\n\n9. Please consider contributing test code for your change, especially if the\n   change impacts any of the core gsutil code (like the gsutil cp command).\n\n10. Please run the yapf linter with the config files in the root of the GitHub\n    repository.\n\n      yapf -irp .\n\n11. When it's time to send us code, please submit a PR to the `gsutil GitHub\n    repository <https://github.com/GoogleCloudPlatform/gsutil>`_. For help on\n    making GitHub PRs, please refer to this\n    `GitHub help document <https://help.github.com/en/articles/about-pull-requests>`_."
          }
        },
        "encoding": {
          "capsule": "Filename encoding and interoperability problems",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "encoding"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "CONVERTING FILENAMES TO UNICODE": "Open-source tools are available to convert filenames for non-Unicode files.\nFor example, to convert from latin1 (a common Windows encoding) to Unicode,\nyou can use\n`Windows iconv <http://gnuwin32.sourceforge.net/packages/libiconv.htm>`_.\nFor Unix-based systems, you can use\n`libiconv <https://www.gnu.org/software/libiconv/>`_.",
            "CROSS-PLATFORM ENCODING PROBLEMS OF WHICH TO BE AWARE": "Using UTF-8 for all object names and filenames will ensure that gsutil doesn't\nencounter character encoding errors while operating on the files.\nUnfortunately, it's still possible that files uploaded / downloaded this way\ncan have interoperability problems, for a number of reasons unrelated to\ngsutil. For example:\n\n- Windows filenames are case-insensitive, while Google Cloud Storage, Linux,\n  and macOS are not. Thus, for example, if you have two filenames on Linux\n  differing only in case and upload both to Google Cloud Storage and then\n  subsequently download them to Windows, you will end up with just one file\n  whose contents came from the last of these files to be written to the\n  filesystem.\n- macOS performs character encoding decomposition based on tables stored in\n  the OS, and the tables change between Unicode versions. Thus the encoding\n  used by an external library may not match that performed by the OS. It is\n  possible that two object names may translate to a single local filename.\n- Windows console support for Unicode is difficult to use correctly.\n\nFor a more thorough list of such issues see `this presentation\n<http://www.i18nguy.com/unicode/filename-issues-iuc33.pdf>`_\n\nThese problems mostly arise when sharing data across platforms (e.g.,\nuploading data from a Windows machine to Google Cloud Storage, and then\ndownloading from Google Cloud Storage to a machine running macOS).\nUnfortunately these problems are a consequence of the lack of a filename\nencoding standard, and users need to be aware of the kinds of problems that\ncan arise when copying filenames across platforms.\n\nThere is one precaution users can exercise to prevent some of these problems:\nWhen using the Windows console specify wildcards or folders (using the -R\noption) rather than explicitly named individual files.",
            "DESCRIPTION": "To reduce the chance for `filename encoding interoperability problems\n<https://en.wikipedia.org/wiki/Filename#Encoding_indication_interoperability>`_\ngsutil uses `UTF-8 <https://en.wikipedia.org/wiki/UTF-8>`_ character encoding\nwhen uploading and downloading files. Because UTF-8 is in widespread (and\ngrowing) use, for most users nothing needs to be done to use UTF-8. Users with\nfiles stored in other encodings (such as\n`Latin 1 <https://en.wikipedia.org/wiki/ISO/IEC_8859-1>`_) must convert those\nfilenames to UTF-8 before attempting to upload the files.\n\nThe most common place where users who have filenames that use some other\nencoding encounter a gsutil error is while uploading files using the recursive\n(-R) option on the gsutil cp , mv, or rsync commands. When this happens you'll\nget an error like this:\n\n    CommandException: Invalid Unicode path encountered\n    ('dir1/dir2/file_name_with_\\xf6n_bad_chars').\n    gsutil cannot proceed with such files present.\n    Please remove or rename this file and try again.\n\nNote that the invalid Unicode characters have been hex-encoded in this error\nmessage because otherwise trying to print them would result in another\nerror.\n\nIf you encounter such an error you can either remove the problematic file(s)\nor try to rename them and re-run the command. If you have a modest number of\nsuch files the simplest thing to do is to think of a different name for the\nfile and manually rename the file (using local filesystem tools). If you have\ntoo many files for that to be practical you can use a tool to convert the old\ncharacter encoding to UTF-8. One such tool is `native2ascii\n<http://docs.oracle.com/javase/7/docs/technotes/tools/solaris/native2ascii.html>`_.\n\nUnicode errors for valid Unicode filepaths can be caused by lack of Python\nlocale configuration on Linux and Mac OSes. If your file paths are Unicode\nand you get encoding errors, ensure the LANG environment variable is set\ncorrectly. Typically, the LANG variable should be set to something like\n\"en_US.UTF-8\" or \"de_DE.UTF-8\".\n\nNote also that there's no restriction on the character encoding used in file\ncontent - it can be UTF-8, a different encoding, or non-character\ndata (like audio or video content). The gsutil UTF-8 character encoding\nrequirement applies only to filenames.",
            "USING UNICODE FILENAMES ON MACOS": "macOS stores filenames in decomposed form (also known as\n`NFD normalization <https://en.wikipedia.org/wiki/Unicode_equivalence>`_).\nFor example, if a filename contains an accented \"e\" character, that character\nwill be converted to an \"e\" followed by an accent before being saved to the\nfilesystem. As a consequence, it's possible to have different name strings\nfor files uploaded from an operating system that doesn't enforce decomposed\nform (like Ubuntu) from one that does (like macOS).\n\nThe following example shows how this behavior could lead to unexpected\nresults. Say you create a file with non-ASCII characters on Ubuntu. Ubuntu\nstores that filename in its composed form. When you upload the file to the\ncloud, it is stored as named. But if you use gsutil rysnc to bring the file to\na macOS machine and edit the file, then when you use gsutil rsync to bring\nthis version back to the cloud, you end up with two different objects, instead\nof overwriting the original. This is because macOS converted the filename to\na decomposed form, and Cloud Storage sees this as a different object name.",
            "USING UNICODE FILENAMES ON WINDOWS": "Windows support for Unicode in the command shell (cmd.exe or powershell) is\nsomewhat painful, because Windows uses a Windows-specific character encoding\ncalled `cp1252 <https://en.wikipedia.org/wiki/Windows-1252>`_. To use Unicode\ncharacters you need to run this command in the command shell before the first\ntime you use gsutil in that shell:\n\n  chcp 65001\n\nIf you neglect to do this before using gsutil, the progress messages while\nuploading files with Unicode names or listing buckets with Unicode object\nnames will look garbled (i.e., with different glyphs than you expect in the\noutput). If you simply run the chcp command and re-run the gsutil command, the\noutput should no longer look garbled.\n\ngsutil attempts to translate between cp1252 encoding and UTF-8 in the main\nplaces that Unicode encoding/decoding problems have been encountered to date\n(traversing the local file system while uploading files, and printing Unicode\nnames while listing buckets). However, because gsutil must perform\ntranslation, it is likely there are other erroneous edge cases when using\nWindows with Unicode. If you encounter problems, you might consider instead\nusing cygwin (on Windows) or Linux or macOS - all of which support Unicode."
          }
        },
        "encryption": {
          "capsule": "Using Encryption Keys",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "encryption"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "By default, Google Cloud Storage encrypts all object data using Google-managed\nencryption keys and the AES256 encryption algorithm. However, you can also use\none of two encryption key types - those that you supply, called\ncustomer-supplied encryption keys (CSEK), or those that you manage through\nGoogle Cloud KMS, called customer-managed encryption keys (CMEK). Google Cloud\nStorage does not permanently store CSEKs on Google's servers or otherwise\nmanage them. You can read more about these encryption options at\n`CMEK <https://cloud.google.com/storage/docs/encryption/customer-managed-keys>`_\nand\n`CSEK <https://cloud.google.com/storage/docs/encryption/customer-supplied-keys>`_.\n\ngsutil accepts CSEKs for interacting with Google Cloud Storage objects using\nthe JSON API. The keys are provided via the .boto configuration file like so:\n\n  [GSUtil]\n  encryption_key = ...\n  decryption_key1 = ...\n  decryption_key2 = ...\n\nEach key is a RFC 4648 Base64-encoded string of 256 bits of data for use\nwith the AES256 encryption algorithm.\n\ngsutil also accepts CMEKs for encrypting objects using the JSON API. Note that\nif your goal is to use a CMEK to encrypt all newly-written objects in some\nbucket, you should set that bucket's default KMS key (see \"gsutil help kms\").\nAlternatively, you can specify the desired CMEK in the .boto configuration\nfile, but you need only to specify the encryption_key attribute:\n\n  [GSUtil]\n  encryption_key = projects/PROJECT_ID/locations/LOCATION/keyRings/KEYRING/cryptoKeys/KEYNAME\n\nWhile decrypting a CSEK-encrypted object requires supplying the CSEK in one of\nthe decryption_key attributes, this is not necessary for decrypting\nCMEK-encrypted objects because the name of the CMEK used to encrypt the\nobject is stored in the object's metadata.\n\nNote that if you'd like to specify CMEKs on a per-command basis without\nneeding to edit your boto file, you may specify the key name as top-level boto\noption:\n\n  gsutil -o 'GSUtil:encryption_key=projects/PROJECT_ID/locations/LOCATION/keyRings/KEYRING/cryptoKeys/KEYNAME' \\\n         cp /some/local/file gs://my-bucket/",
            "ENCRYPTION BEHAVIOR": "A single encryption_key may be specified in the .boto configuration file,\nand multiple decryption_keys may be specified.\n\nIf encryption_key exists in the .boto configuration file, gsutil ensures that\ndata it writes or copies in Google Cloud Storage is encrypted with that key.\nIf encryption_key is not supplied, gsutil ensures that all data it writes or\ncopies instead uses the destination bucket's default encryption type - if the\nbucket has a default KMS key set, that CMEK is used for encryption; if not,\nGoogle-managed encryption is used.\n\nWARNING: This means that when overwriting or rewriting a CSEK- or\nCMEK-encrypted object, if encryption_key is not specified, gsutil will replace\ncustomer-supplied or customer-managed encryption with Google-managed\nencryption (or customer-managed encryption if the bucket has a default KMS key\nset).\n\nObjects encrypted with CSEKs require the matching decryption key any time they\nare downloaded or copied (via the gsutil cat, cp, mv, or rsync commands).\nViewing the CRC32C or MD5 hashes of such objects (via the ls -L or stat\ncommands) also requires the matching decryption key.\n\nIf a matching key exists in the .boto configuration, gsutil provides it\nas needed in requests to Google Cloud Storage and operates on the\ndecrypted results. gsutil never stores encrypted data on your local disk.\n\ngsutil automatically detects the correct CSEK to use for a cloud object by\ncomparing the key's SHA256 hash against the hash of the CSEK. gsutil considers\nthe configured encryption key and up to 100 decryption keys when searching for\na match.  Decryption keys must be listed in the boto configuration file in\nascending numerical order starting with 1. For example, in the following\nconfiguration:\n\n  decryption_key1 = ...\n  decryption_key9 = ...\n  decryption_key10 = ...\n  decryption_key11 = ...\n\ndecryption_keys 9, 10, and 11 will be ignored because no values for\ndecryption_keys 2 through 8 are provided.",
            "GENERATING CUSTOMER-SUPPLIED ENCRYPTION KEYS": "Generating a 256-bit RFC 4648 Base64-encoded string for use as an encryption\nkey can be easily done with Python:\n\n  python -c 'import base64; import os;\\\n             print(base64.encodestring(os.urandom(32)))'",
            "MANAGING CUSTOMER-SUPPLIED ENCRYPTION KEYS": "Because Google does not store CSEKs, if you lose your CSEK, you will\npermanently lose access to all of your data encrypted with that key.\nTherefore, it is recommended that you back up each encryption key to a secure\nlocation. The .boto configuration file should never be the only place where\nyour key is stored.\n\nAlso, when you create a CSEK, anyone who has the key and access to your\nobjects can read those objects' data. Take precautions to ensure that your\nencryption keys are not shared with untrusted parties.",
            "PERFORMANCE IMPLICATIONS FOR ENCRYPTION KEYS": "When performing an object listing, metadata for objects encrypted with a CSEK\nor CMEK will not include the objects' CRC32C or MD5 hashes. For gsutil\ncommands that require these fields, such as `gsutil ls -L`, gsutil performs an\nadditional metadata GET request for each object encrypted with a CSEK or CMEK.\nTherefore, listing such objects with the -L flag will require one additional\noperation per object, which will be substantially slower than listing objects\nencrypted with Google-owned keys.",
            "RESUMABLE OPERATIONS AND ENCRYPTION KEYS": "If the encryption_key in your boto configuration file changes during a\npartially-completed write or copy operation (for example, if you re-run\na `gsutil cp` object upload after hitting ^C or encountering a network\ntimeout), gsutil will restart the partially-completed operation to ensure\nthat the destination object is written with the new key.",
            "ROTATING KEYS": "To rotate CSEKs, you can change your encryption_key configuration value to a\ndecryption_key configuration value and then use a new value for the\nencryption_key. Then you can use the rewrite command to rotate keys in the\ncloud without downloading and re-uploading the data. For example, if your\ninitial configuration is:\n\n  # Old encryption key\n  encryption_key = keyA...\n\nYou can change it the configuration to:\n\n  # New encryption key\n  encryption_key = keyB...\n  # Encryption key prior to rotation\n  decryption_key1 = keyA...\n\nand rotate the encryption key on an object by running:\n\n  gsutil rewrite -k gs://bucket/object temp-file\n\nYou can also do this to apply different CMEKs, but you do not need to add\nCMEKs to decryption_key configuration values. Similarly, you can switch\nbetween CSEK and CMEK encryption depending on the type of key specified in the\nencryption_key configuration value.",
            "SECURITY IMPLICATIONS FOR CUSTOMER-SUPPLIED ENCRYPTION KEYS": "gsutil always sends encryption keys over HTTPS, so your CSEKs will never be\nvisible on the network. However, the keys are present in your .boto\nconfiguration file as well as in the memory of the machine executing gsutil.\nTherefore, if this file or the machine are compromised, your encryption keys\nshould also be considered compromised, and you should immediately perform\nkey rotation for all objects encrypted with the compromised keys.",
            "XML API UNSUPPORTED": "gsutil does not support using the XML API to interact with encrypted objects,\nand will use the JSON API if any encryption_key or decryption_keys are\nspecified in configuration."
          }
        },
        "metadata": {
          "capsule": "Working With Object Metadata",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "metadata"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "CACHE-CONTROL": "Another commonly set piece of metadata is Cache-Control, which allows\nyou to control whether and for how long browser and Internet caches are\nallowed to cache your objects. Cache-Control only applies to objects with\na public-read ACL. Non-public data are not cacheable.\n\nHere's an example of uploading a set of objects to allow caching:\n\n  gsutil -h \"Cache-Control:public,max-age=3600\" cp -a public-read \\\n         -r html gs://bucket/html\n\nThis command would upload all files in the html directory (and subdirectories)\nand make them publicly readable and cacheable, with cache expiration of\none hour.\n\nNote that if you allow caching, at download time you may see older versions\nof objects after uploading a newer replacement object. Note also that because\nobjects can be cached at various places on the Internet there is no way to\nforce a cached object to expire globally (unlike the way you can force your\nbrowser to refresh its cache). If you want to prevent serving cached versions\nof publicly readable objects, set \"Cache-Control:no-cache, max-age=0\" on the\nobject. You can do this with a command such as:\n\n  gsutil -h \"Cache-Control:no-cache,max-age=0\" \\\n         cp -a public-read file.png gs://your-bucket\n\nAnother use of Cache-Control is through the \"no-transform\" value,\nwhich instructs Google Cloud Storage to not apply any content transformations\nbased on specifics of a download request, such as removing gzip\ncontent-encoding for incompatible clients.  Note that this parameter is only\nrespected by the XML API. The Google Cloud Storage JSON API respects only the\npublic, private, no-cache, and max-age Cache-Control parameters.\n\nFor details about how to set the Cache-Control metadata see\n\"gsutil help setmeta\".",
            "CONTENT-DISPOSITION": "You can set Content-Disposition on your objects, to specify presentation\ninformation about the data being transmitted. Here's an example:\n\n  gsutil -h 'Content-Disposition:attachment; filename=filename.ext' \\\n         cp -r attachments gs://bucket/attachments\n\nSetting the Content-Disposition allows you to control presentation style\nof the content, for example determining whether an attachment should be\nautomatically displayed vs should require some form of action from the user to\nopen it.  See https://tools.ietf.org/html/rfc6266\nfor more details about the meaning of Content-Disposition.",
            "CONTENT-ENCODING": "You can specify a Content-Encoding to indicate that an object is compressed\n(for example, with gzip compression) while maintaining its Content-Type.\nYou will need to ensure that the files have been compressed using the\nspecified Content-Encoding before using gsutil to upload them. Consider the\nfollowing example for Linux:\n\n  echo \"Highly compressible text\" | gzip > foo.txt\n  gsutil -h \"Content-Encoding:gzip\" \\\n         -h \"Content-Type:text/plain\" \\\n         cp foo.txt gs://bucket/compressed\n\nNote that this is different from uploading a gzipped object foo.txt.gz with\nContent-Type: application/x-gzip because most browsers are able to\ndynamically decompress and process objects served with Content-Encoding: gzip\nbased on the underlying Content-Type.\n\nFor compressible content, using Content-Encoding: gzip saves network and\nstorage costs, and improves content serving performance. However, for content\nthat is already inherently compressed (archives and many media formats, for\ninstance) applying another level of compression via Content-Encoding is\ntypically detrimental to both object size and performance and should be\navoided.\n\nNote also that gsutil provides an easy way to cause content to be compressed\nand stored with Content-Encoding: gzip: see the -z and -Z options in\n\"gsutil help cp\".",
            "CONTENT-TYPE": "The most commonly set metadata is Content-Type (also known as MIME type),\nwhich allows browsers to render the object properly. gsutil sets the\nContent-Type automatically at upload time, based on each filename extension.\nFor example, uploading files with names ending in .txt will set Content-Type\nto text/plain. If you're running gsutil on Linux or macOS and would prefer to\nhave content type set based on naming plus content examination, see the\nuse_magicfile configuration variable in the .boto configuration file (See\nalso \"gsutil help config\"). In general, using use_magicfile is more robust\nand configurable, but is not available on Windows.\n\nIf you specify Content-Type with -h when uploading content (like the\nexample gsutil command given in the previous section), it overrides the\nContent-Type that would have been set based on filename extension or content.\nThis can be useful if the Content-Type detection algorithm doesn't work as\ndesired for some of your files.",
            "CURRENTLY SET METADATA": "You can see what metadata is currently set on an object by using:\n\n  gsutil ls -L gs://the_bucket/the_object",
            "DESCRIPTION": "You can add your own custom metadata (e.g,. for use by your application)\nto a Google Cloud Storage object by using \"x-goog-meta\" with -h. For example:\n\n  gsutil -h x-goog-meta-reviewer:jane cp mycode.java gs://bucket/reviews\n\nYou can add multiple differently-named custom metadata fields to each object.",
            "FIELDS; FIELD VALUES": "You can't set some metadata fields, such as ETag and Content-Length. The\nfields you can set are:\n\n- Cache-Control\n- Content-Disposition\n- Content-Encoding\n- Content-Language\n- Content-Type\n- Custom metadata\n\nField names are case-insensitive.\n\nAll fields and their values must consist only of ASCII characters, with the\nexception of values for x-goog-meta- fields, which may contain arbitrary\nUnicode values. Note that when setting metadata using the XML API, which sends\ncustom metadata as HTTP headers, Unicode characters will be encoded using\nUTF-8, then url-encoded to ASCII. For example:\n\n  gsutil setmeta -h \"x-goog-meta-foo: \" gs://bucket/object\n\nwould store the custom metadata key-value pair of \"foo\" and \"%C3%A3\".\nSubsequently, running \"ls -L\" using the JSON API to list the object's metadata\nwould print \"%C3%A3\", while \"ls -L\" using the XML API would url-decode this\nvalue automatically, printing the character \"\".",
            "OF METADATA": "Objects can have associated metadata, which control aspects of how\nGET requests are handled, including Content-Type, Cache-Control,\nContent-Disposition, and Content-Encoding (discussed in more detail in\nthe subsections below). In addition, you can set custom metadata that\ncan be used by applications (e.g., tagging that particular objects possess\nsome property).\n\nThere are two ways to set metadata on objects:\n\n- At upload time you can specify one or more metadata properties to\n  associate with objects, using the gsutil -h option.  For example, the\n  following command would cause gsutil to set the Content-Type and\n  Cache-Control for each of the files being uploaded:\n\n    gsutil -h \"Content-Type:text/html\" \\\n           -h \"Cache-Control:public, max-age=3600\" cp -r images \\\n           gs://bucket/images\n\n  Note that -h is an option on the gsutil command, not the cp sub-command.\n\n- You can set or remove metadata fields from already uploaded objects using\n  the gsutil setmeta command. See \"gsutil help setmeta\".\n\nMore details about specific pieces of metadata are discussed below."
          }
        },
        "naming": {
          "capsule": "Object and Bucket Naming",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "naming"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "NAME REQUIREMENTS": "Object names can contain any sequence of Unicode characters, of length 1-1024\nbytes when UTF-8 encoded. Object names must not contain CarriageReturn,\nCarriageReturnLineFeed, or the XML-disallowed surrogate blocks (xFFFE\nor xFFFF).\n\nWe strongly recommend that you abide by the following object naming\nconventions:\n\n- Avoid using control characters that are illegal in XML 1.0 in your object\n  names (#x7F-#x84 and #x86-#x9F). These characters will cause XML listing\n  issues when you try to list your objects.\n\n- Avoid using \"#\" in your object names. gsutil interprets object names ending\n  with #<numeric string> as version identifiers, so including \"#\" in object\n  names can make it difficult or impossible to perform various operations on\n  such objects using gsutil (see 'gsutil help versions').\n\n- Avoid using \"[\", \"]\", \"*\", or \"?\" in your object names. gsutil interprets\n  these characters as wildcards, so including any of these characters in\n  object names can make it difficult or impossible to perform various wildcard\n  operations using gsutil (see 'gsutil help wildcards').\n\nSee also 'gsutil help encoding' about file/object name encoding requirements\nand potential interoperability concerns.",
            "NAMED BUCKETS": "You can carve out parts of the Google Cloud Storage bucket name space\nby creating buckets with domain names (like \"example.com\").\n\nBefore you can create a bucket name containing one or more '.' characters,\nthe following rules apply:\n\n- If the name is a syntactically valid DNS name ending with a\n  currently-recognized top-level domain (such as .com), you will be required\n  to verify domain ownership.\n- Otherwise you will be disallowed from creating the bucket.\n\nIf your project needs to use a domain-named bucket, you need to have\na team member both verify the domain and create the bucket. This is\nbecause Google Cloud Storage checks for domain ownership against the\nuser who creates the bucket, so the user who creates the bucket must\nalso be verified as an owner or manager of the domain.\n\nTo verify as the owner or manager of a domain, use the Google Webmaster\nTools verification process. The Webmaster Tools verification process\nprovides three methods for verifying an owner or manager of a domain:\n\n1. Adding a special Meta tag to a site's homepage.\n2. Uploading a special HTML file to a site.\n3. Adding a DNS TXT record to a domain's DNS configuration.\n\nMeta tag verification and HTML file verification are easier to perform and\nare probably adequate for most situations. DNS TXT record verification is\na domain-based verification method that is useful in situations where a\nsite wants to tightly control who can create domain-named buckets. Once\na site creates a DNS TXT record to verify ownership of a domain, it takes\nprecedence over meta tag and HTML file verification. For example, you might\nhave two IT staff members who are responsible for managing your site, called\n\"example.com.\" If they complete the DNS TXT record verification, only they\nwould be able to create buckets called \"example.com\", \"reports.example.com\",\n\"downloads.example.com\", and other domain-named buckets.\n\nSite-Based Verification\n-----------------------\n\nIf you have administrative control over the HTML files that make up a site,\nyou can use one of the site-based verification methods to verify that you\ncontrol or own a site. When you do this, Google Cloud Storage lets you\ncreate buckets representing the verified site and any sub-sites - provided\nnobody has used the DNS TXT record method to verify domain ownership of a\nparent of the site.\n\nAs an example, assume that nobody has used the DNS TXT record method to verify\nownership of the following domains: abc.def.example.com, def.example.com,\nand example.com. In this case, Google Cloud Storage lets you create a bucket\nnamed abc.def.example.com if you verify that you own or control any of the\nfollowing sites:\n\n  http://abc.def.example.com\n  http://def.example.com\n  http://example.com\n\nDomain-Based Verification\n-------------------------\n\nIf you have administrative control over a domain's DNS configuration, you can\nuse the DNS TXT record verification method to verify that you own or control a\ndomain. When you use the domain-based verification method to verify that you\nown or control a domain, Google Cloud Storage lets you create buckets that\nrepresent any subdomain under the verified domain. Furthermore, Google Cloud\nStorage prevents anybody else from creating buckets under that domain unless\nyou add their name to the list of verified domain owners or they have verified\ntheir domain ownership by using the DNS TXT record verification method.\n\nFor example, if you use the DNS TXT record verification method to verify your\nownership of the domain example.com, Google Cloud Storage will let you create\nbucket names that represent any subdomain under the example.com domain, such\nas abc.def.example.com, example.com/music/jazz, or abc.example.com/music/jazz.\n\nUsing the DNS TXT record method to verify domain ownership supersedes\nverification by site-based verification methods. For example, if you\nuse the Meta tag method or HTML file method to verify domain ownership\nof http://example.com, but someone else uses the DNS TXT record method\nto verify ownership of the example.com domain, Google Cloud Storage will\nnot allow you to create a bucket named example.com. To create the bucket\nexample.com, the domain owner who used the DNS TXT method to verify domain\nownership must add you to the list of verified domain owners for example.com.\n\nThe DNS TXT record verification method is particularly useful if you manage\na domain for a large organization that has numerous subdomains because it\nlets you control who can create buckets representing those domain names.\n\nNote: If you use the DNS TXT record verification method to verify ownership of\na domain, you cannot create a CNAME record for that domain. RFC 1034 disallows\ninclusion of any other resource records if there is a CNAME resource record\npresent. If you want to create a CNAME resource record for a domain, you must\nuse the Meta tag verification method or the HTML file verification method."
          }
        },
        "options": {
          "capsule": "Top-Level Command-Line Options",
          "commands": {},
          "flags": {
            "-D": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Shows HTTP requests/headers and additional debug info needed when\nsting support requests, including exception stack traces.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-D",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-DD": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Shows HTTP requests/headers, additional debug info,\nception stack traces, plus HTTP upstream payload.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-DD",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-h": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Allows you to specify certain HTTP headers, for example:\ngsutil -h \"Cache-Control:public,max-age=3600\" \\\n -h \"Content-Type:text/html\" cp ...\nte that you need to quote the headers/values that\nntain spaces (such as \"Content-Disposition: attachment;\nlename=filename.ext\"), to avoid having the shell split them\nto separate arguments.\ne following headers are stored as object metadata and used\n future requests on the object:\nCache-Control\nContent-Disposition\nContent-Encoding\nContent-Language\nContent-Type\ne following headers are used to check data integrity:\nContent-MD5\nutil also supports custom metadata headers with a matching\noud Storage Provider prefix, such as:\nx-goog-meta-\nte that for gs:// URLs, the Cache Control header is specific to\ne API being used. The XML API will accept any cache control\naders and return them during object downloads. The JSON API\nspects only the public, private, no-cache, and max-age cache\nntrol headers, and may add its own no-transform directive even\n it was not specified. See 'gsutil help apis' for more\nformation on gsutil's interaction with APIs.\ne also \"gsutil help setmeta\" for the ability to set metadata\nelds on objects after they have been uploaded.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-h",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-i": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Allows you to use the configured credentials to impersonate a\nrvice account, for example:\ngsutil -i \"service-account@google.com\" ls gs://pub\nte that this setting will be ignored by the XML API and S3. See\nsutil help creds' for more information on impersonating service\ncounts.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-i",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-m": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Causes supported operations (acl ch, acl set, cp, mv, rm, rsync,\nd setmeta) to run in parallel. This can significantly improve\nrformance if you are performing operations on a large number of\nles over a reasonably fast network connection.\nutil performs the specified operation using a combination of\nlti-threading and multi-processing, using a number of threads\nd processors determined by the parallel_thread_count and\nrallel_process_count values set in the boto configuration\nle. You might want to experiment with these values, as the\nst values can vary based on a number of factors, including\ntwork speed, number of CPUs, and available memory.\ning the -m option may make your performance worse if you\ne using a slower network, such as the typical network speeds\nfered by non-business home network plans. It can also make\nur performance worse for cases that perform all operations\ncally (e.g., gsutil rsync, where both source and destination\nLs are on the local disk), because it can \"thrash\" your local\nsk.\n a download or upload operation using parallel transfer fails\nfore the entire transfer is complete (e.g. failing after 300 of\n00 files have been transferred), you will need to restart the\ntire transfer.\nso, although most commands will normally fail upon encountering\n error when the -m flag is disabled, all commands will\nntinue to try all operations when -m is enabled with multiple\nreads or processes, and the number of failed operations (if any)\nll be reported as an exception at the end of the command's\necution.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-m",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-o": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Set/override values in the boto configuration value, in the format\nection>:<name>=<value>, e.g. gsutil -o \"Boto:proxy=host\" ...\nis will not pass the option to gsutil integration tests, which\nn in a separate process.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-o",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-q": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Causes gsutil to perform operations quietly, i.e., without\nporting progress indicators of files being copied or removed,\nc. Errors are still reported. This option can be useful for\nnning gsutil from a cron job that logs its output to a file, for\nich the only information desired in the log is failures.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-q",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-u": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Allows you to specify a user project to be billed for the request.\nr example:\ngsutil -u \"bill-this-project\" cp ...",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-u",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "options"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "gsutil supports separate options for the top-level gsutil command and\nthe individual sub-commands (like cp, rm, etc.) The top-level options\ncontrol behavior of gsutil that apply across commands. For example, in\nthe command:\n\n  gsutil -m cp -p file gs://bucket/obj\n\nthe -m option applies to gsutil, while the -p option applies to the cp\nsub-command."
          }
        },
        "prod": {
          "capsule": "Scripting Production Transfers",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "prod"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "BACKGROUND ON RESUMABLE TRANSFERS": "First, it's helpful to understand gsutil's resumable transfer mechanism,\nand how your script needs to be implemented around this mechanism to work\nreliably. gsutil uses resumable transfer support when you attempt to download\na file of any size or to upload a file larger than a configurable threshold\n(by default, this threshold is 8 MiB). If a transfer fails partway through\n(e.g., because of an intermittent network problem), gsutil uses a truncated\nrandomized binary exponential backoff-and-retry strategy that by default will\nretry transfers up to 23 times over a 10 minute period of time (see\n\"gsutil help retries\" for details). If the transfer fails each of these\nattempts with no intervening progress, gsutil gives up on the transfer, but\nkeeps a \"tracker\" file for it in a configurable location (the default\nlocation is ~/.gsutil/, in a file named by a combination of the SHA1 hash of\nthe name of the bucket and object being transferred and the last 16\ncharacters of the file name). When transfers fail in this fashion, you can\nrerun gsutil at some later time (e.g., after the networking problem has been\nresolved), and the resumable transfer picks up where it left off.",
            "DESCRIPTION": "If you use gsutil in large production tasks (such as uploading or\ndownloading many GiBs of data each night), there are a number of things\nyou can do to help ensure success. Specifically, this section discusses\nhow to script large production tasks around gsutil's resumable transfer\nmechanism.",
            "SCRIPTING DATA TRANSFER TASKS": "To script large production data transfer tasks around this mechanism,\nyou can implement a script that runs periodically, determines which file\ntransfers have not yet succeeded, and runs gsutil to copy them. Below,\nwe offer a number of suggestions about how this type of scripting should\nbe implemented:\n\n1. When resumable transfers fail without any progress 23 times in a row\n   over the course of up to 10 minutes, it probably won't work to simply\n   retry the transfer immediately. A more successful strategy would be to\n   have a cron job that runs every 30 minutes, determines which transfers\n   need to be run, and runs them. If the network experiences intermittent\n   problems, the script picks up where it left off and will eventually\n   succeed (once the network problem has been resolved).\n\n2. If your business depends on timely data transfer, you should consider\n   implementing some network monitoring. For example, you can implement\n   a task that attempts a small download every few minutes and raises an\n   alert if the attempt fails for several attempts in a row (or more or less\n   frequently depending on your requirements), so that your IT staff can\n   investigate problems promptly. As usual with monitoring implementations,\n   you should experiment with the alerting thresholds, to avoid false\n   positive alerts that cause your staff to begin ignoring the alerts.\n\n3. There are a variety of ways you can determine what files remain to be\n   transferred. We recommend that you avoid attempting to get a complete\n   listing of a bucket containing many objects (e.g., tens of thousands\n   or more). One strategy is to structure your object names in a way that\n   represents your transfer process, and use gsutil prefix wildcards to\n   request partial bucket listings. For example, if your periodic process\n   involves downloading the current day's objects, you could name objects\n   using a year-month-day-object-ID format and then find today's objects by\n   using a command like gsutil ls \"gs://bucket/2011-09-27-*\". Note that it\n   is more efficient to have a non-wildcard prefix like this than to use\n   something like gsutil ls \"gs://bucket/*-2011-09-27\". The latter command\n   actually requests a complete bucket listing and then filters in gsutil,\n   while the former asks Google Storage to return the subset of objects\n   whose names start with everything up to the \"*\".\n\n   For data uploads, another technique would be to move local files from a \"to\n   be processed\" area to a \"done\" area as your script successfully copies\n   files to the cloud. You can do this in parallel batches by using a command\n   like:\n\n     gsutil -m cp -r to_upload/subdir_$i gs://bucket/subdir_$i\n\n   where i is a shell loop variable. Make sure to check the shell $status\n   variable is 0 after each gsutil cp command, to detect if some of the copies\n   failed, and rerun the affected copies.\n\n   With this strategy, the file system keeps track of all remaining work to\n   be done.\n\n4. If you have really large numbers of objects in a single bucket\n   (say hundreds of thousands or more), you should consider tracking your\n   objects in a database instead of using bucket listings to enumerate\n   the objects. For example this database could track the state of your\n   downloads, so you can determine what objects need to be downloaded by\n   your periodic download script by querying the database locally instead\n   of performing a bucket listing.\n\n5. Make sure you don't delete partially downloaded temporary files after a\n   transfer fails: gsutil picks up where it left off (and performs a hash\n   of the final downloaded content to ensure data integrity), so deleting\n   partially transferred files will cause you to lose progress and make\n   more wasteful use of your network.\n\n6. If you have a fast network connection, you can speed up the transfer of\n   large numbers of files by using the gsutil -m (multi-threading /\n   multi-processing) option. Be aware, however, that gsutil doesn't attempt to\n   keep track of which files were downloaded successfully in cases where some\n   files failed to download. For example, if you use multi-threaded transfers\n   to download 100 files and 3 failed to download, it is up to your scripting\n   process to determine which transfers didn't succeed, and retry them. A\n   periodic check-and-run approach like outlined earlier would handle this\n   case.\n\n   If you use parallel transfers (gsutil -m) you might want to experiment with\n   the number of threads being used (via the parallel_thread_count setting\n   in the .boto config file). By default, gsutil uses 10 threads for Linux\n   and 24 threads for other operating systems. Depending on your network\n   speed, available memory, CPU load, and other conditions, this may or may\n   not be optimal. Try experimenting with higher or lower numbers of threads\n   to find the best number of threads for your environment."
          }
        },
        "projects": {
          "capsule": "Working With Projects",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "projects"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "This section discusses how to work with projects in Google Cloud Storage.",
            "HOW PROJECT MEMBERSHIP IS REFLECTED IN BUCKET ACLS": "When you create a bucket without specifying an ACL the bucket is given a\n\"project-private\" ACL, which grants the permissions described in the previous\nsection. Here's an example of such an ACL:\n\n  [\n    {\n      \"entity\": \"project-owners-12345\",\n      \"projectTeam\": {\n        \"projectNumber\": \"12345\",\n        \"team\": \"owners\"\n      },\n      \"role\": \"OWNER\"\n    },\n    {\n      \"entity\": \"project-editors-12345\",\n      \"projectTeam\": {\n        \"projectNumber\": \"12345\",\n        \"team\": \"editors\"\n      },\n      \"role\": \"OWNER\"\n    },\n    {\n      \"entity\": \"project-viewers-12345\",\n      \"projectTeam\": {\n        \"projectNumber\": \"12345\",\n        \"team\": \"viewers\"\n      },\n      \"role\": \"READER\"\n    }\n  ]\n\nYou can edit the bucket ACL if you want to (see \"gsutil help acl\"),\nbut for many cases you'll never need to, and instead can change group\nmembership via the\n`Google Cloud Platform Console <https://cloud.google.com/console#/project>`_.",
            "IDENTIFYING PROJECTS WHEN CREATING AND LISTING BUCKETS": "When you create a bucket you need to provide the project ID that will own\nthe bucket you want to create, and when you want to list your buckets, you\nneed to provide the project ID that you want to list. By default, gsutil uses\nthe default_project_id in your ~/.boto configuration file. You can\ninstead use the -p option (e.g., \"gsutil mb -p <project-id>\" or\n\"gsutil ls -p <project-id>\"). The project ID you use must be either the\nproject ID or the project number from the Google Cloud Platform Console\ndashboard.\n\nNote that the project name is a user-friendly name that you can choose. It is\nnot the same thing as the project ID that is required by the gsutil mb and ls\ncommands.",
            "PROJECT MEMBERS AND PERMISSIONS": "There are three groups of users associated with each project:\n\n- Project Owners are allowed to list, create, and delete buckets,\n  and can also perform administrative tasks like adding and removing team\n  members and changing billing. The project owners group is the owner\n  of all buckets within a project, regardless of who may be the original\n  bucket creator.\n\n- Project Editors are allowed to list, create, and delete buckets.\n\n- Project Viewers are allowed to list buckets within a project.\n\nThese project groups make it easy to set up a bucket and start uploading\nobjects with access control appropriate for a project at your company, as\nthe three group memberships can be configured by your administrative staff.\nControl over projects and their associated memberships is provided by the\n`Google Cloud Platform Console <https://cloud.google.com/console#/project>`_."
          }
        },
        "retries": {
          "capsule": "Retry Handling Strategy",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "retries"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "STRATEGY": "There are a number of reasons that gsutil operations can fail; some are not\nretryable, and require that the user take some action, for example:\n\n- Invalid credentials\n- Network unreachable because of a proxy configuration problem\n- Access denied, because the bucket or object you are trying to use has an\n  ACL that doesn't permit the action you're trying to perform.\n\nIn other cases errors are retryable - transient network failures and HTTP 429\nand 5xx error codes. For these cases, gsutil will retry using a truncated\nbinary exponential backoff strategy:\n\n- Wait a random period between [0..1] seconds and retry;\n- If that fails, wait a random period between [0..2] seconds and retry;\n- If that fails, wait a random period between [0..4] seconds and retry;\n- And so on, up to a configurable maximum number of retries (default = 23),\nwith each retry period bounded by a configurable maximum period of time\n(default = 60 seconds).\n\nThus, by default, gsutil will retry 23 times over 1+2+4+8+16+32+60... seconds\nfor about 10 minutes. You can adjust the number of retries and maximum delay\nof any individual retry by editing the num_retries and max_retry_delay\nconfiguration variables in the \"[Boto]\" section of the .boto config file.\nMost users shouldn't need to change these values.\n\nFor data transfers (the gsutil cp and rsync commands), gsutil provides\nadditional retry functionality, in the form of resumable transfers.\nEssentially, a transfer that was interrupted because of a transient error\ncan be restarted without starting over from scratch. For more details\nabout this, see the \"RESUMABLE TRANSFERS\" section of \"gsutil help cp\"."
          }
        },
        "security": {
          "capsule": "Security and Privacy Considerations",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "security"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "ACCESS CONTROL LISTS": "Unless you specify a different ACL (e.g., via the gsutil cp -a option), by\ndefault objects written to a bucket use the default object ACL on that bucket.\nUnless you modify that ACL (e.g., via the gsutil defacl command), by default\nit will allow all project editors write access to the object and read/write\naccess to the object's metadata and will allow all project viewers read\naccess to the object.\n\nThe Google Cloud Storage access control system includes the ability to\nspecify that objects are publicly readable. Make sure you intend for any\nobjects you write with this permission to be public. Once \"published\", data\non the Internet can be copied to many places, so it's effectively impossible\nto regain read control over an object written with this permission.\n\nThe Google Cloud Storage access control system includes the ability to\nspecify that buckets are publicly writable. While configuring a bucket this\nway can be convenient for various purposes, we recommend against using this\npermission - it can be abused for distributing illegal content, viruses, and\nother malware, and the bucket owner is legally and financially responsible\nfor the content stored in their buckets. If you need to make content\navailable to customers who don't have Google accounts consider instead using\nsigned URLs (see \"gsutil help signurl\").",
            "DATA PRIVACY": "Google will never ask you to share your credentials, password, or other\nsecurity-sensitive information. Beware of potential phishing scams where\nsomeone attempts to impersonate Google and asks for such information.",
            "DESCRIPTION": "This help section provides details about various precautions taken by gsutil\nto protect data security, as well as recommendations for how customers should\nsafeguard security.",
            "ENCRYPTION AT REST": "All Google Cloud Storage data are stored encrypted. For more information see\n`Server-Side Encryption\n<https://cloud.google.com/storage/docs/encryption#server-side>`_.\n\nYou can also provide your own encryption keys. For more information, see\n\"gsutil help encryption\".",
            "LOCAL FILE STORAGE SECURITY": "gsutil takes a number of precautions to protect against security exploits in\nthe files it stores locally:\n\n- When the gsutil config (or gcloud init for Cloud SDK installs) command runs\n  it sets file protection mode 600 (\"-rw-------\") on the .boto\n  configuration file it generates, so only the user (or superuser) can read\n  it. This is important because these files contain security-sensitive\n  information, including credentials and proxy configuration.\n\n- The gsutil config (or gcloud init for Cloud SDK installs) command also uses\n  file protection mode 600 for the private key file stored locally when you\n  create service account credentials.\n\n- The default level of logging output from gsutil commands does not include\n  security-sensitive information, such as OAuth2 tokens and proxy\n  configuration information. (See the \"RECOMMENDED USER PRECAUTIONS\" section\n  below if you increase the level of debug output, using the gsutil -D\n  option.)\n\nNote that protection modes are not supported on Windows, so if you\nuse gsutil on Windows we recommend using an encrypted file system and strong\naccount passwords.",
            "MEASUREMENT DATA": "The gsutil perfdiag command collects a variety of performance-related\nmeasurements and details about your local system and network environment, for\nuse in troubleshooting performance problems. None of this information will be\nsent to Google unless you choose to send it.",
            "PROXY USAGE": "gsutil supports access via proxies, such as Squid and a number of commercial\nproducts. A full description of their capabilities is beyond the scope of this\ndocumentation, but proxies can be configured to support many security-related\nfunctions, including virus scanning, Data Leakage Prevention, control over\nwhich certificates/CA's are trusted, content type filtering, and many more\ncapabilities. Some of these features can slow or block legitimate gsutil\nbehavior. For example, virus scanning depends on decrypting file content,\nwhich in turn requires that the proxy terminate the gsutil connection and\nestablish a new connection - and in some cases proxies will rewrite content in\nways that result in checksum validation errors and other problems.\n\nFor details on configuring proxies see the proxy help text in your .boto\nconfiguration file (generated by the gsutil config or gcloud init command).",
            "RECOMMENDED USER PRECAUTIONS": "The first and foremost precaution is: Never share your credentials. Each user\nshould have distinct credentials.\n\nIf you run gsutil -D (to generate debugging output) it will include OAuth2\nrefresh and access tokens in the output. Make sure to redact this information\nbefore sending this debug output to anyone during troubleshooting/tech support\ninteractions.\n\nIf you run gsutil --trace-token (to send a trace directly to Google),\nsensitive information like OAuth2 tokens and the contents of any files\naccessed during the trace may be included in the content of the trace.\n\nCustomer-supplied encryption key information in the .boto configuration is\nsecurity sensitive.\n\nThe proxy configuration information in the .boto configuration is\nsecurity-sensitive, especially if your proxy setup requires user and\npassword information. Even if your proxy setup doesn't require user and\npassword, the host and port number for your proxy is often considered\nsecurity-sensitive. Protect access to your .boto configuration file.\n\nIf you are using gsutil from a production environment (e.g., via a cron job\nrunning on a host in your data center), use service account credentials rather\nthan individual user account credentials. These credentials were designed for\nsuch use and, for example, protect you from losing access when an employee\nleaves your company.",
            "SECURITY-SENSITIVE FILES WRITTEN TEMPORARILY TO DISK BY GSUTIL": "gsutil buffers data in temporary files in several situations:\n\n- While compressing data being uploaded via gsutil cp -z/-Z, gsutil\n  buffers the data in temporary files with protection 600, which it\n  deletes after the upload is complete (similarly for downloading files\n  that were uploaded with gsutil cp -z/-Z or some other process that sets the\n  Content-Encoding to \"gzip\"). However, if you kill the gsutil process\n  while the upload is under way the partially written file will be left\n  in place. See the \"CHANGING TEMP DIRECTORIES\" section in\n  \"gsutil help cp\" for details of where the temporary files are written\n  and how to change the temp directory location.\n\n- When performing a resumable upload gsutil stores the upload ID (which,\n  as noted above, is a bearer token and thus should be safe-guarded) in a\n  file under ~/.gsutil/tracker-files with protection 600, and deletes this\n  file after the upload is complete. However, if the upload doesn't\n  complete successfully the tracker file is left in place so the resumable\n  upload can be re-attempted later. Over time it's possible to accumulate\n  these tracker files from aborted upload attempts, though resumable\n  upload IDs are only valid for 1 week, so the security risk only exists\n  for files less than that age. If you consider the risk of leaving\n  aborted upload IDs in the tracker directory too high you could modify\n  your upload scripts to delete the tracker files; or you could create a\n  cron job to clear the tracker directory periodically.\n\n- The gsutil rsync command stores temporary files (with protection 600)\n  containing the names, sizes, and checksums of source and destination\n  directories/buckets, which it deletes after the rsync is complete.\n  However, if you kill the gsutil process while the rsync is under way the\n  listing files will be left in place.\n\nNote that gsutil deletes temporary files using the standard OS unlink system\ncall, which does not perform `data wiping\n<https://en.wikipedia.org/wiki/Data_erasure>`_. Thus, the content of such\ntemporary files can be recovered by a determined adversary.",
            "SOFTWARE INTEGRITY AND UPDATES": "gsutil is distributed as a standalone bundle via tar and zip files stored in\nthe gs://pub bucket, as a PyPi module, and as part of the bundled Cloud\nSDK release. Each of these distribution methods takes a variety of security\nprecautions to protect the integrity of the software. We strongly recommend\nagainst getting a copy of gsutil from any other sources (such as mirror\nsites).",
            "TRANSPORT LAYER SECURITY": "gsutil performs all operations using transport-layer encryption (HTTPS), to\nprotect against data leakage over shared network links. This is also important\nbecause gsutil uses \"bearer tokens\" for authentication (OAuth2) as well as for\nresumable upload identifiers, and such tokens must be protected from being\neavesdropped and reused.\n\ngsutil also supports the older HMAC style of authentication via the XML API\n(see \"gsutil help apis\").  While HMAC authentication does not use bearer\ntokens (and thus is not subject to eavesdropping/replay attacks), it's still\nimportant to encrypt data traffic.\n\nPrior to gsutil release 4.0 it was possible to use HTTP instead of HTTPS by\nsetting the \"is_secure\" configuration parameter in the [Boto] section of the\nboto configuration file to False. However, starting with gsutil version 4.0\nsetting is_secure to False is disallowed.  For more details about different\ncredential options, see \"gsutil help creds\"."
          }
        },
        "subdirs": {
          "capsule": "How Subdirectories Work",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "subdirs"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "This section provides details about how subdirectories work in gsutil.\nMost users probably don't need to know these details, and can simply use\nthe commands (like cp -r) that work with subdirectories. We provide this\nadditional documentation to help users understand how gsutil handles\nsubdirectories differently than most GUI / web-based tools (e.g., why\nthose other tools create \"dir_$folder$\" objects), and also to explain cost and\nperformance implications of the gsutil approach, for those interested in such\ndetails.\n\ngsutil provides the illusion of a hierarchical file tree atop the \"flat\"\nname space supported by the Google Cloud Storage service. To the service,\nthe object gs://your-bucket/abc/def.txt is just an object that happens to\nhave \"/\" characters in its name. There is no \"abc\" directory; just a single\nobject with the given name. This diagram:\n\n.. image::  https://cloud.google.com/storage/images/gsutil-subdirectories.svg\n\nillustrates how gsutil provides a hierarchical view of objects in a bucket.\n\ngsutil achieves the hierarchical file tree illusion by applying a variety of\nrules, to try to make naming work the way users would expect. For example, in\norder to determine whether to treat a destination URL as an object name or the\nroot of a directory under which objects should be copied gsutil uses these\nrules:\n\n1. If the destination object ends with a \"/\" gsutil treats it as a directory.\n   For example, if you run the command:\n\n     gsutil cp your-file gs://your-bucket/abc/\n\n   gsutil will create the object gs://your-bucket/abc/your-file.\n\n2. If the destination object is XYZ and an object exists called XYZ_$folder$\n   gsutil treats XYZ as a directory. For example, if you run the command:\n\n     gsutil cp your-file gs://your-bucket/abc\n\n   and there exists an object called abc_$folder$, gsutil will create the\n   object gs://your-bucket/abc/your-file.\n\n3. If you attempt to copy multiple source files to a destination URL, gsutil\n   treats the destination URL as a directory. For example, if you run\n   the command:\n\n     gsutil cp -r your-dir gs://your-bucket/abc\n\n   gsutil will create objects like gs://your-bucket/abc/your-dir/file1, etc.\n   (assuming file1 is a file under the source directory your-dir).\n\n4. If none of the above rules applies, gsutil performs a bucket listing to\n   determine if the target of the operation is a prefix match to the\n   specified string. For example, if you run the command:\n\n     gsutil cp your-file gs://your-bucket/abc\n\n   gsutil will make a bucket listing request for the named bucket, using\n   delimiter=\"/\" and prefix=\"abc\". It will then examine the bucket listing\n   results and determine whether there are objects in the bucket whose path\n   starts with gs://your-bucket/abc/, to determine whether to treat the target\n   as an object name or a directory name. In turn this impacts the name of the\n   object you create: If the above check indicates there is an \"abc\" directory\n   you will end up with the object gs://your-bucket/abc/your-file; otherwise\n   you will end up with the object gs://your-bucket/abc. (See\n   \"HOW NAMES ARE CONSTRUCTED\" under \"gsutil help cp\" for more details.)\n\nThis rule-based approach stands in contrast to the way many tools work, which\ncreate objects to mark the existence of folders (such as \"dir_$folder$\").\ngsutil understands several conventions used by such tools but does not\nrequire such marker objects to implement naming behavior consistent with\nUNIX commands.\n\nA downside of the gsutil subdirectory naming approach is it requires an extra\nbucket listing before performing the needed cp or mv command. However those\nlistings are relatively inexpensive, because they use delimiter and prefix\nparameters to limit result data. Moreover, gsutil makes only one bucket\nlisting request per cp/mv command, and thus amortizes the bucket listing cost\nacross all transferred objects (e.g., when performing a recursive copy of a\ndirectory to the cloud).",
            "POTENTIAL FOR SURPRISING DESTINATION SUBDIRECTORY NAMING": "The above rules-based approach for determining how destination paths are\nconstructed can lead to the following surprise: Suppose you start by trying to\nupload everything under a local directory to a bucket \"subdirectory\" that\ndoesn't yet exist:\n\n  gsutil cp -r ./your-dir/* gs://your-bucket/new\n\nwhere there are directories under your-dir (say, dir1 and dir2). The first\ntime you run this command it will create the objects:\n\n  gs://your-bucket/new/dir1/abc\n  gs://your-bucket/new/dir2/abc\n\nbecause gs://your-bucket/new doesn't yet exist. If you run the same command\nagain, because gs://your-bucket/new does now exist, it will create the\nadditional objects:\n\n  gs://your-bucket/new/your-dir/dir1/abc\n  gs://your-bucket/new/your-dir/dir2/abc\n\nBeyond the fact that this naming behavior can surprise users, one particular\ncase you should be careful about is if you script gsutil uploads with a retry\nloop. If you do this and the first attempt copies some but not all files,\nthe second attempt will encounter an already existing source subdirectory\nand result in the above-described naming problem.\n\nThere are a couple of ways to avoid this problem:\n\n1. Use gsutil rsync. Since rsync doesn't use the Unix cp-defined directory\nnaming rules, it will work consistently whether the destination subdirectory\nexists or not.\n\n2. If using rsync won't work for you, you can start by creating a\n\"placeholder\" object to establish that the destination is a subdirectory, by\nrunning a command such as:\n\n  gsutil cp some-file gs://your-bucket/new/placeholder\n\nAt this point running the gsutil cp -r command noted above will\nconsistently treat gs://your-bucket/new as a subdirectory. Once you have\nat least one object under that subdirectory you can delete the placeholder\nobject and subsequent uploads to that subdirectory will continue to work\nwith naming working as you'd expect."
          }
        },
        "support": {
          "capsule": "Google Cloud Storage Support",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "support"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "AND ACCOUNT QUESTIONS": "A) For billing documentation, please visit\nhttps://cloud.google.com/storage/pricing.\nIf you want to cancel billing, follow the instructions at\n`Cloud Storage FAQ <https://cloud.google.com/storage/docs/faq#disablebilling>`_.\nCaution: When you disable billing, you also disable the Google Cloud Storage\nservice. Make sure you want to disable the Google Cloud Storage service\nbefore you disable billing.\n\nB) For support regarding billing, please see\n`billing support <https://support.google.com/cloud/contact/cloud_platform_billing>`_.\nFor other questions regarding your account, Terms Of Service, Google\nCloud Console, or other administration-related questions please see\n`Google Cloud Platform support <https://support.google.com/cloud/answer/6282346#gcp>`_.",
            "DESCRIPTION": "If you have any questions or encounter any problems with Google Cloud Storage,\nplease first read the `FAQ <https://cloud.google.com/storage/docs/faq>`_.\n\nIf you still have questions please use one of the following methods as\nappropriate, providing the details noted below:\n\nA) For API, tool usage, or other software development-related questions,\nplease search for and post questions on Stack Overflow, using the official\n`google-cloud-storage tag\n<http://stackoverflow.com/questions/tagged/google-cloud-storage>`_. Our\nsupport team actively monitors questions to this tag and we'll do our best to\nrespond.\n\nB) For gsutil bugs or feature requests, please check if there is already a\n`existing GitHub issue <https://github.com/GoogleCloudPlatform/gsutil/issues>`_\nthat covers your request. If not, create a\n`new GitHub issue <https://github.com/GoogleCloudPlatform/gsutil/issues/new>`_.\n\nTo help us diagnose any issues you encounter, when creating a new issue\nplease provide these details in addition to the description of your problem:\n\n- The resource you are attempting to access (bucket name, object name),\n  assuming they are not sensitive.\n- The operation you attempted (GET, PUT, etc.)\n- The time and date (including timezone) at which you encountered the problem\n- If you can use gsutil to reproduce your issue, specify the -D option to\n  display your request's HTTP details, and provide these details in the\n  issue.\n\nWarning: The gsutil -d, -D, and -DD options will also print the authentication\nheader with authentication credentials for your Google Cloud Storage account.\nMake sure to remove any \"Authorization:\" headers before you post HTTP details\nto the issue. Note also that if you upload files large enough to use resumable\nuploads, the resumable upload IDs are security-sensitive while an upload\nis not yet complete, so should not be posted on public forums.\n\nIf you make any local modifications to gsutil, please make sure to use\na released copy of gsutil (instead of your locally modified copy) when\nproviding the gsutil -D output noted above. We cannot support versions\nof gsutil that include local modifications. (However, we're open to user\ncontributions; see \"gsutil help dev\".)"
          }
        },
        "throttling": {
          "capsule": "Throttling gsutil",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "throttling"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Particularly when used with the -m (multi-threading) option, gsutil can\nconsume a significant amount of network bandwidth. In some cases this can\ncause problems, for example if you start a large rsync operation over a\nnetwork link that's also used by a number of other important jobs.\n\nWhile gsutil has no built-in support for throttling requests, there are\nvarious tools available on Linux and macOS that can be used to throttle\ngsutil requests.\n\nOne tool is `trickle <https://github.com/mariusae/trickle>`_ (available via\napt-get on Ubuntu systems), which will let you limit how much bandwidth gsutil\nconsumes. For example, the following command would limit upload and download\nbandwidth consumed by gsutil rsync to 100 KBps:\n\n    trickle -d 100 -u 100 gsutil -o \"GSUtil:parallel_process_count=1\" \\\n      -o \"GSUtil:parallel_thread_count=1\" rsync -r ./dir gs://some bucket\n\nNote that we recommend against using the -m flag with gsutil when running via\ntrickle, as this may cause resource starvation and prevent your command from\nfinishing.\n\nAnother tool is\n`ionice <http://www.tutorialspoint.com/unix_commands/ionice.htm>`_ (built\nin to many Linux systems), which will let you limit how much I/O capacity\ngsutil consumes (e.g., to avoid letting it monopolize your local disk). For\nexample, the following command would reduce I/O priority of gsutil so it\ndoesn't monopolize your local disk:\n\n    ionice -c 2 -n 7 gsutil -m rsync -r ./dir gs://some bucket"
          }
        },
        "versions": {
          "capsule": "Object Versioning and Concurrency Control",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "versions"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "CONCURRENCY CONTROL": "If you are building an application using Google Cloud Storage, you may need to\nbe careful about concurrency control. Normally gsutil itself isn't used for\nthis purpose, but it's possible to write scripts around gsutil that perform\nconcurrency control.\n\nFor example, suppose you want to implement a \"rolling update\" system using\ngsutil, where a periodic job computes some data and uploads it to the cloud.\nOn each run, the job starts with the data that it computed from last run, and\ncomputes a new value. To make this system robust, you need to have multiple\nmachines on which the job can run, which raises the possibility that two\nsimultaneous runs could attempt to update an object at the same time. This\nleads to the following potential race condition:\n\n- job 1 computes the new value to be written\n- job 2 computes the new value to be written\n- job 2 writes the new value\n- job 1 writes the new value\n\nIn this case, the value that job 1 read is no longer current by the time\nit goes to write the updated object, and writing at this point would result\nin stale (or, depending on the application, corrupt) data.\n\nTo prevent this, you can find the version-specific name of the object that was\ncreated, and then use the information contained in that URL to specify an\nx-goog-if-generation-match header on a subsequent gsutil cp command. You can\ndo this in two steps. First, use the gsutil cp -v option at upload time to get\nthe version-specific name of the object that was created, for example:\n\n  gsutil cp -v file gs://bucket/object\n\nmight output:\n\n  Created: gs://bucket/object#1360432179236000\n\nYou can extract the generation value from this object and then construct a\nsubsequent gsutil command like this:\n\n  gsutil -h x-goog-if-generation-match:1360432179236000 cp newfile \\\n      gs://bucket/object\n\nThis command requests Google Cloud Storage to attempt to upload newfile\nbut to fail the request if the generation of newfile that is live at the\ntime of the upload does not match that specified.\n\nIf the command you use updates object metadata, you will need to find the\ncurrent metageneration for an object. To do this, use the gsutil ls -a and\n-l options. For example, the command:\n\n  gsutil ls -l -a gs://bucket/object\n\nwill output something like:\n\n    64  2013-02-12T19:59:13Z  gs://bucket/object#1360699153986000  metageneration=3\n  1521  2013-02-13T02:04:08Z  gs://bucket/object#1360721048778000  metageneration=2\n\nGiven this information, you could use the following command to request setting\nthe ACL on the older version of the object, such that the command will fail\nunless that is the current version of the data+metadata:\n\n  gsutil -h x-goog-if-generation-match:1360699153986000 -h \\\n    x-goog-if-metageneration-match:3 acl set public-read \\\n    gs://bucket/object#1360699153986000\n\nWithout adding these headers, the update would simply overwrite the existing\nACL. Note that in contrast, the \"gsutil acl ch\" command uses these headers\nautomatically, because it performs a read-modify-write cycle in order to edit\nACLs.\n\nIf you want to experiment with how generations and metagenerations work, try\nthe following. First, upload an object; then use gsutil ls -l -a to list all\nversions of the object, along with each version's metageneration; then re-\nupload the object and repeat the gsutil ls -l -a. You should see two object\nversions, each with metageneration=1. Now try setting the ACL, and rerun the\ngsutil ls -l -a. You should see the most recent object generation now has\nmetageneration=2.",
            "COPYING VERSIONED BUCKETS": "You can copy data between two versioned buckets, using a command like:\n\n  gsutil cp -r -A gs://bucket1/* gs://bucket2\n\nWhen run using versioned buckets, this command will cause every object version\nto be copied. The copies made in gs://bucket2 will have different generation\nnumbers (since a new generation is assigned when the object copy is made),\nbut the object sort order will remain consistent. For example, gs://bucket1\nmight contain:\n\n  % gsutil ls -la gs://bucket1 10  2013-06-06T02:33:11Z\n  53  2013-02-02T22:30:57Z  gs://bucket1/file#1359844257574000  metageneration=1\n  12  2013-02-02T22:30:57Z  gs://bucket1/file#1359844257615000  metageneration=1\n  97  2013-02-02T22:30:57Z  gs://bucket1/file#1359844257665000  metageneration=1\n\nand after the copy, gs://bucket2 might contain:\n\n  % gsutil ls -la gs://bucket2\n  53  2013-06-06T02:33:11Z  gs://bucket2/file#1370485991580000  metageneration=1\n  12  2013-06-06T02:33:14Z  gs://bucket2/file#1370485994328000  metageneration=1\n  97  2013-06-06T02:33:17Z  gs://bucket2/file#1370485997376000  metageneration=1\n\nNote that the object versions are in the same order (as can be seen by the\nsame sequence of sizes in both listings), but the generation numbers (and\ntimestamps) are newer in gs://bucket2.",
            "DESCRIPTION": "Versioning-enabled buckets maintain noncurrent versions of objects, providing\na way to un-delete data that you accidentally deleted, or to retrieve older\nversions of your data. You can turn versioning on or off for a bucket at any\ntime. Turning versioning off leaves existing object versions in place and\nsimply causes the bucket to overwrite the live version of the object whenever\na new version is uploaded.\n\nRegardless of whether you have enabled versioning on a bucket, every object\nhas two associated positive integer fields:\n\n- the generation, which is updated when the content of an object is\n  overwritten.\n- the metageneration, which identifies the metadata generation. It starts\n  at 1; is updated every time the metadata (e.g., ACL or Content-Type) for a\n  given content generation is updated; and gets reset when the generation\n  number changes.\n\nOf these two integers, only the generation is used when working with versioned\ndata. Both generation and metageneration can be used with concurrency control\n(discussed in a later section).\n\nTo work with object versioning in gsutil, you can use a flavor of storage URLs\nthat embed the object generation, which we refer to as version-specific URLs.\nFor example, the version-less object URL:\n\n  gs://bucket/object\n\nmight have have two versions, with these version-specific URLs:\n\n  gs://bucket/object#1360383693690000\n  gs://bucket/object#1360383802725000\n\nThe following sections discuss how to work with versioning and concurrency\ncontrol.",
            "FOR MORE INFORMATION": "For more details on how to use versioning and preconditions, see\nhttps://cloud.google.com/storage/docs/object-versioning",
            "OBJECT VERSIONING": "You can view, enable, and disable object versioning on a bucket using\nthe 'versioning get' and 'versioning set' commands. For example:\n\n  gsutil versioning set on gs://bucket\n\nwill enable versioning for the named bucket. See 'gsutil help versioning'\nfor additional details.\n\nTo see all object versions in a versioning-enabled bucket along with\ntheir generation.metageneration information, use gsutil ls -a:\n\n  gsutil ls -a gs://bucket\n\nYou can also specify particular objects for which you want to find the\nversion-specific URL(s), or you can use wildcards:\n\n  gsutil ls -a gs://bucket/object1 gs://bucket/images/*.jpg\n\nThe generation values form a monotonically increasing sequence as you create\nadditional object versions.  Because of this, the latest object version is\nalways the last one listed in the gsutil ls output for a particular object.\nFor example, if a bucket contains these three versions of gs://bucket/object:\n\n  gs://bucket/object#1360035307075000\n  gs://bucket/object#1360101007329000\n  gs://bucket/object#1360102216114000\n\nthen gs://bucket/object#1360102216114000 is the latest version and\ngs://bucket/object#1360035307075000 is the oldest available version.\n\nIf you specify version-less URLs with gsutil, you will operate only on the\nlive version of an object, for example:\n\n  gsutil cp gs://bucket/object ./dir\n\nor:\n\n  gsutil rm gs://bucket/object\n\nThe same is true when using wildcards like * and **. These will operate only\non the live version of matching objects. For example, this command will remove\nthe live version and create a noncurrent version for each object in a bucket:\n\n  gsutil rm gs://bucket/**\n\nTo operate on a specific object version, use a version-specific URL.\nFor example, suppose the output of the above gsutil ls -a command is:\n\n  gs://bucket/object#1360035307075000\n  gs://bucket/object#1360101007329000\n\nIn this case, the command:\n\n  gsutil cp gs://bucket/object#1360035307075000 ./dir\n\nwill retrieve the second most recent version of the object.\n\nNote that version-specific URLs cannot be the target of the gsutil cp\ncommand (trying to do so will result in an error), because writing to a\nversioned object always creates a new version.\n\nNote also that some shells treat \"#\" as a special character (e.g., zsh with\nthe extendedglob option enabled). If you're using a shell that treats \"#\" as a\nspecial character you will need to surround the argument with quotes, such as:\n\n  gsutil cp 'gs://bucket/object#1360035307075000' ./dir\n\nIf an object has been deleted, it will not show up in a normal gsutil ls\nlisting (i.e., ls without the -a option). You can restore a deleted object by\nrunning gsutil ls -a to find the available versions, and then copying one of\nthe version-specific URLs to the version-less URL, for example:\n\n  gsutil cp gs://bucket/object#1360101007329000 gs://bucket/object\n\nNote that when you do this it creates a new object version, which will incur\nadditional charges. You can get rid of the extra copy by deleting the older\nversion-specfic object:\n\n  gsutil rm gs://bucket/object#1360101007329000\n\nOr you can combine the two steps by using the gsutil mv command:\n\n  gsutil mv gs://bucket/object#1360101007329000 gs://bucket/object\n\nIf you remove the live version of an object in a versioning-enabled bucket,\na noncurrent version will be preserved:\n\n  gsutil rm gs://bucket/object\n\nIf you remove a version-specific URL for an object (even if it is the live\nversion), that version will be deleted permanently:\n\n  gsutil rm gs://bucket/object#1360101007329000\n\nIf you want to remove all versions of an object, use the gsutil rm -a option:\n\n  gsutil rm -a gs://bucket/object\n\nIf you want to remove all versions of all objects in a bucket (and the bucket\nitself), use the rm -r option (-r implies the -a option):\n\n  gsutil rm -r gs://bucket\n\n\nNote that there is no limit to the number of older versions of an object you\nwill create if you continue to upload to the same object in a versioning-\nenabled bucket. It is your responsibility to delete versions beyond the ones\nyou want to retain."
          }
        },
        "wildcards": {
          "capsule": "Wildcard Names",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "help",
            "wildcards"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "BEHAVIOR FOR \"DOT\" FILES IN LOCAL FILE SYSTEM": "Per standard Unix behavior, the wildcard \"*\" only matches files that don't\nstart with a \".\" character (to avoid confusion with the \".\" and \"..\"\ndirectories present in all Unix directories). gsutil provides this same\nbehavior when using wildcards over a file system URI, but does not provide\nthis behavior over cloud URIs. For example, the following command will copy\nall objects from gs://bucket1 to gs://bucket2:\n\n  gsutil cp gs://bucket1/* gs://bucket2\n\nbut the following command will copy only files that don't start with a \".\"\nfrom the directory \"dir\" to gs://bucket1:\n\n  gsutil cp dir/* gs://bucket1",
            "BY DIRECTORY VS RECURSIVE WILDCARDS": "The \"*\" wildcard only matches up to the end of a path within\na subdirectory. For example, if bucket contains objects\nnamed gs://bucket/data/abcd, gs://bucket/data/abcdef,\nand gs://bucket/data/abcxyx, as well as an object in a sub-directory\n(gs://bucket/data/abc/def) the above gsutil cp command would match the\nfirst 3 object names but not the last one.\n\nIf you want matches to span directory boundaries, use a '**' wildcard:\n\n  gsutil cp gs://bucket/data/abc** .\n\nwill match all four objects above.\n\nNote that gsutil supports the same wildcards for both objects and file names.\nThus, for example:\n\n  gsutil cp data/abc* gs://bucket\n\nwill match all names in the local file system. Most command shells also\nsupport wildcarding, so if you run the above command probably your shell\nis expanding the matches before running gsutil. However, most shells do not\nsupport recursive wildcards ('**'), and you can cause gsutil's wildcarding\nsupport to work for such shells by single-quoting the arguments so they\ndon't get interpreted by the shell before being passed to gsutil:\n\n  gsutil cp 'data/abc**' gs://bucket",
            "CONSIDERATION: USING MID-PATH WILDCARDS": "Suppose you have a bucket with these objects:\n\n  gs://bucket/obj1\n  gs://bucket/obj2\n  gs://bucket/obj3\n  gs://bucket/obj4\n  gs://bucket/dir1/obj5\n  gs://bucket/dir2/obj6\n\nIf you run the command:\n\n  gsutil ls gs://bucket/*/obj5\n\ngsutil will perform a /-delimited top-level bucket listing and then one bucket\nlisting for each subdirectory, for a total of 3 bucket listings:\n\n  GET /bucket/?delimiter=/\n  GET /bucket/?prefix=dir1/obj5&delimiter=/\n  GET /bucket/?prefix=dir2/obj5&delimiter=/\n\nThe more bucket listings your wildcard requires, the slower and more expensive\nit will be. The number of bucket listings required grows as:\n\n- the number of wildcard components (e.g., \"gs://bucket/a??b/c*/*/d\"\n  has 3 wildcard components);\n- the number of subdirectories that match each component; and\n- the number of results (pagination is implemented using one GET\n  request per 1000 results, specifying markers for each).\n\nIf you want to use a mid-path wildcard, you might try instead using a\nrecursive wildcard, for example:\n\n  gsutil ls gs://bucket/**/obj5\n\nThis will match more objects than \"gs://bucket/*/obj5\" (since it spans\ndirectories), but is implemented using a delimiter-less bucket listing\nrequest (which means fewer bucket requests, though it will list the entire\nbucket and filter locally, so that could require a non-trivial amount of\nnetwork traffic).",
            "CONSIDERATION: USING WILDCARDS OVER MANY OBJECTS": "It is more efficient, faster, and less network traffic-intensive\nto use wildcards that have a non-wildcard object-name prefix, like:\n\n  gs://bucket/abc*.txt\n\nthan it is to use wildcards as the first part of the object name, like:\n\n  gs://bucket/*abc.txt\n\nThis is because the request for \"gs://bucket/abc*.txt\" asks the server to send\nback the subset of results whose object name start with \"abc\" at the bucket\nroot, and then gsutil filters the result list for objects whose name ends with\n\".txt\".  In contrast, \"gs://bucket/*abc.txt\" asks the server for the complete\nlist of objects in the bucket root, and then filters for those objects whose\nname ends with \"abc.txt\". This efficiency consideration becomes increasingly\nnoticeable when you use buckets containing thousands or more objects. It is\nsometimes possible to set up the names of your objects to fit with expected\nwildcard matching patterns, to take advantage of the efficiency of doing\nserver-side prefix requests. See, for example \"gsutil help prod\" for a\nconcrete use case example.",
            "DESCRIPTION": "You can specify wildcards for bucket names within a single project. For\nexample:\n\n  gsutil ls gs://data*.example.com\n\nwill list the contents of all buckets whose name starts with \"data\" and\nends with \".example.com\" in the default project. The -p option can be used\nto specify a project other than the default.  For example:\n\n  gsutil ls -p other-project gs://data*.example.com\n\nYou can also combine bucket and object name wildcards. For example this\ncommand will remove all \".txt\" files in any of your Google Cloud Storage\nbuckets in the default project:\n\n  gsutil rm gs://*/**.txt",
            "SURPRISING BEHAVIOR WHEN USING WILDCARDS": "There are a couple of ways that using wildcards can result in surprising\nbehavior:\n\n1. Shells (like bash and zsh) can attempt to expand wildcards before passing\n   the arguments to gsutil. If the wildcard was supposed to refer to a cloud\n   object, this can result in surprising \"Not found\" errors (e.g., if the\n   shell tries to expand the wildcard \"gs://my-bucket/*\" on the local\n   machine, matching no local files, and failing the command).\n\n   Note that some shells include additional characters in their wildcard\n   character sets. For example, if you use zsh with the extendedglob option\n   enabled it will treat \"#\" as a special character, which conflicts with\n   that character's use in referencing versioned objects (see\n   \"gsutil help versions\" for details).\n\n   To avoid these problems, surround the wildcarded expression with single\n   quotes (on Linux) or double quotes (on Windows).\n\n2. Attempting to specify a filename that contains wildcard characters won't\n   work, because gsutil will try to expand the wildcard characters rather\n   than using them as literal characters. For example, running the command:\n\n     gsutil cp './file[1]' gs://my-bucket\n\n   will cause gsutil to try to match the '[1]' part as a wildcard.\n\n   There's an open issue to support a \"raw\" mode for gsutil to provide a\n   way to work with file names that contain wildcard characters, but until /\n   unless that support is implemented there's no really good way to use\n   gsutil with such file names. You could use a wildcard to name such files,\n   for example replacing the above command with:\n\n     gsutil cp './file*1*' gs://my-bucket\n\n   but that approach may be difficult to use in general.",
            "WILDCARD CHARACTERS": "In addition to '*', you can use these wildcards:\n\n?\n  Matches a single character. For example \"gs://bucket/??.txt\"\n  only matches objects with two characters followed by .txt.\n\n[chars]\n  Match any of the specified characters. For example\n  \"gs://bucket/[aeiou].txt\" matches objects that contain a single vowel\n  character followed by .txt\n\n[char range]\n  Match any of the range of characters. For example\n  \"gs://bucket/[a-m].txt\" matches objects that contain letters\n  a, b, c, ... or m, and end with .txt.\n\nYou can combine wildcards to provide more powerful matches, for example:\n\n  gs://bucket/[a-m]??.j*g"
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "help"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {}
    },
    "hmac": {
      "capsule": "CRUD operations on service account HMAC keys.",
      "commands": {
        "create": {
          "capsule": "CRUD operations on service account HMAC keys.",
          "commands": {},
          "flags": {
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<project_id> Specify a project in which to create a key.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "hmac",
            "create"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``hmac create`` command creates an HMAC key for the specified service\naccount:\n\n  gsutil hmac create test.service.account@test_project.iam.gserviceaccount.com\n\nThe secret key material is only available upon creation, so be sure to store\nthe returned secret along with the access_id."
          }
        },
        "delete": {
          "capsule": "CRUD operations on service account HMAC keys.",
          "commands": {},
          "flags": {
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<project_id> Specify a project from which to delete a key.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "hmac",
            "delete"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``hmac delete`` command permanently deletes the specified HMAC key:\n\n  gsutil hmac delete GOOG56JBMFZX6PMPTQ62VD2\n\nNote that keys must be updated to be in the INACTIVE state before they can be\ndeleted."
          }
        },
        "get": {
          "capsule": "CRUD operations on service account HMAC keys.",
          "commands": {},
          "flags": {
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<project_id> Specify a project from which to get a key.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "hmac",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``hmac get`` command retrieves the specified HMAC key's metadata:\n\n  gsutil hmac get GOOG56JBMFZX6PMPTQ62VD2\n\nNote that there is no option to retrieve a key's secret material after it has\nbeen created."
          }
        },
        "list": {
          "capsule": "CRUD operations on service account HMAC keys.",
          "commands": {},
          "flags": {
            "-a": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Show all keys, including recently deleted\n keys.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-a",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-l": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Use long listing format. Shows each key's full\n metadata excluding the secret.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-l",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<project_id> Specify a project from which to list keys.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-u": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<service_account_email> Filter keys for a single service account.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-u",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "hmac",
            "list"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``hmac list`` command lists the HMAC key metadata for keys in the\nspecified project. If no project is specified in the command, the default\nproject is used."
          }
        },
        "update": {
          "capsule": "CRUD operations on service account HMAC keys.",
          "commands": {},
          "flags": {
            "-e": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<etag> If provided, the update will only be performed\n if the specified etag matches the etag of the\n stored key.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-e",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<project_id> Specify a project in which to update a key.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-s": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<ACTIVE|INACTIVE> Sets the state of the specified key to either\n ACTIVE or INACTIVE.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-s",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "hmac",
            "update"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``hmac update`` command sets the state of the specified key:\n\n  gsutil hmac update -s INACTIVE -e M42da= GOOG56JBMFZX6PMPTQ62VD2\n\nValid state arguments are ACTIVE and INACTIVE. To set a key to state DELETED\nuse the \"hmac delete\" command on an INACTIVE key. If an etag is set in the\ncommand, it will only succeed if the provided etag matches the etag of the\nstored key."
          }
        }
      },
      "flags": {
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Show all keys, including recently deleted\n keys.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<etag> If provided, the update will only be performed\n if the specified etag matches the etag of the\n stored key.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-l": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Use long listing format. Shows each key's full\n metadata excluding the secret.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-l",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<project_id> Specify a project in which to update a key.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<ACTIVE|INACTIVE> Sets the state of the specified key to either\n ACTIVE or INACTIVE.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-u": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<service_account_email> Filter keys for a single service account.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-u",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "hmac"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CREATE": "The ``hmac create`` command creates an HMAC key for the specified service\naccount:\n\n  gsutil hmac create test.service.account@test_project.iam.gserviceaccount.com\n\nThe secret key material is only available upon creation, so be sure to store\nthe returned secret along with the access_id.",
        "DELETE": "The ``hmac delete`` command permanently deletes the specified HMAC key:\n\n  gsutil hmac delete GOOG56JBMFZX6PMPTQ62VD2\n\nNote that keys must be updated to be in the INACTIVE state before they can be\ndeleted.",
        "DESCRIPTION": "The hmac command is used to interact with service account `HMAC keys\n<https://cloud.google.com/storage/docs/authentication/hmackeys>`_.\n\nThe hmac command has five sub-commands:",
        "GET": "The ``hmac get`` command retrieves the specified HMAC key's metadata:\n\n  gsutil hmac get GOOG56JBMFZX6PMPTQ62VD2\n\nNote that there is no option to retrieve a key's secret material after it has\nbeen created.",
        "LIST": "The ``hmac list`` command lists the HMAC key metadata for keys in the\nspecified project. If no project is specified in the command, the default\nproject is used.",
        "UPDATE": "The ``hmac update`` command sets the state of the specified key:\n\n  gsutil hmac update -s INACTIVE -e M42da= GOOG56JBMFZX6PMPTQ62VD2\n\nValid state arguments are ACTIVE and INACTIVE. To set a key to state DELETED\nuse the \"hmac delete\" command on an INACTIVE key. If an etag is set in the\ncommand, it will only succeed if the provided etag matches the etag of the\nstored key."
      }
    },
    "iam": {
      "capsule": "Get, set, or change bucket and/or object IAM permissions.",
      "commands": {
        "ch": {
          "capsule": "Get, set, or change bucket and/or object IAM permissions.",
          "commands": {},
          "flags": {
            "-d": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Removes roles granted to the specified member.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-d",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-f": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Default gsutil error handling is fail-fast. This flag\nanges the request to fail-silent mode. This is implicitly\nt when invoking the gsutil -m option.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-f",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-r": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Performs ``iam ch`` recursively to all objects under the\necified bucket.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-r",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "iam",
            "ch"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``iam ch`` command incrementally updates IAM policies. You may specify\nmultiple access grants and removals in a single command invocation, which\nwill be batched and applied as a whole to each url via an IAM patch.\nThe patch will be constructed by applying each access grant or removal in the\norder in which they appear in the command line arguments. Each access change\nspecifies a member and the role that will be either granted or revoked.\n\nThe gsutil -m option may be set to handle object-level operations more\nefficiently.\n\nNOTE: The ``iam ch`` command may NOT be used to change the IAM policy of a\nresource that contains conditions in its policy bindings. Attempts to do so\nwill result in an error. To change the IAM policy of such a resource, you can\nperform a read-modify-write operation by using ``gsutil iam get`` to save the\npolicy to a file, editing the file, and using ``gsutil iam set`` to set the\nupdated policy.",
            "EXAMPLES": "Examples for the ``ch`` sub-command:\n\nTo grant a single role to a single member for some targets:\n\n  gsutil iam ch user:john.doe@example.com:objectCreator gs://ex-bucket\n\nTo make a bucket's objects publicly readable:\n\n  gsutil iam ch allUsers:objectViewer gs://ex-bucket\n\nTo grant multiple bindings to a bucket:\n\n  gsutil iam ch user:john.doe@example.com:objectCreator \\\n                domain:www.my-domain.org:objectViewer gs://ex-bucket\n\nTo specify more than one role for a particular member:\n\n  gsutil iam ch user:john.doe@example.com:objectCreator,objectViewer \\\n                gs://ex-bucket\n\nTo specify a custom role for a particular member:\n\n  gsutil iam ch user:john.doe@example.com:roles/customRoleName gs://ex-bucket\n\nTo apply a grant and simultaneously remove a binding to a bucket:\n\n  gsutil iam ch -d group:readers@example.com:legacyBucketReader \\\n                group:viewers@example.com:objectViewer gs://ex-bucket\n\nTo remove a user from all roles on a bucket:\n\n  gsutil iam ch -d user:john.doe@example.com gs://ex-bucket"
          }
        },
        "get": {
          "capsule": "Get, set, or change bucket and/or object IAM permissions.",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "iam",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``iam get`` command gets the IAM policy for a bucket or object, which you\ncan save and edit for use with the ``iam set`` command.\n\nFor example:\n\n  gsutil iam get gs://example > bucket_iam.txt\n  gsutil iam get gs://example/important.txt > object_iam.txt\n\nThe IAM policy returned by ``iam get`` includes the etag of the IAM policy and\nwill be used in the precondition check for ``iam set``, unless the etag is\noverridden by setting the ``iam set -e`` option."
          }
        },
        "set": {
          "capsule": "Get, set, or change bucket and/or object IAM permissions.",
          "commands": {},
          "flags": {
            "-a": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Performs ``iam set`` request on all object versions.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-a",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-e": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<etag> Performs the precondition check on each object with the\necified etag before setting the policy.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-e",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-f": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Default gsutil error handling is fail-fast. This flag\nanges the request to fail-silent mode. This is implicitly\nt when invoking the gsutil -m option.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-f",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-r": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Performs ``iam set`` recursively to all objects under the\necified bucket.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-r",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "iam",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``iam set`` command sets the IAM policy for one or more buckets and / or\nobjects. It overwrites the current IAM policy that exists on a bucket (or\nobject) with the policy specified in the input file. The ``iam set`` command\ntakes as input a file with an IAM policy in the format of the output\ngenerated by ``iam get``.\n\nThe ``iam ch`` command can be used to edit an existing policy. It works\ncorrectly in the presence of concurrent updates. You may also do this\nmanually by using the -e flag and overriding the etag returned in ``iam get``.\nSpecifying -e with an empty string (i.e. ``gsutil iam set -e '' ...``) will\ninstruct gsutil to skip the precondition check when setting the IAM policy.\n\nIf you wish to set an IAM policy on a large number of objects, you may want\nto use the gsutil -m option for concurrent processing. The following command\nwill apply iam.txt to all objects in the \"cats\" bucket.\n\n  gsutil -m iam set -r iam.txt gs://cats\n\nNote that only object-level IAM applications are parallelized; you do not\ngain any additional performance when applying an IAM policy to a large\nnumber of buckets with the -m flag."
          }
        }
      },
      "flags": {
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Performs ``iam set`` request on all object versions.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Removes roles granted to the specified member.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<etag> Performs the precondition check on each object with the\necified etag before setting the policy.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Default gsutil error handling is fail-fast. This flag\nanges the request to fail-silent mode. This is implicitly\nt when invoking the gsutil -m option.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Performs ``iam ch`` recursively to all objects under the\necified bucket.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "iam"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CH": "The ``iam ch`` command incrementally updates IAM policies. You may specify\nmultiple access grants and removals in a single command invocation, which\nwill be batched and applied as a whole to each url via an IAM patch.\nThe patch will be constructed by applying each access grant or removal in the\norder in which they appear in the command line arguments. Each access change\nspecifies a member and the role that will be either granted or revoked.\n\nThe gsutil -m option may be set to handle object-level operations more\nefficiently.\n\nNOTE: The ``iam ch`` command may NOT be used to change the IAM policy of a\nresource that contains conditions in its policy bindings. Attempts to do so\nwill result in an error. To change the IAM policy of such a resource, you can\nperform a read-modify-write operation by using ``gsutil iam get`` to save the\npolicy to a file, editing the file, and using ``gsutil iam set`` to set the\nupdated policy.",
        "DESCRIPTION": "The iam command has three sub-commands:",
        "EXAMPLES": "Examples for the ``ch`` sub-command:\n\nTo grant a single role to a single member for some targets:\n\n  gsutil iam ch user:john.doe@example.com:objectCreator gs://ex-bucket\n\nTo make a bucket's objects publicly readable:\n\n  gsutil iam ch allUsers:objectViewer gs://ex-bucket\n\nTo grant multiple bindings to a bucket:\n\n  gsutil iam ch user:john.doe@example.com:objectCreator \\\n                domain:www.my-domain.org:objectViewer gs://ex-bucket\n\nTo specify more than one role for a particular member:\n\n  gsutil iam ch user:john.doe@example.com:objectCreator,objectViewer \\\n                gs://ex-bucket\n\nTo specify a custom role for a particular member:\n\n  gsutil iam ch user:john.doe@example.com:roles/customRoleName gs://ex-bucket\n\nTo apply a grant and simultaneously remove a binding to a bucket:\n\n  gsutil iam ch -d group:readers@example.com:legacyBucketReader \\\n                group:viewers@example.com:objectViewer gs://ex-bucket\n\nTo remove a user from all roles on a bucket:\n\n  gsutil iam ch -d user:john.doe@example.com gs://ex-bucket",
        "GET": "The ``iam get`` command gets the IAM policy for a bucket or object, which you\ncan save and edit for use with the ``iam set`` command.\n\nFor example:\n\n  gsutil iam get gs://example > bucket_iam.txt\n  gsutil iam get gs://example/important.txt > object_iam.txt\n\nThe IAM policy returned by ``iam get`` includes the etag of the IAM policy and\nwill be used in the precondition check for ``iam set``, unless the etag is\noverridden by setting the ``iam set -e`` option.",
        "SET": "The ``iam set`` command sets the IAM policy for one or more buckets and / or\nobjects. It overwrites the current IAM policy that exists on a bucket (or\nobject) with the policy specified in the input file. The ``iam set`` command\ntakes as input a file with an IAM policy in the format of the output\ngenerated by ``iam get``.\n\nThe ``iam ch`` command can be used to edit an existing policy. It works\ncorrectly in the presence of concurrent updates. You may also do this\nmanually by using the -e flag and overriding the etag returned in ``iam get``.\nSpecifying -e with an empty string (i.e. ``gsutil iam set -e '' ...``) will\ninstruct gsutil to skip the precondition check when setting the IAM policy.\n\nIf you wish to set an IAM policy on a large number of objects, you may want\nto use the gsutil -m option for concurrent processing. The following command\nwill apply iam.txt to all objects in the \"cats\" bucket.\n\n  gsutil -m iam set -r iam.txt gs://cats\n\nNote that only object-level IAM applications are parallelized; you do not\ngain any additional performance when applying an IAM policy to a large\nnumber of buckets with the -m flag."
      }
    },
    "kms": {
      "capsule": "Configure Cloud KMS encryption",
      "commands": {
        "authorize": {
          "capsule": "Configure Cloud KMS encryption",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "kms",
            "authorize"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The authorize sub-command checks that the default (or supplied) project has a\nCloud Storage-owned service account created for it, and if not, it creates\none. It then adds appropriate encrypt/decrypt permissions to Cloud KMS\nresources such that the Cloud Storage service account can write and read Cloud\nKMS-encrypted objects in buckets associated with the specified project.",
            "EXAMPLES": "Authorize your default project to use a Cloud KMS key:\n\n  gsutil kms authorize \\\n      -k projects/key-project/locations/us-east1/keyRings/key-ring/cryptoKeys/my-key\n\nAuthorize \"my-project\" to use a Cloud KMS key:\n\n  gsutil kms authorize -p my-project \\\n      -k projects/key-project/locations/us-east1/keyRings/key-ring/cryptoKeys/my-key"
          }
        },
        "encryption": {
          "capsule": "Configure Cloud KMS encryption",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "kms",
            "encryption"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The encryption sub-command is used to set, display, or clear a bucket's\ndefault KMS key, which is used to encrypt newly-written objects if no other\nkey is specified.",
            "EXAMPLES": "Set the default KMS key for my-bucket:\n\n  gsutil kms encryption \\\n      -k projects/key-project/locations/us-east1/keyRings/key-ring/cryptoKeys/my-key \\\n      gs://my-bucket\n\nSet the default KMS key for my-bucket, but display a warning rather than failing if\ngsutil is unable to verify that the specified key contains the correct IAM bindings\nfor encryption/decryption. This is useful for users that do not have getIamPolicy\npermission but know that the key has the correct IAM policy for encryption in the\nuser's project.\n\n  gsutil kms encryption \\\n      -k projects/key-project/locations/us-east1/keyRings/key-ring/cryptoKeys/my-key \\\n      -w \\\n      gs://my-bucket\n\nShow the default KMS key for my-bucket, if one is set:\n\n  gsutil kms encryption gs://my-bucket\n\nClear the default KMS key so newly-written objects will not be encrypted:\n\n  gsutil kms encryption -d gs://my-bucket"
          }
        },
        "serviceaccount": {
          "capsule": "Configure Cloud KMS encryption",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "kms",
            "serviceaccount"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The serviceaccount sub-command displays the Cloud Storage-owned service\naccount that is used to perform Cloud KMS operations against your default\nproject (or a supplied project).",
            "EXAMPLES": "Show the service account for your default project:\n\n  gsutil kms serviceaccount\n\nShow the service account for my-project:\n\n  gsutil kms serviceaccount -p my-project"
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "kms"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "AUTHORIZE": "The authorize sub-command checks that the default (or supplied) project has a\nCloud Storage-owned service account created for it, and if not, it creates\none. It then adds appropriate encrypt/decrypt permissions to Cloud KMS\nresources such that the Cloud Storage service account can write and read Cloud\nKMS-encrypted objects in buckets associated with the specified project.",
        "DESCRIPTION": "The kms command is used to configure Google Cloud Storage and Cloud KMS\nresources to support encryption of Cloud Storage objects with Cloud KMS keys.\n\nThe kms command has several sub-commands that deal with configuring\nCloud Storage's integration with Cloud KMS:",
        "ENCRYPTION": "The encryption sub-command is used to set, display, or clear a bucket's\ndefault KMS key, which is used to encrypt newly-written objects if no other\nkey is specified.",
        "EXAMPLES": "Show the service account for your default project:\n\n  gsutil kms serviceaccount\n\nShow the service account for my-project:\n\n  gsutil kms serviceaccount -p my-project",
        "SERVICEACCOUNT": "The serviceaccount sub-command displays the Cloud Storage-owned service\naccount that is used to perform Cloud KMS operations against your default\nproject (or a supplied project)."
      }
    },
    "label": {
      "capsule": "Get, set, or change the label configuration of a bucket.",
      "commands": {
        "ch": {
          "capsule": "Get, set, or change the label configuration of a bucket.",
          "commands": {},
          "flags": {
            "-d": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Remove the label with the specified key.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-d",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-l": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Add or update a label with the specified key and value.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-l",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "label",
            "ch"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"label ch\" command updates a bucket's label configuration, applying the\nlabel changes specified by the -l and -d flags. You can specify multiple\nlabel changes in a single command run; all changes will be made atomically to\neach bucket.",
            "EXAMPLES": "Examples for \"ch\" sub-command:\n\nAdd the label \"key-foo:value-bar\" to the bucket \"example-bucket\":\n\n  gsutil label ch -l key-foo:value-bar gs://example-bucket\n\nChange the above label to have a new value:\n\n  gsutil label ch -l key-foo:other-value gs://example-bucket\n\nAdd a new label and delete the old one from above:\n\n  gsutil label ch -l new-key:new-value -d key-foo gs://example-bucket"
          }
        },
        "get": {
          "capsule": "Get, set, or change the label configuration of a bucket.",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "label",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"label get\" command gets the\n`labels <https://cloud.google.com/storage/docs/key-terms#bucket-labels>`_\napplied to a bucket, which you can save and edit for use with the \"label set\"\ncommand."
          }
        },
        "set": {
          "capsule": "Get, set, or change the label configuration of a bucket.",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "label",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"label set\" command allows you to set the labels on one or more\nbuckets. You can retrieve a bucket's labels using the \"label get\" command,\nsave the output to a file, edit the file, and then use the \"label set\"\ncommand to apply those labels to the specified bucket(s). For\nexample:\n\n  gsutil label get gs://bucket > labels.json\n\nMake changes to labels.json, such as adding an additional label, then:\n\n  gsutil label set labels.json gs://example-bucket\n\nNote that you can set these labels on multiple buckets at once:\n\n  gsutil label set labels.json gs://bucket-foo gs://bucket-bar"
          }
        }
      },
      "flags": {
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Remove the label with the specified key.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-l": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Add or update a label with the specified key and value.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-l",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "label"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CH": "The \"label ch\" command updates a bucket's label configuration, applying the\nlabel changes specified by the -l and -d flags. You can specify multiple\nlabel changes in a single command run; all changes will be made atomically to\neach bucket.",
        "DESCRIPTION": "Gets, sets, or changes the label configuration (also called the tagging\nconfiguration by other storage providers) of one or more buckets. An example\nlabel JSON document looks like the following:\n\n  {\n    \"your_label_key\": \"your_label_value\",\n    \"your_other_label_key\": \"your_other_label_value\"\n  }\n\nThe label command has three sub-commands:",
        "EXAMPLES": "Examples for \"ch\" sub-command:\n\nAdd the label \"key-foo:value-bar\" to the bucket \"example-bucket\":\n\n  gsutil label ch -l key-foo:value-bar gs://example-bucket\n\nChange the above label to have a new value:\n\n  gsutil label ch -l key-foo:other-value gs://example-bucket\n\nAdd a new label and delete the old one from above:\n\n  gsutil label ch -l new-key:new-value -d key-foo gs://example-bucket",
        "GET": "The \"label get\" command gets the\n`labels <https://cloud.google.com/storage/docs/key-terms#bucket-labels>`_\napplied to a bucket, which you can save and edit for use with the \"label set\"\ncommand.",
        "SET": "The \"label set\" command allows you to set the labels on one or more\nbuckets. You can retrieve a bucket's labels using the \"label get\" command,\nsave the output to a file, edit the file, and then use the \"label set\"\ncommand to apply those labels to the specified bucket(s). For\nexample:\n\n  gsutil label get gs://bucket > labels.json\n\nMake changes to labels.json, such as adding an additional label, then:\n\n  gsutil label set labels.json gs://example-bucket\n\nNote that you can set these labels on multiple buckets at once:\n\n  gsutil label set labels.json gs://bucket-foo gs://bucket-bar"
      }
    },
    "lifecycle": {
      "capsule": "Get or set lifecycle configuration for a bucket",
      "commands": {
        "get": {
          "capsule": "Get or set lifecycle configuration for a bucket",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "lifecycle",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Gets the lifecycle configuration for a given bucket. You can get the\nlifecycle configuration for only one bucket at a time. The output can be\nredirected into a file, edited and then updated via the set sub-command."
          }
        },
        "set": {
          "capsule": "Get or set lifecycle configuration for a bucket",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "lifecycle",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "Sets the lifecycle configuration on one or more buckets. The config-json-file\nspecified on the command line should be a path to a local file containing\nthe lifecycle configuration JSON document."
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "lifecycle"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The lifecycle command can be used to get or set lifecycle management policies\nfor the given bucket(s). This command is supported for buckets only, not\nobjects. For more information on object lifecycle management, please see the\n`Google Cloud Storage docs <https://cloud.google.com/storage/docs/lifecycle>`_.\n\nThe lifecycle command has two sub-commands:",
        "EXAMPLES": "The following lifecycle configuration JSON document specifies that all objects\nin this bucket that are more than 365 days old will be deleted automatically:\n\n  {\n    \"rule\":\n    [\n      {\n        \"action\": {\"type\": \"Delete\"},\n        \"condition\": {\"age\": 365}\n      }\n    ]\n  }\n\nThe following (empty) lifecycle configuration JSON document removes all\nlifecycle configuration for a bucket:\n\n  {}",
        "GET": "Gets the lifecycle configuration for a given bucket. You can get the\nlifecycle configuration for only one bucket at a time. The output can be\nredirected into a file, edited and then updated via the set sub-command.",
        "SET": "Sets the lifecycle configuration on one or more buckets. The config-json-file\nspecified on the command line should be a path to a local file containing\nthe lifecycle configuration JSON document."
      }
    },
    "logging": {
      "capsule": "Configure or retrieve logging on buckets",
      "commands": {
        "get": {
          "capsule": "Configure or retrieve logging on buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "logging",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "If logging is enabled for the specified bucket url, the server responds\nwith a JSON document that looks something like this:\n\n  {\n    \"logBucket\": \"my_logging_bucket\",\n    \"logObjectPrefix\": \"AccessLog\"\n  }\n\nYou can download log data from your log bucket using the gsutil cp command."
          }
        },
        "set": {
          "capsule": "Configure or retrieve logging on buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "logging",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The set sub-command has two sub-commands:",
            "OFF": "This command will disable access logging of the buckets named by the\nspecified URLs. All URLs must name buckets (e.g., gs://bucket).\n\nNo logging data is removed from the log buckets when you disable logging,\nbut Google Cloud Storage will stop delivering new logs once you have\nrun this command.",
            "ON": "The \"gsutil logging set on\" command will enable access logging of the\nbuckets named by the specified URLs, outputting log files in the specified\nlogging_bucket. logging_bucket must already exist, and all URLs must name\nbuckets (e.g., gs://bucket). The required bucket parameter specifies the\nbucket to which the logs are written, and the optional log_object_prefix\nparameter specifies the prefix for log object names. The default prefix\nis the bucket name. For example, the command:\n\n  gsutil logging set on -b gs://my_logging_bucket -o AccessLog \\\n      gs://my_bucket1 gs://my_bucket2\n\nwill cause all read and write activity to objects in gs://mybucket1 and\ngs://mybucket2 to be logged to objects prefixed with the name \"AccessLog\",\nwith those log objects written to the bucket gs://my_logging_bucket.\n\nIn addition to enabling logging on your bucket(s), you will also need to grant\ncloud-storage-analytics@google.com write access to the log bucket, using this\ncommand:\n\n  gsutil acl ch -g cloud-storage-analytics@google.com:W gs://my_logging_bucket\n\nNote that log data may contain sensitive information, so you should make\nsure to set an appropriate default bucket ACL to protect that data. (See\n\"gsutil help defacl\".)"
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "logging"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "Google Cloud Storage offers access logs and storage data in the form of\nCSV files that you can download and view. Access logs provide information\nfor all of the requests made on a specified bucket in the last 24 hours,\nwhile the storage logs provide information about the storage consumption of\nthat bucket for the last 24 hour period. The logs and storage data files\nare automatically created as new objects in a bucket that you specify, in\n24 hour intervals.\n\nThe logging command has two sub-commands:",
        "GET": "If logging is enabled for the specified bucket url, the server responds\nwith a JSON document that looks something like this:\n\n  {\n    \"logBucket\": \"my_logging_bucket\",\n    \"logObjectPrefix\": \"AccessLog\"\n  }\n\nYou can download log data from your log bucket using the gsutil cp command.",
        "LOG AND STORAGE DATA FIELDS": "For a complete list of access log fields and storage data fields, see:\nhttps://cloud.google.com/storage/docs/access-logs#format",
        "OFF": "This command will disable access logging of the buckets named by the\nspecified URLs. All URLs must name buckets (e.g., gs://bucket).\n\nNo logging data is removed from the log buckets when you disable logging,\nbut Google Cloud Storage will stop delivering new logs once you have\nrun this command.",
        "ON": "The \"gsutil logging set on\" command will enable access logging of the\nbuckets named by the specified URLs, outputting log files in the specified\nlogging_bucket. logging_bucket must already exist, and all URLs must name\nbuckets (e.g., gs://bucket). The required bucket parameter specifies the\nbucket to which the logs are written, and the optional log_object_prefix\nparameter specifies the prefix for log object names. The default prefix\nis the bucket name. For example, the command:\n\n  gsutil logging set on -b gs://my_logging_bucket -o AccessLog \\\n      gs://my_bucket1 gs://my_bucket2\n\nwill cause all read and write activity to objects in gs://mybucket1 and\ngs://mybucket2 to be logged to objects prefixed with the name \"AccessLog\",\nwith those log objects written to the bucket gs://my_logging_bucket.\n\nIn addition to enabling logging on your bucket(s), you will also need to grant\ncloud-storage-analytics@google.com write access to the log bucket, using this\ncommand:\n\n  gsutil acl ch -g cloud-storage-analytics@google.com:W gs://my_logging_bucket\n\nNote that log data may contain sensitive information, so you should make\nsure to set an appropriate default bucket ACL to protect that data. (See\n\"gsutil help defacl\".)",
        "SET": "The set sub-command has two sub-commands:"
      }
    },
    "ls": {
      "capsule": "List providers, buckets, or objects",
      "commands": {},
      "flags": {
        "-L": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prints even more detail than -l.\nte: If you use this option with the (non-default) XML API it\nll generate an additional request per object being listed,\nich makes the -L option run much more slowly (and cost more)\ning the XML API than the default JSON API.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-L",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Includes non-current object versions / generations in the listing\nnly useful with a versioning-enabled bucket). If combined with\n option also prints metageneration for each listed object.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-b": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prints info about the bucket when used with a bucket URL.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-b",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "List matching subdirectory names instead of contents, and do not\ncurse into matching subdirectories even if the -R option is\necified.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Include ETag in long listing (-l) output.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-h": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "When used with -l, prints object sizes in human readable format\n.g., 1 KiB, 234 MiB, 2 GiB, etc.)",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-h",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-l": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prints long listing (owner, length).",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-l",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "proj_id Specifies the project ID to use for listing buckets.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Requests a recursive listing, performing at least one listing\neration per subdirectory. If you have a large number of\nbdirectories and do not require recursive-style output ordering,\nu may be able to instead use wildcards to perform a flat\nsting, e.g. `gsutil ls gs://mybucket/**`, which will generally\nrform fewer listing operations.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "ls"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "BUCKET DETAILS": "If you want to see information about the bucket itself, use the -b\noption. For example:\n\n  gsutil ls -L -b gs://bucket\n\nwill print something like:\n\n  gs://bucket/ :\n          Storage class:                STANDARD\n          Location constraint:          US\n          Versioning enabled:           False\n          Logging configuration:        None\n          Website configuration:        None\n          CORS configuration:           None\n          Lifecycle configuration:      None\n          Requester Pays enabled:       True\n          Labels:                       None\n          Default KMS key:              None\n          Time created:                 Thu, 14 Jan 2016 19:25:17 GMT\n          Time updated:                 Thu, 08 Jun 2017 21:17:59 GMT\n          Metageneration:               1\n          Bucket Policy Only enabled:   False\n          ACL:\n            [\n              {\n                \"entity\": \"project-owners-867489160491\",\n                \"projectTeam\": {\n                  \"projectNumber\": \"867489160491\",\n                  \"team\": \"owners\"\n                },\n                \"role\": \"OWNER\"\n              }\n            ]\n          Default ACL:\n            [\n              {\n                \"entity\": \"project-owners-867489160491\",\n                \"projectTeam\": {\n                  \"projectNumber\": \"867489160491\",\n                  \"team\": \"owners\"\n                },\n                \"role\": \"OWNER\"\n              }\n            ]\n\nNote that some fields above (time created, time updated, metageneration) are\nnot available with the (non-default) XML API.",
        "OBJECT DETAILS": "If you specify the -l option, gsutil will output additional information\nabout each matching provider, bucket, subdirectory, or object. For example:\n\n  gsutil ls -l gs://bucket/*.txt\n\nwill print the object size, creation time stamp, and name of each matching\nobject, along with the total count and sum of sizes of all matching objects:\n\n     2276224  2017-03-02T19:25:17Z  gs://bucket/obj1\n     3914624  2017-03-02T19:30:27Z  gs://bucket/obj2\n  TOTAL: 2 objects, 6190848 bytes (5.9 MiB)\n\nNote that the total listed in parentheses above is in mebibytes (or gibibytes,\ntebibytes, etc.), which corresponds to the unit of billing measurement for\nGoogle Cloud Storage.\n\nYou can get a listing of all the objects in the top-level bucket directory\n(along with the total count and sum of sizes) using a command like:\n\n  gsutil ls -l gs://bucket\n\nTo print additional detail about objects and buckets use the gsutil ls -L\noption. For example:\n\n  gsutil ls -L gs://bucket/obj1\n\nwill print something like:\n\n  gs://bucket/obj1:\n          Creation time:                    Fri, 26 May 2017 22:55:44 GMT\n          Update time:                      Tue, 18 Jul 2017 12:31:18 GMT\n          Storage class:                    STANDARD\n          Content-Length:                   60183\n          Content-Type:                     image/jpeg\n          Hash (crc32c):                    zlUhtg==\n          Hash (md5):                       Bv86IAzFzrD1Z2io/c7yqA==\n          ETag:                             5ca67960a586723b7344afffc81\n          Generation:                       1378862725952000\n          Metageneration:                   1\n          ACL:                              [\n    {\n      \"entity\": \"project-owners-867484910061\",\n      \"projectTeam\": {\n        \"projectNumber\": \"867484910061\",\n        \"team\": \"owners\"\n      },\n      \"role\": \"OWNER\"\n    },\n    {\n      \"email\": \"jane@gmail.com\",\n      \"entity\": \"user-jane@gmail.com\",\n      \"role\": \"OWNER\"\n    }\n  ]\n  TOTAL: 1 objects, 60183 bytes (58.77 KiB)\n\nNote that results may contain additional fields, such as custom metadata or\na storage class update time, if they are applicable to the object.\n\nAlso note that some fields, such as update time, are not available with the\n(non-default) XML API.\n\nSee also \"gsutil help acl\" for getting a more readable version of the ACL.",
        "PROVIDERS, BUCKETS, SUBDIRECTORIES, AND OBJECTS": "If you run gsutil ls without URLs, it lists all of the Google Cloud Storage\nbuckets under your default project ID:\n\n  gsutil ls\n\n(For details about projects, see \"gsutil help projects\" and also the -p\noption in the OPTIONS section below.)\n\nIf you specify one or more provider URLs, gsutil ls will list buckets at\neach listed provider:\n\n  gsutil ls gs://\n\nIf you specify bucket URLs, gsutil ls will list objects at the top level of\neach bucket, along with the names of each subdirectory. For example:\n\n  gsutil ls gs://bucket\n\nmight produce output like:\n\n  gs://bucket/obj1.htm\n  gs://bucket/obj2.htm\n  gs://bucket/images1/\n  gs://bucket/images2/\n\nThe \"/\" at the end of the last 2 URLs tells you they are subdirectories,\nwhich you can list using:\n\n  gsutil ls gs://bucket/images*\n\nIf you specify object URLs, gsutil ls will list the specified objects. For\nexample:\n\n  gsutil ls gs://bucket/*.txt\n\nwill list all files whose name matches the above wildcard at the top level\nof the bucket.\n\nSee \"gsutil help wildcards\" for more details on working with wildcards."
      }
    },
    "mb": {
      "capsule": "Make buckets",
      "commands": {},
      "flags": {
        "--retention": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "time Specifies the retention policy. Default is no retention\n policy. This can only be set on gs:// buckets and\n requires using the JSON API.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "--retention",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-b": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<on|off> Specifies the uniform bucket-level access setting.\n Default is \"off\"",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-b",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "class Specifies the default storage class.\n Default is \"Standard\".",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-l": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "location Can be any supported location. See\n https://cloud.google.com/storage/docs/locations\n for a discussion of this distinction. Default is US.\n Locations are case insensitive.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-l",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "proj_id Specifies the project ID under which to create the\n bucket.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "class Same as -c.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "mb"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "BUCKET-LEVEL ACCESS": "You can specify one of the available settings for a bucket\nwith the -b option.\n\nExamples:\n\n  gsutil mb -b off gs://bucket-with-acls\n\n  gsutil mb -b on gs://bucket-with-no-acls",
        "DESCRIPTION": "The mb command creates a new bucket. Google Cloud Storage has a single\nnamespace, so you are not allowed to create a bucket with a name already\nin use by another user. You can, however, carve out parts of the bucket name\nspace corresponding to your company's domain name (see \"gsutil help naming\").\n\nIf you don't specify a project ID using the -p option, the bucket is created\nusing the default project ID specified in your gsutil configuration file\n(see \"gsutil help config\"). For more details about projects see \"gsutil help\nprojects\".\n\nThe -c and -l options specify the storage class and location, respectively,\nfor the bucket. Once a bucket is created in a given location and with a\ngiven storage class, it cannot be moved to a different location, and the\nstorage class cannot be changed. Instead, you would need to create a new\nbucket and move the data over and then delete the original bucket.\n\nThe --retention option specifies the retention period for the bucket. For more\ndetails about retention policy see \"gsutil help retention\".\n\nThe -b option specifies the uniform bucket-level access setting of the bucket.\nACLs assigned to objects are not evaluated in buckets with uniform bucket-\nlevel access enabled. Consequently, only IAM policies grant access to objects\nin these buckets.",
        "LOCATIONS": "You can specify one of the `available locations\n<https://cloud.google.com/storage/docs/locations>`_ for a bucket\nwith the -l option.\n\nExamples:\n\n  gsutil mb -l asia gs://some-bucket\n\n  gsutil mb -c standard -l us-east1 gs://some-bucket\n\nIf you don't specify a -l option, the bucket is created in the default\nlocation (US).",
        "STORAGE CLASSES": "You can specify one of the `storage classes\n<https://cloud.google.com/storage/docs/storage-classes>`_ for a bucket\nwith the -c option.\n\nExample:\n\n  gsutil mb -c nearline gs://some-bucket\n\nSee online documentation for\n`pricing <https://cloud.google.com/storage/pricing>`_ and\n`SLA <https://cloud.google.com/storage/sla>`_ details.\n\nIf you don't specify a -c option, the bucket is created with the\ndefault storage class Standard Storage."
      }
    },
    "mv": {
      "capsule": "Move/rename objects",
      "commands": {},
      "flags": {},
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "mv"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The gsutil mv command allows you to move data between your local file\nsystem and the cloud, move data within the cloud, and move data between\ncloud storage providers. For example, to move all objects from a\nbucket to a local directory you could use:\n\n  gsutil mv gs://my_bucket/* dir\n\nSimilarly, to move all objects from a local directory to a bucket you could\nuse:\n\n  gsutil mv ./dir gs://my_bucket",
        "GROUPS OF OBJECTS": "You can use the gsutil mv command to rename all objects with a given prefix\nto have a new prefix. This is accomplished by copying each object to a new\nobject with the desired name and deleting the old one. For example, the\ncommand:\n\n  gsutil mv gs://my_bucket/oldprefix gs://my_bucket/newprefix\n\nwould rename all objects under gs://my_bucket/oldprefix to be under\ngs://my_bucket/newprefix, otherwise preserving the naming structure.\n\nIf you do a rename as specified above and you want to preserve ACLs, you\nshould use the -p option (see OPTIONS).\n\nNote that when using mv to rename groups of objects with a common prefix\nyou cannot specify the source URL using wildcards. You need to spell out\nthe complete name:\n\n  gsutil mv gs://my_bucket/oldprefix gs://my_bucket/newprefix\n\nIf you have a large number of files to move you might want to use the\ngsutil -m option, to perform a multi-threaded/multi-processing move:\n\n  gsutil -m mv gs://my_bucket/oldprefix gs://my_bucket/newprefix",
        "OPERATION": "Unlike the case with many file systems, the gsutil mv command does not\nperform a single atomic operation. Rather, it performs a copy from source\nto destination followed by removing the source for each object.\n\nA consequence of this is that, in addition to normal network and operation\ncharges, if you move a Nearline Storage, Coldline Storage, or Archive Storage\nobject, deletion and data retrieval charges apply. See the `documentation\n<https://cloud.google.com/storage/pricing>`_ for pricing details."
      }
    },
    "notification": {
      "capsule": "Configure object change notification",
      "commands": {
        "create": {
          "capsule": "Configure object change notification",
          "commands": {},
          "flags": {
            "-e": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Specify an event type filter for this notification config. Cloud\nage will only send notifications of this type. You may specify\n parameter multiple times to allow multiple event types. If not\nified, Cloud Storage will send notifications for all event\ns. The valid types are:\nJECT_FINALIZE - An object has been created.\nJECT_METADATA_UPDATE - The metadata of an object has changed.\nJECT_DELETE - An object has been permanently deleted.\nJECT_ARCHIVE - A live version of an object has become a\nnoncurrent version.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-e",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-f": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Specifies the payload format of notification messages. Must be\ner \"json\" for a payload matches the object metadata for the\n API, or \"none\" to specify no payload at all. In either case,\nfication details are available in the message attributes.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-f",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-m": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Specifies a key:value attribute that will be appended to the set\nttributes sent to Cloud Pub/Sub for all events associated with\n notification config. You may specify this parameter multiple\ns to set multiple attributes.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-m",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-p": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Specifies a prefix path filter for this notification config. Cloud\nage will only send notifications for objects in this bucket\ne names begin with the specified prefix.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-p",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-s": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "Skips creation and permission assignment of the Cloud Pub/Sub topic.\n is useful if the caller does not have permission to access\ntopic in question, or if the topic already exists and has the\nopriate publish permission assigned.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-s",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-t": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "The Cloud Pub/Sub topic to which notifications should be sent. If\nspecified, this command will choose a topic whose project is\n default project and whose ID is the same as the Cloud Storage\net name.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-t",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "notification",
            "create"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The create sub-command creates a notification config on a bucket, establishing\na flow of event notifications from Cloud Storage to a Cloud Pub/Sub topic. As\npart of creating this flow, the create command also verifies that the\ndestination Cloud Pub/Sub topic exists, creating it if necessary, and verifies\nthat the Cloud Storage bucket has permission to publish events to that topic,\ngranting the permission if necessary.\n\nIf a destination Cloud Pub/Sub topic is not specified with the -t flag, Cloud\nStorage will by default choose a topic name in the default project whose ID is\nthe same the bucket name. For example, if the default project ID specified is\n'default-project' and the bucket being configured is gs://example-bucket, the\ncreate command will use the Cloud Pub/Sub topic\n\"projects/default-project/topics/example-bucket\".\n\nIn order to enable notifications, a `special Cloud Storage service account\n<https://cloud.google.com/storage/docs/projects#service-accounts>`_ unique to\neach project must have the IAM permission \"pubsub.topics.publish\". This\ncommand will check to see if that permission exists and, if not, will attempt\nto grant it.\n\nYou can create multiple notification configurations for a bucket, but their\ntriggers cannot overlap such that a single event could send multiple\nnotifications. Attempting to create a notification configuration that\noverlaps with an existing notification configuration results in an error.",
            "EXAMPLES": "Begin sending notifications of all changes to the bucket example-bucket\nto the Cloud Pub/Sub topic projects/default-project/topics/example-bucket:\n\n  gsutil notification create -f json gs://example-bucket\n\nThe same as above, but specifies the destination topic ID 'files-to-process'\nin the default project:\n\n  gsutil notification create -f json \\\n    -t files-to-process gs://example-bucket\n\nThe same as above, but specifies a Cloud Pub/Sub topic belonging to the\nspecific cloud project 'example-project':\n\n  gsutil notification create -f json \\\n    -t projects/example-project/topics/files-to-process gs://example-bucket\n\nCreate a notification config that will only send an event when a new object\nhas been created:\n\n  gsutil notification create -f json -e OBJECT_FINALIZE gs://example-bucket\n\nCreate a topic and notification config that will only send an event when\nan object beginning with \"photos/\" is affected:\n\n  gsutil notification create -p photos/ gs://example-bucket\n\nList all of the notificationConfigs in bucket example-bucket:\n\n  gsutil notification list gs://example-bucket\n\nDelete all notitificationConfigs for bucket example-bucket:\n\n  gsutil notification delete gs://example-bucket\n\nDelete one specific notificationConfig for bucket example-bucket:\n\n  gsutil notification delete \\\n    projects/_/buckets/example-bucket/notificationConfigs/1",
            "STEPS": "Once the create command has succeeded, Cloud Storage will publish a message to\nthe specified Cloud Pub/Sub topic when eligible changes occur. In order to\nreceive these messages, you must create a Pub/Sub subscription for your\nPub/Sub topic. To learn more about creating Pub/Sub subscriptions, see `the\nPub/Sub Subscriber Overview <https://cloud.google.com/pubsub/docs/subscriber>`_.\n\nYou can create a simple Pub/Sub subscription using the ``gcloud`` command-line\ntool. For example, to create a new subscription on the topic \"myNewTopic\" and\nattempt to pull messages from it, you could run:\n\n  gcloud beta pubsub subscriptions create --topic myNewTopic testSubscription\n  gcloud beta pubsub subscriptions pull --auto-ack testSubscription"
          }
        },
        "delete": {
          "capsule": "Configure object change notification",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "notification",
            "delete"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The delete sub-command deletes notification configs from a bucket. If a\nnotification config name is passed as a parameter, that notification config\nalone will be deleted. If a bucket name is passed, all notification configs\nassociated with that bucket will be deleted.\n\nCloud Pub/Sub topics associated with this notification config will not be\ndeleted by this command. Those must be deleted separately, for example with\nthe gcloud command `gcloud beta pubsub topics delete`.\n\nObject Change Notification subscriptions cannot be deleted with this command.\nFor that, see the command `gsutil notification stopchannel`.",
            "EXAMPLES": "Delete a single notification config (with ID 3) in the bucket example-bucket:\n\n  gsutil notification delete projects/_/buckets/example-bucket/notificationConfigs/3\n\nDelete all notification configs in the bucket example-bucket:\n\n  gsutil notification delete gs://example-bucket"
          }
        },
        "list": {
          "capsule": "Configure object change notification",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "notification",
            "list"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The list sub-command provides a list of notification configs belonging to a\ngiven bucket. The listed name of each notification config can be used with\nthe delete sub-command to delete that specific notification config.\n\nFor listing Object Change Notifications instead of Cloud Pub/Sub notification\nsubscription configs, add a -o flag.",
            "EXAMPLES": "Fetch the list of notification configs for the bucket example-bucket:\n\n  gsutil notification list gs://example-bucket\n\nThe same as above, but for Object Change Notifications instead of Cloud\nPub/Sub notification subscription configs:\n\n  gsutil notification list -o gs://example-bucket\n\nFetch the notification configs in all buckets matching a wildcard:\n\n  gsutil notification list gs://example-*\n\nFetch all of the notification configs for buckets in the default project:\n\n  gsutil notification list gs://*"
          }
        }
      },
      "flags": {
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specify an event type filter for this notification config. Cloud\nage will only send notifications of this type. You may specify\n parameter multiple times to allow multiple event types. If not\nified, Cloud Storage will send notifications for all event\ns. The valid types are:\nJECT_FINALIZE - An object has been created.\nJECT_METADATA_UPDATE - The metadata of an object has changed.\nJECT_DELETE - An object has been permanently deleted.\nJECT_ARCHIVE - A live version of an object has become a\nnoncurrent version.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies the payload format of notification messages. Must be\ner \"json\" for a payload matches the object metadata for the\n API, or \"none\" to specify no payload at all. In either case,\nfication details are available in the message attributes.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-m": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies a key:value attribute that will be appended to the set\nttributes sent to Cloud Pub/Sub for all events associated with\n notification config. You may specify this parameter multiple\ns to set multiple attributes.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-m",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies a prefix path filter for this notification config. Cloud\nage will only send notifications for objects in this bucket\ne names begin with the specified prefix.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Skips creation and permission assignment of the Cloud Pub/Sub topic.\n is useful if the caller does not have permission to access\ntopic in question, or if the topic already exists and has the\nopriate publish permission assigned.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-t": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "The Cloud Pub/Sub topic to which notifications should be sent. If\nspecified, this command will choose a topic whose project is\n default project and whose ID is the same as the Cloud Storage\net name.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-t",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "notification"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "AND PARALLEL COMPOSITE UPLOADS": "By default, gsutil enables parallel composite uploads for large files (see\n\"gsutil help cp\"), which means that an upload of a large object can result\nin multiple temporary component objects being uploaded before the actual\nintended object is created. Any subscriber to notifications for this bucket\nwill then see a notification for each of these components being created and\ndeleted. If this is a concern for you, note that parallel composite uploads\ncan be disabled by setting \"parallel_composite_upload_threshold = 0\" in your\nboto config file. Alternately, your subscriber code can filter out gsutil's\nparallel composite uploads by ignoring any notification about objects whose\nnames contain (but do not start with) the following string:\n  \"/gsutil/tmp/parallel_composite_uploads/for_details_see/gsutil_help_cp/\".",
        "CHANGE NOTIFICATIONS": "For more information on the Object Change Notification feature, please see\n`the Object Change Notification docs\n<https://cloud.google.com/storage/docs/object-change-notification>`_.\n\nThe \"watchbucket\" and \"stopchannel\" sub-commands enable and disable Object\nChange Notifications.",
        "CREATE": "The create sub-command creates a notification config on a bucket, establishing\na flow of event notifications from Cloud Storage to a Cloud Pub/Sub topic. As\npart of creating this flow, the create command also verifies that the\ndestination Cloud Pub/Sub topic exists, creating it if necessary, and verifies\nthat the Cloud Storage bucket has permission to publish events to that topic,\ngranting the permission if necessary.\n\nIf a destination Cloud Pub/Sub topic is not specified with the -t flag, Cloud\nStorage will by default choose a topic name in the default project whose ID is\nthe same the bucket name. For example, if the default project ID specified is\n'default-project' and the bucket being configured is gs://example-bucket, the\ncreate command will use the Cloud Pub/Sub topic\n\"projects/default-project/topics/example-bucket\".\n\nIn order to enable notifications, a `special Cloud Storage service account\n<https://cloud.google.com/storage/docs/projects#service-accounts>`_ unique to\neach project must have the IAM permission \"pubsub.topics.publish\". This\ncommand will check to see if that permission exists and, if not, will attempt\nto grant it.\n\nYou can create multiple notification configurations for a bucket, but their\ntriggers cannot overlap such that a single event could send multiple\nnotifications. Attempting to create a notification configuration that\noverlaps with an existing notification configuration results in an error.",
        "DELETE": "The delete sub-command deletes notification configs from a bucket. If a\nnotification config name is passed as a parameter, that notification config\nalone will be deleted. If a bucket name is passed, all notification configs\nassociated with that bucket will be deleted.\n\nCloud Pub/Sub topics associated with this notification config will not be\ndeleted by this command. Those must be deleted separately, for example with\nthe gcloud command `gcloud beta pubsub topics delete`.\n\nObject Change Notification subscriptions cannot be deleted with this command.\nFor that, see the command `gsutil notification stopchannel`.",
        "DESCRIPTION": "The notification command is used to configure Google Cloud Storage support for\nsending notifications to Cloud Pub/Sub as well as to configure the object\nchange notification feature.",
        "EXAMPLES": "Stop the notification event channel with channel identifier channel1 and\nresource identifier SoGqan08XDIFWr1Fv_nGpRJBHh8:\n\n  gsutil notification stopchannel channel1 SoGqan08XDIFWr1Fv_nGpRJBHh8",
        "LIST": "The list sub-command provides a list of notification configs belonging to a\ngiven bucket. The listed name of each notification config can be used with\nthe delete sub-command to delete that specific notification config.\n\nFor listing Object Change Notifications instead of Cloud Pub/Sub notification\nsubscription configs, add a -o flag.",
        "PUB/SUB": "The \"create\", \"list\", and \"delete\" sub-commands deal with configuring Cloud\nStorage integration with Google Cloud Pub/Sub.",
        "STEPS": "Once the create command has succeeded, Cloud Storage will publish a message to\nthe specified Cloud Pub/Sub topic when eligible changes occur. In order to\nreceive these messages, you must create a Pub/Sub subscription for your\nPub/Sub topic. To learn more about creating Pub/Sub subscriptions, see `the\nPub/Sub Subscriber Overview <https://cloud.google.com/pubsub/docs/subscriber>`_.\n\nYou can create a simple Pub/Sub subscription using the ``gcloud`` command-line\ntool. For example, to create a new subscription on the topic \"myNewTopic\" and\nattempt to pull messages from it, you could run:\n\n  gcloud beta pubsub subscriptions create --topic myNewTopic testSubscription\n  gcloud beta pubsub subscriptions pull --auto-ack testSubscription",
        "STOPCHANNEL": "The stopchannel sub-command can be used to stop sending change events to a\nnotification channel.\n\nThe channel_id and resource_id parameters should match the values from the\nresponse of a bucket watch request.",
        "WATCHBUCKET": "The watchbucket sub-command can be used to watch a bucket for object changes.\nA service account must be used when running this command.\n\nThe app_url parameter must be an HTTPS URL to an application that will be\nnotified of changes to any object in the bucket. The URL endpoint must be\na verified domain on your project. See `Notification Authorization\n<https://cloud.google.com/storage/docs/object-change-notification#_Authorization>`_\nfor details.\n\nThe optional id parameter can be used to assign a unique identifier to the\ncreated notification channel. If not provided, a random UUID string will be\ngenerated.\n\nThe optional token parameter can be used to validate notifications events.\nTo do this, set this custom token and store it to later verify that\nnotification events contain the client token you expect."
      }
    },
    "perfdiag": {
      "capsule": "Run performance diagnostic",
      "commands": {},
      "flags": {
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the number of processes to use while running throughput\nperiments. The default value is 1.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the directory to store temporary local files in. If not\necified, a default temporary directory will be used.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-i": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Reads the JSON output file created using the -o command and prints\nformatted description of the results.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-i",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-j": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Applies gzip transport encoding and sets the target compression\ntio for the generated test files. This ratio can be an integer\ntween 0 and 100 (inclusive), with 0 generating a file with\niform data, and 100 generating random data. When you specify\ne -j option, files being uploaded are compressed in-memory and\n-the-wire only. See cp -j for specific semantics.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-j",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-k": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the number of threads per process to use while running\nroughput experiments. Each process will receive an equal number\n threads. The default value is 1.\nTE: All specified threads and processes will be created, but may\nt by saturated with work if too few objects (specified with -n)\nd too few components (specified with -y) are specified.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-k",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-m": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Adds metadata to the result JSON file. Multiple -m values can be\necified. Example:\n gsutil perfdiag -m \"key1:val1\" -m \"key2:val2\" gs://bucketname\nch metadata key will be added to the top-level \"metadata\"\nctionary in the output JSON file.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-m",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-n": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the number of objects to use when downloading and uploading\nles during tests. Defaults to 5.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-n",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-o": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Writes the results of the diagnostic to an output file. The output\n a JSON file containing system information and performance\nagnostic results. The file can be read and reported later using\ne -i option.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-o",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the type of parallelism to be used (only applicable when\nreads or processes are specified and threads * processes > 1).\ne default is to use fan. Must be one of the following:\nn\n Use one thread per object. This is akin to using gsutil -m cp,\n with sliced object download / parallel composite upload\n disabled.\nice\n Use Y (specified with -y) threads for each object, transferring\n one object at a time. This is akin to using parallel object\n download / parallel composite upload, without -m. Sliced\n uploads not supported for s3.\nth\n Use Y (specified with -y) threads for each object, transferring\n multiple objects at a time. This is akin to simultaneously\n using sliced object download / parallel composite upload and\n gsutil -m cp. Sliced uploads not supported for s3.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the size (in bytes) for each of the N (set with -n) objects\ned in the read and write throughput tests. The default is 1 MiB.\nis can also be specified using byte suffixes such as 500K or 1M.\nTE: these values are interpreted as multiples of 1024 (K=1024,\n1024*1024, etc.)\nTE: If rthru_file or wthru_file are performed, N (set with -n)\nmes as much disk space as specified will be required for the\neration.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-t": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the list of diagnostic tests to perform. The default is to\nn the lat, rthru, and wthru diagnostic tests. Must be a\nmma-separated list containing one or more of the following:\nt\n For N (set with -n) objects, write the object, retrieve its\n metadata, read the object, and finally delete the object.\n Record the latency of each operation.\nst\n Write N (set with -n) objects to the bucket, record how long\n it takes for the eventually consistent listing call to return\n the N objects in its result, delete the N objects, then record\n how long it takes listing to stop returning the N objects.\nhru\n Runs N (set with -n) read operations, with at most C\n (set with -c) reads outstanding at any given time.\nhru_file\n The same as rthru, but simultaneously writes data to the disk,\n to gauge the performance impact of the local disk on downloads.\nhru\n Runs N (set with -n) write operations, with at most C\n (set with -c) writes outstanding at any given time.\nhru_file\n The same as wthru, but simultaneously reads data from the disk,\n to gauge the performance impact of the local disk on uploads.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-t",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-y": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Sets the number of slices to divide each file/object into while\nansferring data. Only applicable with the slice (or both)\nrallelism type. The default is 4 slices.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-y",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "perfdiag"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "AVAILABILITY": "The perfdiag command ignores the boto num_retries configuration parameter.\nInstead, it always retries on HTTP errors in the 500 range and keeps track of\nhow many 500 errors were encountered during the test. The availability\nmeasurement is reported at the end of the test.\n\nNote that HTTP responses are only recorded when the request was made in a\nsingle process. When using multiple processes or threads, read and write\nthroughput measurements are performed in an external process, so the\navailability numbers reported won't include the throughput measurements.",
        "DESCRIPTION": "The perfdiag command runs a suite of diagnostic tests for a given Google\nStorage bucket.\n\nThe 'url' parameter must name an existing bucket (e.g. gs://foo) to which\nthe user has write permission. Several test files will be uploaded to and\ndownloaded from this bucket. All test files will be deleted at the completion\nof the diagnostic if it finishes successfully.\n\ngsutil performance can be impacted by many factors at the client, server,\nand in-between, such as: CPU speed; available memory; the access path to the\nlocal disk; network bandwidth; contention and error rates along the path\nbetween gsutil and Google; operating system buffering configuration; and\nfirewalls and other network elements. The perfdiag command is provided so\nthat customers can run a known measurement suite when troubleshooting\nperformance problems.",
        "DIAGNOSTIC OUTPUT TO GOOGLE CLOUD STORAGE TEAM": "If the Google Cloud Storage Team asks you to run a performance diagnostic\nplease use the following command, and email the output file (output.json)\nto gs-team@google.com:\n\n  gsutil perfdiag -o output.json gs://your-bucket",
        "NOTE": "The perfdiag command collects system information. It collects your IP address,\nexecutes DNS queries to Google servers and collects the results, collects\nnetwork statistics information from the output of netstat -s, and looks at the\nBIOS product name string. It will also attempt to connect to your proxy server\nif you have one configured and will look up the location and storage class of\nthe bucket being used for performance testing. None of this information will\nbe sent to Google unless you choose to send it."
      }
    },
    "rb": {
      "capsule": "Remove buckets",
      "commands": {},
      "flags": {
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Continues silently (without printing error messages) despite\nrors when removing buckets. If some buckets couldn't be removed,\nutil's exit status will be non-zero even if this flag is set.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "rb"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The rb command deletes a bucket. Buckets must be empty before you can delete\nthem.\n\nBe certain you want to delete a bucket before you do so, as once it is\ndeleted the name becomes available and another user may create a bucket with\nthat name. (But see also \"DOMAIN NAMED BUCKETS\" under \"gsutil help naming\"\nfor help carving out parts of the bucket name space.)"
      }
    },
    "requesterpays": {
      "capsule": "Enable or disable requester pays for one or more buckets",
      "commands": {
        "get": {
          "capsule": "Enable or disable requester pays for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "requesterpays",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"get\" sub-command gets the requester pays configuration for a\nbucket and displays whether or not it is enabled."
          }
        },
        "set": {
          "capsule": "Enable or disable requester pays for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "requesterpays",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"set\" sub-command requires an additional sub-command, either \"on\" or\n\"off\", which, respectively, will enable or disable requester pays for the\nspecified bucket(s)."
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "requesterpays"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The Requester Pays Configuration feature enables you to configure a Google\nCloud Storage bucket to indicate that the requester will pay all costs\nrelated to accessing the bucket and its objects.\n\nThe gsutil requesterpays command has two sub-commands:",
        "GET": "The \"get\" sub-command gets the requester pays configuration for a\nbucket and displays whether or not it is enabled.",
        "SET": "The \"set\" sub-command requires an additional sub-command, either \"on\" or\n\"off\", which, respectively, will enable or disable requester pays for the\nspecified bucket(s)."
      }
    },
    "retention": {
      "capsule": "Provides utilities to interact with Retention Policy feature.",
      "commands": {},
      "flags": {},
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "retention"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CLEAR": "The \"gsutil retention clear\" command removes an unlocked Retention Policy\nfrom one or more buckets. A locked Retention Policy cannot be removed or\nreduced in duration.",
        "DESCRIPTION": "",
        "EVENT": "The \"gsutil retention event\" command will enable or disable a Event-Based\nHold on an object.",
        "EVENT-DEFAULT": "The \"gsutil retention event-default\" command sets the default value for an\nEvent-Based Hold on one or more buckets.\n\nBy setting the default Event-Based Hold on a bucket, newly created objects\nwill inherit that value as their Event-Based Hold (it is not applied\nretroactively).",
        "EXAMPLES": "Setting the Temporary Hold on object(s):\n\n  gsutil retention temp set gs://my-bucket/my-object\n\nReleasing the Temporary Hold on object(s):\n\n  gsutil retention temp release gs://my-bucket/my-object\n\nYou can also provide a precondition on an object's meta-generation in order to\navoid potential race conditions. You can use gsutil's '-h' option to specify\npreconditions. For example, the following specifies a precondition that checks\nan object's metageneration before setting the Event-Based hold on the object:\n\n  gsutil -h \"x-goog-if-metageneration-match: 1\" \\\n    retention temp set gs://my-bucket/my-object\n\nIf you want to (un)set a Temporary Hold on a large number of objects, then\nyou might want to use the top-level '-m' option to perform a parallel update.\nFor example, the following command sets a Temporary Hold on objects ending\nwith .jpg in parallel, in the root folder:\n\n  gsutil -m retention temp set gs://bucket/*.jpg",
        "FORMATS": "Formats for the \"set\" subcommand include:\n\n<number>s\n    Specifies retention period of <number> seconds for objects in this bucket.\n\n<number>d\n    Specifies retention period of <number> days for objects in this bucket.\n\n<number>m\n    Specifies retention period of <number> months for objects in this bucket.\n\n<number>y\n    Specifies retention period of <number> years for objects in this bucket.\n\nGCS JSON API accepts retention period as number of seconds. Durations provided\nin terms of days, months or years will be converted to their rough equivalent\nvalues in seconds, using the following conversions:\n\n- A month is considered to be 31 days or 2,678,400 seconds.\n- A year is considered to be 365.25 days or 31,557,600 seconds.\n\nFor more information, see the `Bucket Lock documentation\n<https://cloud.google.com/storage/docs/bucket-lock>`_.\n\nProvided retention period must be greater than 0 and less than 100 years.\nClients may define retention duration only in one form (seconds, days, months,\nor years) and not a combination of them.\n\nIt is important to note that, while it is possible to specify durations\nshorter than a day (using seconds), enforcement of such retentions is not\nguaranteed. Such durations may only be used for testing purposes.",
        "GET": "The \"gsutil retention get\" command retrieves the Retention Policy for given\nbucket and displays a human-readable representation of the configuration.",
        "LOCK": "The \"gsutil retention lock\" command will PERMANENTLY lock unlocked\nRetention Policy on one or more buckets.\n\nCAUTION: A locked Retention Policy cannot be removed from a bucket or reduced\nin duration. Once locked, deleting the bucket is the only way to \"remove\" a\nRetention Policy.",
        "SET": "The \"gsutil retention set\" command will allow you to set or update the\nRetention Policy on one or more buckets.\n\nIf you would like to remove an unlocked Retention Policy from one or more\nbuckets, use the \"gsutil retention clear\" command.\n\nThe \"set\" sub-command can set retention policy with the following formats:",
        "TEMP": "The \"gsutil retention temp\" command will enable or disable a Temporary Hold\non an object."
      }
    },
    "rewrite": {
      "capsule": "Rewrite objects",
      "commands": {},
      "flags": {
        "-I": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes gsutil to read the list of objects to rewrite from stdin.\nThis allows you to run a program that generates the list of\nobjects to rewrite.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-I",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-O": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Rewrite objects with the bucket's default object ACL instead of\nthe existing object ACL. This is needed if you do not have\nOWNER permission on the object.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-O",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Continues silently (without printing error messages) despite\nerrors when rewriting multiple objects. If some of the objects\ncould not be rewritten, gsutil's exit status will be non-zero\neven if this flag is set. This option is implicitly set when\nrunning \"gsutil -m rewrite ...\".",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-k": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Rewrite objects with the current encryption key specified in\nyour boto configuration file. The value for encryption_key may\nbe either a base64-encoded CSEK or a fully-qualified KMS key\nname. If encryption_key is specified, encrypt all objects with\nthis key. If encryption_key is unspecified, customer-managed or\ncustomer-supplied encryption keys that were used on the original\nobjects aren't used for the rewritten objects. Instead,\nrewritten objects are encrypted with either the bucket's default\nKMS key (if one is set) or Google-managed encryption (no CSEK\nor CMEK). See 'gsutil help encryption' for details on encryption\nconfiguration.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-k",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "The -R and -r options are synonymous. Causes bucket or bucket\nsubdirectory contents to be rewritten recursively.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<class> Rewrite objects using the specified storage class.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "rewrite"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The gsutil rewrite command rewrites cloud objects, applying the specified\ntransformations to them. The transformation(s) are atomic and\napplied based on the input transformation flags. Object metadata values are\npreserved unless altered by a transformation.\n\nThe -k flag is supported to add, rotate, or remove encryption keys on\nobjects.  For example, the command:\n\n  gsutil rewrite -k gs://bucket/**\n\nwill update all objects in gs://bucket with the current encryption key\nfrom your boto config file, which may either be a base64-encoded CSEK or the\nfully-qualified name of a Cloud KMS key.\n\nYou can also use the -r option to specify recursive object transform; this is\nsynonymous with the ** wildcard. Thus, either of the following two commands\nwill perform encryption key transforms on gs://bucket/subdir and all objects\nand subdirectories under it:\n\n  gsutil rewrite -k gs://bucket/subdir**\n  gsutil rewrite -k -r gs://bucket/subdir\n\nThe rewrite command acts only on live object versions, so specifying a\nURL with a generation number fails. If you want to rewrite a noncurrent\nversion, first copy it to the live version, then rewrite it, for example:\n\n  gsutil cp gs://bucket/object#123 gs://bucket/object\n  gsutil rewrite -k gs://bucket/object\n\nYou can use the -s option to specify a new storage class for objects.  For\nexample, the command:\n\n  gsutil rewrite -s nearline gs://bucket/foo\n\nwill rewrite the object, changing its storage class to nearline.\n\nIf you specify the -k option and you have an encryption key set in your boto\nconfiguration file, the rewrite command will skip objects that are already\nencrypted with the specifed key.  For example, if you run:\n\n  gsutil rewrite -k gs://bucket/**\n\nand gs://bucket contains objects encrypted with the key specified in your boto\nconfiguration file, gsutil will skip rewriting those objects and only rewrite\nobjects that are not encrypted with the specified key. This avoids the cost of\nperforming redundant rewrite operations.\n\nIf you specify the -k option and you do not have an encryption key set in your\nboto configuration file, gsutil will always rewrite each object, without\nexplicitly specifying an encryption key. This results in rewritten objects\nbeing encrypted with either the bucket's default KMS key (if one is set) or\nGoogle-managed encryption (no CSEK or CMEK). Gsutil does not attempt to\ndetermine whether the operation is redundant (and thus skippable) because\ngsutil cannot be sure how the object will be encrypted after the rewrite. Note\nthat if your goal is to encrypt objects with a bucket's default KMS key, you\ncan avoid redundant rewrite costs by specifying the bucket's default KMS key\nin your boto configuration file; this allows gsutil to perform an accurate\ncomparison of the objects' current and desired encryption configurations and\nskip rewrites for objects already encrypted with that key.\n\nIf have an encryption key set in your boto configuration file and specify\nmultiple transformations, gsutil will only skip those that would not change\nthe object's state. For example, if you run:\n\n  gsutil rewrite -s nearline -k gs://bucket/**\n\nand gs://bucket contains objects that already match the encryption\nconfiguration but have a storage class of standard, the only transformation\napplied to those objects would be the change in storage class.\n\nYou can pass a list of URLs (one per line) to rewrite on stdin instead of as\ncommand line arguments by using the -I option. This allows you to use gsutil\nin a pipeline to rewrite objects identified by a program, such as:\n\n  some_program | gsutil -m rewrite -k -I\n\nThe contents of stdin can name cloud URLs and wildcards of cloud URLs.\n\nThe rewrite command requires OWNER permissions on each object to preserve\nobject ACLs. You can bypass this by using the -O flag, which will cause\ngsutil not to read the object's ACL and instead apply the default object ACL\nto the rewritten object:\n\n  gsutil rewrite -k -O gs://bucket/**"
      }
    },
    "rm": {
      "capsule": "Remove objects",
      "commands": {},
      "flags": {
        "-I": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes gsutil to read the list of objects to remove from stdin.\nis allows you to run a program that generates the list of\njects to remove.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-I",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Delete all versions of an object.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Continues silently (without printing error messages) despite\nrors when removing multiple objects. If some of the objects\nuld not be removed, gsutil's exit status will be non-zero even\n this flag is set. Execution will still halt if an inaccessible\ncket is encountered. This option is implicitly set when running\nsutil -m rm ...\".",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "The -R and -r options are synonymous. Causes bucket or bucket\nbdirectory contents (all objects and subdirectories that it\nntains) to be removed recursively. If used with a bucket-only\nL (like gs://bucket), after deleting objects and subdirectories\nutil will delete the bucket. This option implies the -a option\nd will delete all object versions.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "rm"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The gsutil rm command removes objects and/or buckets.\nFor example, the command:\n\n  gsutil rm gs://bucket/subdir/*\n\nwill remove all objects in gs://bucket/subdir, but not in any of its\nsub-directories. In contrast:\n\n  gsutil rm gs://bucket/subdir/**\n\nwill remove all objects under gs://bucket/subdir or any of its\nsubdirectories.\n\nYou can also use the -r option to specify recursive object deletion. Thus, for\nexample, either of the following two commands will remove gs://bucket/subdir\nand all objects and subdirectories under it:\n\n  gsutil rm gs://bucket/subdir**\n  gsutil rm -r gs://bucket/subdir\n\nThe -r option will also delete all object versions in the subdirectory for\nversioning-enabled buckets, whereas the ** command will only delete the live\nversion of each object in the subdirectory.\n\nRunning gsutil rm -r on a bucket will delete all versions of all objects in\nthe bucket, and then delete the bucket:\n\n  gsutil rm -r gs://bucket\n\nIf you want to delete all objects in the bucket, but not the bucket itself,\nthis command will work:\n\n  gsutil rm gs://bucket/**\n\nIf you have a large number of objects to remove you might want to use the\ngsutil -m option, to perform parallel (multi-threaded/multi-processing)\nremoves:\n\n  gsutil -m rm -r gs://my_bucket/subdir\n\nYou can pass a list of URLs (one per line) to remove on stdin instead of as\ncommand line arguments by using the -I option. This allows you to use gsutil\nin a pipeline to remove objects identified by a program, such as:\n\n  some_program | gsutil -m rm -I\n\nThe contents of stdin can name cloud URLs and wildcards of cloud URLs.\n\nNote that gsutil rm will refuse to remove files from the local\nfile system. For example this will fail:\n\n  gsutil rm *.txt\n\nWARNING: Object removal cannot be undone. Google Cloud Storage is designed\nto give developers a high amount of flexibility and control over their data,\nand Google maintains strict controls over the processing and purging of\ndeleted data. To protect yourself from mistakes, you can configure object\nversioning on your bucket(s). See 'gsutil help versions' for details.",
        "RESTORATION FROM ACCIDENTAL DELETION OR OVERWRITES": ""
      }
    },
    "rsync": {
      "capsule": "Synchronize content of two buckets/directories",
      "commands": {},
      "flags": {
        "-C": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "If an error occurs, continue to attempt to copy the remaining\n files. If errors occurred, gsutil's exit status will be\n non-zero even if this flag is set. This option is implicitly\n set when running \"gsutil -m rsync...\".\n NOTE: -C only applies to the actual copying operation. If an\n error occurs while iterating over the files in the local\n directory (e.g., invalid Unicode file name) gsutil will print\n an error message and abort.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-C",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-J": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Applies gzip transport encoding to file uploads. This option\n works like the -j option described above, but it applies to\n all uploaded files, regardless of extension.\n CAUTION: If you use this option and some of the source files\n don't compress well (e.g., that's often true of binary data),\n this option may result in longer uploads.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-J",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-P": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes POSIX attributes to be preserved when objects are\n copied. With this feature enabled, gsutil rsync will copy\n fields provided by stat. These are the user ID of the owner,\n the group ID of the owning group, the mode (permissions) of the\n file, and the access/modification time of the file. For\n downloads, these attributes will only be set if the source\n objects were uploaded with this flag enabled.\n On Windows, this flag will only set and restore access time and\n modification time. This is because Windows doesn't have a\n notion of POSIX uid/gid/mode.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-P",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-U": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Skip objects with unsupported object types instead of failing.\n Unsupported object types are Amazon S3 Objects in the GLACIER\n storage class.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-U",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-a": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "canned_acl Sets named canned_acl when uploaded objects created. See\n \"gsutil help acls\" for further details. Note that rsync will\n decide whether or not to perform a copy based only on object\n size and modification time, not current ACL state. Also see the\n -p option below.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-a",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes the rsync command to compute and compare checksums\n (instead of comparing mtime) for files if the size of source\n and destination match. This option increases local disk I/O and\n run time if either src_url or dst_url are on the local file\n system.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Delete extra files under dst_url not found under src_url. By\n default extra files are not deleted.\n NOTE: this option can delete data quickly if you specify the\n wrong source/destination combination. See the help section\n above, \"BE CAREFUL WHEN USING -d OPTION!\".",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Exclude symlinks. When specified, symbolic links will be\n ignored. Note that gsutil does not follow directory symlinks,\n regardless of whether -e is specified.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-j": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<ext,...> Applies gzip transport encoding to any file upload whose\n extension matches the -j extension list. This is useful when\n uploading files with compressible content (such as .js, .css,\n or .html files) because it saves network bandwidth while\n also leaving the data uncompressed in Google Cloud Storage.\n When you specify the -j option, files being uploaded are\n compressed in-memory and on-the-wire only. Both the local\n files and Cloud Storage objects remain uncompressed. The\n uploaded objects retain the Content-Type and name of the\n original files.\n Note that if you want to use the top-level -m option to\n parallelize copies along with the -j/-J options, you should\n prefer using multiple processes instead of multiple threads;\n when using -j/-J, multiple threads in the same process are\n bottlenecked by Python's GIL. Thread and process count can be\n set using the \"parallel_thread_count\" and\n \"parallel_process_count\" boto config options, e.g.:\n gsutil -o \"GSUtil:parallel_process_count=8\" \\\n -o \"GSUtil:parallel_thread_count=1\" \\\n -m rsync -j /local/source/dir gs://bucket/path",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-j",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-n": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes rsync to run in \"dry run\" mode, i.e., just outputting\n what would be copied or deleted without actually doing any\n copying/deleting.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-n",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Causes ACLs to be preserved when objects are copied. Note that\n rsync will decide whether or not to perform a copy based only\n on object size and modification time, not current ACL state.\n Thus, if the source and destination differ in size or\n modification time and you run gsutil rsync -p, the file will be\n copied and ACL preserved. However, if the source and\n destination don't differ in size or checksum but have different\n ACLs, running gsutil rsync -p will have no effect.\n Note that this option has performance and cost implications\n when using the XML API, as it requires separate HTTP calls for\n interacting with ACLs. The performance issue can be mitigated\n to some degree by using gsutil -m rsync to cause parallel\n synchronization. Also, this option only works if you have OWNER\n access to all of the objects that are copied.\n You can avoid the additional performance and cost of using\n rsync -p if you want all objects in the destination bucket to\n end up with the same ACL by setting a default object ACL on\n that bucket instead of using rsync -p. See 'gsutil help\n defacl'.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "The -R and -r options are synonymous. Causes directories,\n buckets, and bucket subdirectories to be synchronized\n recursively. If you neglect to use this option gsutil will make\n only the top-level directory in the source and destination URLs\n match, skipping any sub-directories.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-u": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "When a file/object is present in both the source and\n destination, if mtime is available for both, do not perform\n the copy if the destination mtime is newer.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-u",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-x": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "pattern Causes files/objects matching pattern to be excluded, i.e., any\n matching files/objects will not be copied or deleted. Note that\n the pattern is a Python regular expression, not a wildcard (so,\n matching any string ending in \"abc\" would be specified using\n \".*abc$\" rather than \"*abc\"). Note also that the exclude path\n is always relative (similar to Unix rsync or tar exclude\n options). For example, if you run the command:\n gsutil rsync -x \"data./.*\\.txt$\" dir gs://my-bucket\n it will skip the file dir/data1/a.txt.\n You can use regex alternation to specify multiple exclusions,\n for example:\n gsutil rsync -x \".*\\.txt$|.*\\.jpg$\" dir gs://my-bucket\n will skip all .txt and .jpg files in dir.\n NOTE: When using this on the Windows command line, use ^ as an\n escape character instead of \\ and escape the | character.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-x",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "rsync"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "CAREFUL WHEN SYNCHRONIZING OVER OS-SPECIFIC FILE TYPES (SYMLINKS, DEVICES, ETC.)": "Running gsutil rsync over a directory containing operating system-specific\nfile types (symbolic links, device files, sockets, named pipes, etc.) can\ncause various problems. For example, running a command like:\n\n  gsutil rsync -r ./dir gs://my-bucket\n\nwill cause gsutil to follow any symbolic links in ./dir, creating objects in\nmy-bucket containing the data from the files to which the symlinks point. This\ncan cause various problems:\n\n* If you use gsutil rsync as a simple way to backup a directory to a bucket,\n  restoring from that bucket will result in files where the symlinks used\n  to be. At best this is wasteful of space, and at worst it can result in\n  outdated data or broken applications -- depending on what is consuming\n  the symlinks.\n\n* If you use gsutil rsync over directories containing broken symlinks,\n  gsutil rsync will abort (unless you pass the -e option).\n\n* gsutil rsync skips symlinks that point to directories.\n\nSince gsutil rsync is intended to support data operations (like moving a data\nset to the cloud for computational processing) and it needs to be compatible\nboth in the cloud and across common operating systems, there are no plans for\ngsutil rsync to support operating system-specific file types like symlinks.\n\nWe recommend that users do one of the following:\n\n* Don't use gsutil rsync over directories containing symlinks or other OS-\n  specific file types.\n* Use the -e option to exclude symlinks or the -x option to exclude\n  OS-specific file types by name.\n* Use a tool (such as tar) that preserves symlinks and other OS-specific file\n  types, packaging up directories containing such files before uploading to\n  the cloud.",
        "CHECKSUMS": "If you find that CRC32C checksum computation runs slowly, this is likely\nbecause you don't have a compiled CRC32c on your system. Try running:\n\n  gsutil ver -l\n\nIf the output contains:\n\n  compiled crcmod: False\n\nyou are running a Python library for computing CRC32C, which is much slower\nthan using the compiled code. For information on getting a compiled CRC32C\nimplementation, see 'gsutil help crc32c'.",
        "CONSISTENCY WITH NON-GOOGLE CLOUD PROVIDERS": "While Google Cloud Storage is strongly consistent, some cloud providers\nonly support eventual consistency. You may encounter scenarios where rsync\nsynchronizes using stale listing data when working with these other cloud\nproviders. For example, if you run rsync immediately after uploading an\nobject to an eventually consistent cloud provider, the added object may not\nyet appear in the provider's listing. Consequently, rsync will miss adding\nthe object to the destination. If this happens you can rerun the rsync\noperation again later (after the object listing has \"caught up\").",
        "DESCRIPTION": "The gsutil rsync command makes the contents under dst_url the same as the\ncontents under src_url, by copying any missing files/objects (or those whose\ndata has changed), and (if the -d option is specified) deleting any extra\nfiles/objects. src_url must specify a directory, bucket, or bucket\nsubdirectory. For example, to sync the contents of the local directory \"data\"\nto the bucket gs://mybucket/data, you could do:\n\n  gsutil rsync data gs://mybucket/data\n\nTo recurse into directories use the -r option:\n\n  gsutil rsync -r data gs://mybucket/data\n\nIf you have a large number of objects to synchronize you might want to use the\ngsutil -m option, to perform parallel (multi-threaded/multi-processing)\nsynchronization:\n\n  gsutil -m rsync -r data gs://mybucket/data\n\nThe -m option typically will provide a large performance boost if either the\nsource or destination (or both) is a cloud URL. If both source and\ndestination are file URLs the -m option will typically thrash the disk and\nslow synchronization down.\n\nNote 1: Shells (like bash, zsh) sometimes attempt to expand wildcards in ways\nthat can be surprising. Also, attempting to copy files whose names contain\nwildcard characters can result in problems. For more details about these\nissues see the section \"POTENTIALLY SURPRISING BEHAVIOR WHEN USING WILDCARDS\"\nunder \"gsutil help wildcards\".\n\nNote 2: If you are synchronizing a large amount of data between clouds you\nmight consider setting up a\n`Google Compute Engine <https://cloud.google.com/products/compute-engine>`_\naccount and running gsutil there. Since cross-provider gsutil data transfers\nflow through the machine where gsutil is running, doing this can make your\ntransfer run significantly faster than running gsutil on your local\nworkstation.",
        "DETECTION ALGORITHM": "To determine if a file or object has changed, gsutil rsync first checks\nwhether the file modification time (mtime) of both the source and destination\nis available. If mtime is available at both source and destination, and the\ndestination mtime is different than the source, or if the source and\ndestination file size differ, gsutil rsync will update the destination. If the\nsource is a cloud bucket and the destination is a local file system, and if\nmtime is not available for the source, gsutil rsync will use the time created\nfor the cloud object as a substitute for mtime. Otherwise, if mtime is not\navailable for either the source or the destination, gsutil rsync will fall\nback to using checksums. If the source and destination are both cloud buckets\nwith checksums available, gsutil rsync will use these hashes instead of mtime.\nHowever, gsutil rsync will still update mtime at the destination if it is not\npresent. If the source and destination have matching checksums and only the\nsource has an mtime, gsutil rsync will copy the mtime to the destination. If\nneither mtime nor checksums are available, gsutil rsync will resort to\ncomparing file sizes.\n\nChecksums will not be available when comparing composite Google Cloud Storage\nobjects with objects at a cloud provider that does not support CRC32C (which\nis the only checksum available for composite objects). See 'gsutil help\ncompose' for details about composite objects.",
        "IN THE CLOUD AND METADATA PRESERVATION": "If both the source and destination URL are cloud URLs from the same provider,\ngsutil copies data \"in the cloud\" (i.e., without downloading to and uploading\nfrom the machine where you run gsutil). In addition to the performance and\ncost advantages of doing this, copying in the cloud preserves metadata (like\nContent-Type and Cache-Control). In contrast, when you download data from the\ncloud it ends up in a file, which has no associated metadata, other than file\nmodification time (mtime). Thus, unless you have some way to hold on to or\nre-create that metadata, synchronizing a bucket to a directory in the local\nfile system will not retain the metadata other than mtime.\n\nNote that by default, the gsutil rsync command does not copy the ACLs of\nobjects being synchronized and instead will use the default bucket ACL (see\n\"gsutil help defacl\"). You can override this behavior with the -p option (see\nOPTIONS below).",
        "LIMITATIONS": "1. The gsutil rsync command will only allow non-negative file modification\n   times to be used in its comparisons. This means gsutil rsync will resort to\n   using checksums for any file with a timestamp before 1970-01-01 UTC.\n\n2. The gsutil rsync command considers only the live object version in\n   the source and destination buckets when deciding what to copy / delete. If\n   versioning is enabled in the destination bucket then gsutil rsync's\n   overwriting or deleting objects will end up creating versions, but the\n   command doesn't try to make any noncurrent versions match in the source\n   and destination buckets.\n\n3. The gsutil rsync command does not support copying special file types\n   such as sockets, device files, named pipes, or any other non-standard\n   files intended to represent an operating system resource. If you run\n   gsutil rsync on a source directory that includes such files (for example,\n   copying the root directory on Linux that includes /dev ), you should use\n   the -x flag to exclude these files. Otherwise, gsutil rsync may fail or\n   hang.\n\n4. The gsutil rsync command copies changed files in their entirety and does\n   not employ the\n   `rsync delta-transfer algorithm <https://rsync.samba.org/tech_report/>`_\n   to transfer portions of a changed file. This is because cloud objects are\n   immutable and no facility exists to read partial cloud object checksums or\n   perform partial overwrites.",
        "VALIDATION AND FAILURE HANDLING": "At the end of every upload or download, the gsutil rsync command validates\nthat the checksum of the source file/object matches the checksum of the\ndestination file/object. If the checksums do not match, gsutil will delete\nthe invalid copy and print a warning message. This very rarely happens, but\nif it does, please contact gs-team@google.com.\n\nThe rsync command will retry when failures occur, but if enough failures\nhappen during a particular copy or delete operation the command will fail.\n\nIf the -C option is provided, the command will instead skip the failing\nobject and move on. At the end of the synchronization run if any failures\nwere not successfully retried, the rsync command will report the count of\nfailures, and exit with non-zero status. At this point you can run the rsync\ncommand again, and it will attempt any remaining needed copy and/or delete\noperations.\n\nNote that there are cases where retrying will never succeed, such as if you\ndon't have write permission to the destination bucket or if the destination\npath for some objects is longer than the maximum allowed length.\n\nFor more details about gsutil's retry handling, please see\n\"gsutil help retries\"."
      }
    },
    "setmeta": {
      "capsule": "Set metadata on already uploaded objects",
      "commands": {},
      "flags": {
        "-h": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies a header:value to be added, or header to be removed,\nom each named object.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-h",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "setmeta"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The gsutil setmeta command allows you to set or remove the metadata on one\nor more objects. It takes one or more header arguments followed by one or\nmore URLs, where each header argument is in one of two forms:\n\n- if you specify header:value, it will set the given header on all\n  named objects.\n\n- if you specify header (with no value), it will remove the given header\n  from all named objects.\n\nFor example, the following command would set the Content-Type and\nCache-Control and remove the Content-Disposition on the specified objects:\n\n  gsutil setmeta -h \"Content-Type:text/html\" \\\n    -h \"Cache-Control:public, max-age=3600\" \\\n    -h \"Content-Disposition\" gs://bucket/*.html\n\nIf you have a large number of objects to update you might want to use the\ngsutil -m option, to perform a parallel (multi-threaded/multi-processing)\nupdate:\n\n  gsutil -m setmeta -h \"Content-Type:text/html\" \\\n    -h \"Cache-Control:public, max-age=3600\" \\\n    -h \"Content-Disposition\" gs://bucket/*.html\n\nYou can also use the setmeta command to set custom metadata on an object:\n\n  gsutil setmeta -h \"x-goog-meta-icecreamflavor:vanilla\" gs://bucket/object\n\nSee \"gsutil help metadata\" for details about how you can set metadata\nwhile uploading objects, what metadata fields can be set and the meaning of\nthese fields, use of custom metadata, and how to view currently set metadata.\n\nNOTE: By default, publicly readable objects are served with a Cache-Control\nheader allowing such objects to be cached for 3600 seconds. For more details\nabout this default behavior see the CACHE-CONTROL section of\n\"gsutil help metadata\". If you need to ensure that updates become visible\nimmediately, you should set a Cache-Control header of \"Cache-Control:private,\nmax-age=0, no-transform\" on such objects.  You can do this with the command:\n\n  gsutil setmeta -h \"Content-Type:text/html\" \\\n    -h \"Cache-Control:private, max-age=0, no-transform\" gs://bucket/*.html\n\nThe setmeta command reads each object's current generation and metageneration\nand uses those as preconditions unless they are otherwise specified by\ntop-level arguments. For example:\n\n  gsutil -h \"x-goog-if-metageneration-match:2\" setmeta\n    -h \"x-goog-meta-icecreamflavor:vanilla\"\n\nwill set the icecreamflavor:vanilla metadata if the current live object has a\nmetageneration of 2."
      }
    },
    "signurl": {
      "capsule": "Create a signed url",
      "commands": {},
      "flags": {
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies the content type for which the signed url is\nalid for.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-d": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies the duration that the signed url should be valid\nor, default duration is 1 hour.\nimes may be specified with no suffix (default hours), or\nith s = seconds, m = minutes, h = hours, d = days.\nhis option may be specified multiple times, in which case\nhe duration the link remains valid is the sum of all the\nuration options.\nhe max duration allowed is 7d.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-d",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-m": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specifies the HTTP method to be authorized for use\nith the signed url, default is GET. You may also specify\nESUMABLE to create a signed resumable upload start URL. When\nsing a signed URL to start a resumable upload session, you will\need to specify the 'x-goog-resumable:start' header in the\nequest or else signature validation will fail.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-m",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Specify the keystore password instead of prompting.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-r": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<region> Specifies the `region\nhttps://cloud.google.com/storage/docs/locations>`_ in\nhich the resources for which you are creating signed URLs are\ntored.\nefault value is 'auto' which will cause gsutil to fetch the\negion for the resource. When auto-detecting the region, the\nurrent gsutil user's credentials, not the credentials from the\nrivate-key-file, are used to fetch the bucket's metadata.\nhis option must be specified and not 'auto' when generating a\nigned URL to create a bucket.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-r",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-u": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "--use-service-account\nse service account credentials instead of a private key file\no sign the url.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-u",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "signurl"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The signurl command will generate a signed URL that embeds authentication data\nso the URL can be used by someone who does not have a Google account. Please\nsee the `Signed URLs documentation\n<https://cloud.google.com/storage/docs/access-control/signed-urls>`_ for\nbackground about signed URLs.\n\nMultiple gs:// urls may be provided and may contain wildcards. A signed url\nwill be produced for each provided url, authorized\nfor the specified HTTP method and valid for the given duration.\n\nNOTE: Unlike the gsutil ls command, the signurl command does not support\noperations on sub-directories. For example, unless you have an object named\n``some-directory/`` stored inside the bucket ``some-bucket``, the following\ncommand returns an error: ``gsutil signurl <private-key-file> gs://some-bucket/some-directory/``\n\nThe signurl command uses the private key for a service account (the\n'<private-key-file>' argument) to generate the cryptographic\nsignature for the generated URL. The private key file must be in PKCS12\nor JSON format. If the private key is encrypted the signed url command will\nprompt for the passphrase used to protect the private key file\n(default 'notasecret'). For more information regarding generating a private\nkey for use with the signurl command please see the `Authentication\ndocumentation.\n<https://cloud.google.com/storage/docs/authentication#generating-a-private-key>`_\n\nIf you used `service account credentials\n<https://cloud.google.com/storage/docs/gsutil/addlhelp/CredentialTypesSupportingVariousUseCases#supported-credential-types_1>`_\nfor authentication, you can replace the  <private-key-file> argument with\nthe -u or --use-service-account option to use the system-managed private key\ndirectly. This avoids the need to download the private key file.",
        "USAGE": "Create a signed url for downloading an object valid for 10 minutes:\n\n  gsutil signurl -d 10m <private-key-file> gs://<bucket>/<object>\n\n\nCreate a signed url without a private key, using a service account's\ncredentials:\n\n  gsutil signurl -d 10m -u gs://<bucket>/<object>\n\nCreate a signed url by impersonating a service account:\n\n  gsutil -i <service account email> signurl -d 10m -u gs://<bucket>/<object>\n\nCreate a signed url, valid for one hour, for uploading a plain text\nfile via HTTP PUT:\n\n  gsutil signurl -m PUT -d 1h -c text/plain <private-key-file> \\\n      gs://<bucket>/<obj>\n\nTo construct a signed URL that allows anyone in possession of\nthe URL to PUT to the specified bucket for one day, creating\nan object of Content-Type image/jpg, run:\n\n  gsutil signurl -m PUT -d 1d -c image/jpg <private-key-file> \\\n      gs://<bucket>/<obj>\n\nTo construct a signed URL that allows anyone in possession of\nthe URL to POST a resumable upload to the specified bucket for one day,\ncreating an object of Content-Type image/jpg, run:\n\n  gsutil signurl -m RESUMABLE -d 1d -c image/jpg <private-key-file> \\\n      gs://bucket/<obj>"
      }
    },
    "stat": {
      "capsule": "Display object status",
      "commands": {},
      "flags": {},
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "stat"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The stat command will output details about the specified object URLs.\nIt is similar to running:\n\n  gsutil ls -L gs://some-bucket/some-object\n\nbut is more efficient because it avoids performing bucket listings and gets\nthe minimum necessary amount of object metadata. Moreover, because it avoids\nperforming bucket listings (which for some storage providers are eventually\nconsistent) the gsutil stat command provides a strongly consistent way to\ncheck for the existence (and read the metadata) of an object.\n\nThe gsutil stat command will, however, perform bucket listings if you specify\nURLs using wildcards.\n\nIf run with the gsutil -q option nothing will be printed, e.g.:\n\n  gsutil -q stat gs://some-bucket/some-object\n\nThis behavior can be useful when writing scripts: even though nothing is\nprinted from the command, it still has an exit status of 0 for an existing\nobject and 1 for a non-existent object.\n\nNOTE: Unlike the gsutil ls command, the stat command does not support\noperations on sub-directories. For example, if you run the command:\n\n  gsutil -q stat gs://some-bucket/some-subdir/\n\ngsutil will look for information about an object called \"some-subdir/\" (with a\ntrailing slash) inside the bucket \"some-bucket\", as opposed to operating on\nobjects nested under gs://some-bucket/some-subdir/. Unless you actually have\nan object with that name, the operation will fail. However, you can use the\nstat command on objects within subdirectories. For example, this command will\nwork as expected:\n\n  gsutil -q stat gs://some-bucket/some-subdir/file.txt"
      }
    },
    "test": {
      "capsule": "Run gsutil unit/integration tests (for developers)",
      "commands": {},
      "flags": {
        "-b": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Run tests against multi-regional US buckets. By default,\nsts run against regional buckets in us-central1.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-b",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-c": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Output coverage information.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-c",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-f": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Exit on first sequential test failure.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-f",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-l": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "List available tests.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-l",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-p": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "N Run at most N tests in parallel. The default value is 5.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-p",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-s": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Run tests against S3 instead of GS.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-s",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-u": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Only run unit tests.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-u",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "test"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The gsutil test command runs the gsutil unit tests and integration tests.\nThe unit tests use an in-memory mock storage service implementation, while\nthe integration tests send requests to the production service using the\npreferred API set in the boto configuration file (see \"gsutil help apis\" for\ndetails).\n\nTo run both the unit tests and integration tests, run the command with no\narguments:\n\n  gsutil test\n\nTo run the unit tests only (which run quickly):\n\n  gsutil test -u\n\nTests run in parallel regardless of whether the top-level -m flag is\npresent. To limit the number of tests run in parallel to 10 at a time:\n\n  gsutil test -p 10\n\nTo force tests to run sequentially:\n\n  gsutil test -p 1\n\nTo have sequentially-run tests stop running immediately when an error occurs:\n\n  gsutil test -f\n\nTo run tests for one or more individual commands add those commands as\narguments. For example, the following command will run the cp and mv command\ntests:\n\n  gsutil test cp mv\n\nTo list available tests, run the test command with the -l argument:\n\n  gsutil test -l\n\nThe tests are defined in the code under the gslib/tests module. Each test\nfile is of the format test_[name].py where [name] is the test name you can\npass to this command. For example, running \"gsutil test ls\" would run the\ntests in \"gslib/tests/test_ls.py\".\n\nYou can also run an individual test class or function name by passing the\ntest module followed by the class name and optionally a test name. For\nexample, to run the an entire test class by name:\n\n  gsutil test naming.GsutilNamingTests\n\nor an individual test function:\n\n  gsutil test cp.TestCp.test_streaming\n\nYou can list the available tests under a module or class by passing arguments\nwith the -l option. For example, to list all available test functions in the\ncp module:\n\n  gsutil test -l cp\n\nTo output test coverage:\n\n  gsutil test -c -p 500\n  coverage html\n\nThis will output an HTML report to a directory named 'htmlcov'.\n\nTest coverage is compatible with v4.1 of the coverage module\n(https://pypi.python.org/pypi/coverage)."
      }
    },
    "ubla": {
      "capsule": "Configure Uniform bucket-level access",
      "commands": {
        "get": {
          "capsule": "Configure Uniform bucket-level access",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "ubla",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``ubla get`` command shows whether uniform bucket-level access is enabled\nfor the specified Cloud Storage bucket(s).",
            "EXAMPLES": "Check if your buckets are using uniform bucket-level access:\n\n  gsutil ubla get gs://redbucket gs://bluebucket"
          }
        },
        "set": {
          "capsule": "Configure Uniform bucket-level access",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "ubla",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The ``ubla set`` command enables or disables uniform\nbucket-level access for Google Cloud Storage buckets.",
            "EXAMPLES": "Configure your buckets to use uniform bucket-level access:\n\n  gsutil ubla set on gs://redbucket gs://bluebucket\n\nConfigure your buckets to NOT use uniform bucket-level access:\n\n  gsutil ubla set off gs://redbucket gs://bluebucket"
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "ubla"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The ``ubla`` command is used to retrieve or configure the\n`uniform bucket-level access\n<https://cloud.google.com/storage/docs/bucket-policy-only>`_ setting of\nCloud Storage bucket(s). This command has two sub-commands, ``get`` and\n``set``.",
        "EXAMPLES": "Configure your buckets to use uniform bucket-level access:\n\n  gsutil ubla set on gs://redbucket gs://bluebucket\n\nConfigure your buckets to NOT use uniform bucket-level access:\n\n  gsutil ubla set off gs://redbucket gs://bluebucket",
        "GET": "The ``ubla get`` command shows whether uniform bucket-level access is enabled\nfor the specified Cloud Storage bucket(s).",
        "SET": "The ``ubla set`` command enables or disables uniform\nbucket-level access for Google Cloud Storage buckets."
      }
    },
    "version": {
      "capsule": "Print version info about gsutil",
      "commands": {},
      "flags": {
        "-l": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "Prints additional information, such as the version of Python\ning used, the version of the Boto library, a checksum of the\nde, the path to gsutil, and the path to gsutil's configuration\nle.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-l",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": false,
      "is_hidden": false,
      "path": [
        "gsutil",
        "version"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "Prints information about the version of gsutil."
      }
    },
    "versioning": {
      "capsule": "Enable or suspend versioning for one or more buckets",
      "commands": {
        "get": {
          "capsule": "Enable or suspend versioning for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "versioning",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"get\" sub-command gets the versioning configuration for a\nbucket and displays whether or not it is enabled."
          }
        },
        "set": {
          "capsule": "Enable or suspend versioning for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "versioning",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"set\" sub-command requires an additional sub-command, either \"on\" or\n\"off\", which, respectively, will enable or disable versioning for the\nspecified bucket(s)."
          }
        }
      },
      "flags": {},
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "versioning"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The `Versioning Configuration\n<https://cloud.google.com/storage/docs/object-versioning>`_ feature\nenables you to configure a Google Cloud Storage bucket to keep old\nversions of objects.\n\nThe gsutil versioning command has two sub-commands:",
        "GET": "The \"get\" sub-command gets the versioning configuration for a\nbucket and displays whether or not it is enabled.",
        "SET": "The \"set\" sub-command requires an additional sub-command, either \"on\" or\n\"off\", which, respectively, will enable or disable versioning for the\nspecified bucket(s)."
      }
    },
    "web": {
      "capsule": "Set a main page and/or error page for one or more buckets",
      "commands": {
        "get": {
          "capsule": "Set a main page and/or error page for one or more buckets",
          "commands": {},
          "flags": {},
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "web",
            "get"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"gsutil web get\" command will gets the web semantics configuration for\na bucket and displays a JSON representation of the configuration.\n\nIn Google Cloud Storage, this would look like:\n\n  {\n    \"notFoundPage\": \"404.html\",\n    \"mainPageSuffix\": \"index.html\"\n  }"
          }
        },
        "set": {
          "capsule": "Set a main page and/or error page for one or more buckets",
          "commands": {},
          "flags": {
            "-e": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<404.html> Specifies the error page to serve when a request is made\n for a non-existent object via the CNAME alias to\n c.storage.googleapis.com.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-e",
              "nargs": "0",
              "type": "bool",
              "value": ""
            },
            "-m": {
              "attr": {},
              "category": "",
              "default": "",
              "description": "<index.html> Specifies the object name to serve when a bucket\n listing is requested via the CNAME alias to\n c.storage.googleapis.com.",
              "group": "",
              "is_global": false,
              "is_hidden": false,
              "is_required": false,
              "name": "-m",
              "nargs": "0",
              "type": "bool",
              "value": ""
            }
          },
          "groups": {},
          "is_group": false,
          "is_hidden": false,
          "path": [
            "gsutil",
            "web",
            "set"
          ],
          "positionals": [],
          "release": "GA",
          "sections": {
            "DESCRIPTION": "The \"gsutil web set\" command will allow you to configure or disable\nWebsite Configuration on your bucket(s). The \"set\" sub-command has the\nfollowing options (leave both options blank to disable):"
          }
        }
      },
      "flags": {
        "-e": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<404.html> Specifies the error page to serve when a request is made\n for a non-existent object via the CNAME alias to\n c.storage.googleapis.com.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-e",
          "nargs": "0",
          "type": "bool",
          "value": ""
        },
        "-m": {
          "attr": {},
          "category": "",
          "default": "",
          "description": "<index.html> Specifies the object name to serve when a bucket\n listing is requested via the CNAME alias to\n c.storage.googleapis.com.",
          "group": "",
          "is_global": false,
          "is_hidden": false,
          "is_required": false,
          "name": "-m",
          "nargs": "0",
          "type": "bool",
          "value": ""
        }
      },
      "groups": {},
      "is_group": true,
      "is_hidden": false,
      "path": [
        "gsutil",
        "web"
      ],
      "positionals": [],
      "release": "GA",
      "sections": {
        "DESCRIPTION": "The Website Configuration feature enables you to configure a Google Cloud\nStorage bucket to behave like a static website. This means requests made via a\ndomain-named bucket aliased using a Domain Name System \"CNAME\" to\nc.storage.googleapis.com will work like any other website, i.e., a GET to the\nbucket will serve the configured \"main\" page instead of the usual bucket\nlisting and a GET for a non-existent object will serve the configured error\npage.\n\nFor example, suppose your company's Domain name is example.com. You could set\nup a website bucket as follows:\n\n1. Create a bucket called www.example.com (see the \"DOMAIN NAMED BUCKETS\"\n   section of \"gsutil help naming\" for details about creating such buckets).\n\n2. Create index.html and 404.html files and upload them to the bucket.\n\n3. Configure the bucket to have website behavior using the command:\n\n     gsutil web set -m index.html -e 404.html gs://www.example.com\n\n4. Add a DNS CNAME record for www.example.com pointing to\n   c.storage.googleapis.com (ask your DNS administrator for help with this).\n\nNow if you open a browser and navigate to http://www.example.com, it will\ndisplay the main page instead of the default bucket listing.\n\nNOTE: It can take time for DNS updates to propagate because of caching used\nby the DNS, so it may take up to a day for the domain-named bucket website to\nwork after you create the CNAME DNS record.\n\nAdditional notes:\n\n1. Because the main page is only served when a bucket listing request is made\n   via the CNAME alias, you can continue to use \"gsutil ls\" to list the bucket\n   and get the normal bucket listing (rather than the main page).\n\n2. The main_page_suffix applies to each subdirectory of the bucket. For\n   example, with the main_page_suffix configured to be index.html, a GET\n   request for http://www.example.com would retrieve\n   http://www.example.com/index.html, and a GET request for\n   http://www.example.com/photos would retrieve\n   http://www.example.com/photos/index.html.\n\n3. There is just one 404.html page: For example, a GET request for\n   http://www.example.com/photos/missing would retrieve\n   http://www.example.com/404.html, not\n   http://www.example.com/photos/404.html.\n\n4. For additional details see\n   https://cloud.google.com/storage/docs/website-configuration.\n\nThe web command has two sub-commands:",
        "GET": "The \"gsutil web get\" command will gets the web semantics configuration for\na bucket and displays a JSON representation of the configuration.\n\nIn Google Cloud Storage, this would look like:\n\n  {\n    \"notFoundPage\": \"404.html\",\n    \"mainPageSuffix\": \"index.html\"\n  }",
        "SET": "The \"gsutil web set\" command will allow you to configure or disable\nWebsite Configuration on your bucket(s). The \"set\" sub-command has the\nfollowing options (leave both options blank to disable):"
      }
    }
  },
  "flags": {
    "-D": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Shows HTTP requests/headers and additional debug info needed when\nsting support requests, including exception stack traces.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-D",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-DD": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Shows HTTP requests/headers, additional debug info,\nception stack traces, plus HTTP upstream payload.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-DD",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-h": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Allows you to specify certain HTTP headers, for example:\ngsutil -h \"Cache-Control:public,max-age=3600\" \\\n -h \"Content-Type:text/html\" cp ...\nte that you need to quote the headers/values that\nntain spaces (such as \"Content-Disposition: attachment;\nlename=filename.ext\"), to avoid having the shell split them\nto separate arguments.\ne following headers are stored as object metadata and used\n future requests on the object:\nCache-Control\nContent-Disposition\nContent-Encoding\nContent-Language\nContent-Type\ne following headers are used to check data integrity:\nContent-MD5\nutil also supports custom metadata headers with a matching\noud Storage Provider prefix, such as:\nx-goog-meta-\nte that for gs:// URLs, the Cache Control header is specific to\ne API being used. The XML API will accept any cache control\naders and return them during object downloads. The JSON API\nspects only the public, private, no-cache, and max-age cache\nntrol headers, and may add its own no-transform directive even\n it was not specified. See 'gsutil help apis' for more\nformation on gsutil's interaction with APIs.\ne also \"gsutil help setmeta\" for the ability to set metadata\nelds on objects after they have been uploaded.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-h",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-i": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Allows you to use the configured credentials to impersonate a\nrvice account, for example:\ngsutil -i \"service-account@google.com\" ls gs://pub\nte that this setting will be ignored by the XML API and S3. See\nsutil help creds' for more information on impersonating service\ncounts.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-i",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-m": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Causes supported operations (acl ch, acl set, cp, mv, rm, rsync,\nd setmeta) to run in parallel. This can significantly improve\nrformance if you are performing operations on a large number of\nles over a reasonably fast network connection.\nutil performs the specified operation using a combination of\nlti-threading and multi-processing, using a number of threads\nd processors determined by the parallel_thread_count and\nrallel_process_count values set in the boto configuration\nle. You might want to experiment with these values, as the\nst values can vary based on a number of factors, including\ntwork speed, number of CPUs, and available memory.\ning the -m option may make your performance worse if you\ne using a slower network, such as the typical network speeds\nfered by non-business home network plans. It can also make\nur performance worse for cases that perform all operations\ncally (e.g., gsutil rsync, where both source and destination\nLs are on the local disk), because it can \"thrash\" your local\nsk.\n a download or upload operation using parallel transfer fails\nfore the entire transfer is complete (e.g. failing after 300 of\n00 files have been transferred), you will need to restart the\ntire transfer.\nso, although most commands will normally fail upon encountering\n error when the -m flag is disabled, all commands will\nntinue to try all operations when -m is enabled with multiple\nreads or processes, and the number of failed operations (if any)\nll be reported as an exception at the end of the command's\necution.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-m",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-o": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Set/override values in the boto configuration value, in the format\nection>:<name>=<value>, e.g. gsutil -o \"Boto:proxy=host\" ...\nis will not pass the option to gsutil integration tests, which\nn in a separate process.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-o",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-q": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Causes gsutil to perform operations quietly, i.e., without\nporting progress indicators of files being copied or removed,\nc. Errors are still reported. This option can be useful for\nnning gsutil from a cron job that logs its output to a file, for\nich the only information desired in the log is failures.",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-q",
      "nargs": "0",
      "type": "bool",
      "value": ""
    },
    "-u": {
      "attr": {},
      "category": "",
      "default": "",
      "description": "Allows you to specify a user project to be billed for the request.\nr example:\ngsutil -u \"bill-this-project\" cp ...",
      "group": "",
      "is_global": true,
      "is_hidden": false,
      "is_required": false,
      "name": "-u",
      "nargs": "0",
      "type": "bool",
      "value": ""
    }
  },
  "groups": {},
  "is_group": true,
  "is_hidden": false,
  "path": [
    "gsutil"
  ],
  "positionals": [],
  "release": "GA",
  "sections": {}
}
